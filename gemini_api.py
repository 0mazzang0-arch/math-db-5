# gemini_api.py
import google.generativeai as genai
from google.api_core.exceptions import ResourceExhausted, InternalServerError, ServiceUnavailable
import json
import re
import warnings
import time
import base64
import ast
from PIL import Image
from openai import OpenAI

from config import (
    GOOGLE_API_KEYS, OPENAI_API_KEY, MODEL_NAME_OCR, MODEL_NAME_ANALYSIS, OPENAI_MODEL_NAME,
    INSIGHT_SYSTEM_PROMPT, CONCEPT_SYSTEM_PROMPT, OCR_SYSTEM_PROMPT
)

warnings.filterwarnings("ignore")

# [ë¹„ìš© ì ˆê°] GPT ì‚¬ìš© ì—¬ë¶€ ìŠ¤ìœ„ì¹˜ (í˜„ì¬ False ê¶Œì¥)
USE_GPT_FALLBACK = False 

# ==========================================================
# [EXTREME PROMPT V30] THE "EVERYTHING" PROMPT
# ==========================================================
# ê¸°ì¡´ì˜ ëª¨ë“  íƒœê·¸(LIST í¬í•¨)ë¥¼ ì‚´ë¦¬ê³ , Teacher's Decoding í¬ë§·ì„ ì ìš©í•©ë‹ˆë‹¤.
# AI Annotationë§Œ Teacher's Decoding ë‚´ë¶€ë¡œ í†µí•©ë©ë‹ˆë‹¤.
# ==========================================================
TAGGED_SYSTEM_PROMPT = r"""
# Role Definition
You are a "Forensic Mathematical Logic Auditor" (ë””ì§€í„¸ í¬ë Œì‹ ìˆ˜í•™ ë…¼ë¦¬ ê°ì‚¬ê´€) and a "Top-Tier Mathematical Strategist".
Your duty is to extract content from handwritten math solutions with **Zero Tolerance for Omission**, and then independently generate universal mathematical strategies.

**CORE DIRECTIVE (THE PRIME DIRECTIVE):**
1. **NO SUMMARIZATION:** You are FORBIDDEN from summarizing. You must transcribe every detail.
2. **NO INTERPRETATION IN DB:** Do not interpret implied meanings for the Database columns. Extract only what is explicitly marked.
3. **SEPARATION OF CONCERNS (CRITICAL - SEPARATION OF BRAINS):**
   * The user's handwriting (arrows `->`, notes, symbols) represents the **"Teacher's View"**. All of it goes into `SECTION A`. Do NOT steal user's handwritten logic to create AI strategies.
   * The `ACTION_PROTOCOL` and `STRATEGY` represent the **"AI's Independent Brain"**. You must generate this from scratch based purely on the mathematical nature of the problem text.
   * Handwriting -> Go to **Verbatim**.

   ### [CRITICAL ADDITION] ANSWER EXTRACTION
* **Target:** You MUST identify the final answer of the problem.
* **Format:** Extract ONLY the final value (e.g., "3", "5", "42", "3\sqrt{2}", "â‘£").
* **Location:** Look at the end of the solution or inside the `[[GOAL]]` section.
* **Processing (DB):** Put this extracted value into the 'correct_answer' column.
---

# [PART 1] DETAILED EXTRACTION PROTOCOLS

## SECTION A: TEACHER'S DECODING (Teacher's View - 100% User Dependent)
**CRITICAL CHANGE:** For the following tags, you MUST use the format: `Symbol | Content | AI_Interpretation`
* **Symbol:** The mark used by the teacher (e.g., ğŸ¯, âš¡, â—, ğŸ”‘, â‘ , (ê°€), or arrows `->`).
* **Content:** The verbatim handwritten text next to the symbol.
* **AI_Interpretation:** Your mathematical explanation of what this implies.

### 1. NECESSITY (í•„ì—°ì„±) - `[[NECESSITY]]`
* **Trigger:** Text followed by an arrow (`->`), `[...]`, or `(í•„)`.
* **Instruction:** ALL logical flows and arrows drawn by the user MUST be captured here. This is the Teacher's thought process.
* **Format:** `[Symbol/Arrow] | [Text inside brackets or pointed to] | [Why is this necessary?]`

### 2. KEY IDEA (í•µì‹¬) - `[[KEY_IDEA]]`
* **Trigger:** `ãŠ„`, `(í•µ)`, or `ğŸ”‘`.
* **Format:** `[Symbol] | [Text] | [What theorem/concept is used?]`

### 3. SPECIAL POINT (íŠ¹ì´ì ) - `[[SPECIAL_POINT]]`
* **Trigger:** `ãŠ•`, `(íŠ¹)`, or `â—`.
* **Format:** `[Symbol] | [Text] | [Why is this a trap/special case?]`

### 4. GOAL (êµ¬í•˜ëŠ” ëª©í‘œ) - `[[GOAL]]`
* **Trigger:** `ãŠˆ`, `(êµ¬)`, or `ğŸ¯`.
* **Format:** `[Symbol] | [Text] | [What is the final target variable?]`

### 5. CONDITIONS (ì¡°ê±´) - `[[CONDITIONS]]`
* **Trigger:** `â‘ `, `â‘¡`, `(ê°€)`, `(ë‚˜)`, or `âš¡`.
* **Format:** `[Symbol] | [Text] | [Mathematical translation of condition]`

---

## SECTION B: BODY CONTENT & SUPPLEMENTARY (AI's Independent Brain)

### 1. ACTION PROTOCOL - `[[ACTION_PROTOCOL]]`
* **Target:** THIS IS YOUR INDEPENDENT STRATEGY. Do NOT copy the user's handwritten arrows here.
* **Instruction:** Look at the original problem text. What is the universal heuristic for this *type* of problem?
* **Format:** Write 2-3 bullet points of universal mathematical behavior rules (e.g., **[Trigger]** ... -> **[Action]** ...).

### 2. STRATEGY - `[[STRATEGY]]`
* **Target:** Overall workflow. Substitute â‘  with actual meaning. (Independent of user's specific steps).

### 3. PRACTICAL CONCEPTS - `[[PRACTICAL_CONCEPTS]]`
* **Trigger:** `ã‰¦` or `(ì‹¤)`. Format: `Title: ... || Content: ...`

### 4. BASIC CONCEPTS - `[[BASIC_CONCEPTS]]`
* **Trigger:** `ãŠ‚` or `(ê¸°)`. Basic definitions used.

### 5. FIGURE ANALYSIS - `[[FIGURE_ANALYSIS]]`
* **Target:** Description of graphs or geometric figures.

### 6. VERBATIM - `[[VERBATIM]]`
* **Target:** ALL handwriting. Strict LaTeX. No Korean inside `$`. Every pixel must be translated to LaTeX.

### 7. SUPPLEMENTARY LISTS (Safety Net)
* **KEY_IDEAS_LIST:** If multiple key ideas exist, list them here too.
* **SPECIAL_POINTS_LIST:** If multiple special points exist, list them here too.

---

# [PART 2] OUTPUT FORMAT (STRICT TAG SYSTEM)

**Generate output strictly in KOREAN.**

[[NECESSITY_START]]
(Format: Symbol | Content | Interpretation)
[[NECESSITY_END]]

[[KEY_IDEA_START]]
(Format: Symbol | Content | Interpretation)
[[KEY_IDEA_END]]

[[KEY_IDEAS_LIST_START]]
(Supplementary list for safety)
[[KEY_IDEAS_LIST_END]]

[[SPECIAL_POINT_START]]
(Format: Symbol | Content | Interpretation)
[[SPECIAL_POINT_END]]

[[SPECIAL_POINTS_LIST_START]]
(Supplementary list for safety)
[[SPECIAL_POINTS_LIST_END]]

[[GOAL_START]]
(Format: Symbol | Content | Interpretation)
[[GOAL_END]]

[[CONDITIONS_START]]
(Format: Symbol | Content | Interpretation)
[[CONDITIONS_END]]

[[ACTION_PROTOCOL_START]]
(AI's independent generalized strategy heuristics)
[[ACTION_PROTOCOL_END]]

[[STRATEGY_START]]
(Workflow summary)
[[STRATEGY_END]]

[[PRACTICAL_CONCEPTS_START]]
(Format: Title: ... || Content: ...)
[[PRACTICAL_CONCEPTS_END]]

[[BASIC_CONCEPTS_START]]
(Basic definitions)
[[BASIC_CONCEPTS_END]]

[[FIGURE_ANALYSIS_START]]
(Graph description)
[[FIGURE_ANALYSIS_END]]

[[VERBATIM_START]]
(Pixel-perfect transcription)
[[VERBATIM_END]]

[[AI_SOLUTION_START]]
(Standard solution)
[[AI_SOLUTION_END]]

[[DEEP_INSIGHT_START]]
(Leave empty)
[[DEEP_INSIGHT_END]]
"""

gpt_client = None
if OPENAI_API_KEY and OPENAI_API_KEY.startswith("sk-"):
    try: gpt_client = OpenAI(api_key=OPENAI_API_KEY)
    except: pass

CURRENT_KEY_INDEX = 0

def initialize_api():
    global CURRENT_KEY_INDEX
    if not GOOGLE_API_KEYS: return
    genai.configure(api_key=GOOGLE_API_KEYS[CURRENT_KEY_INDEX])

def rotate_api_key():
    global CURRENT_KEY_INDEX
    if len(GOOGLE_API_KEYS) <= 1: return False
    prev_index = CURRENT_KEY_INDEX
    CURRENT_KEY_INDEX = (CURRENT_KEY_INDEX + 1) % len(GOOGLE_API_KEYS)
    genai.configure(api_key=GOOGLE_API_KEYS[CURRENT_KEY_INDEX])
    print(f"\nğŸ”„ [Quota] API Key êµì²´ ì™„ë£Œ (Key {prev_index + 1} -> Key {CURRENT_KEY_INDEX + 1})")
    return True

initialize_api()
REQUEST_OPTIONS = {"timeout": 600}

# [ëª¨ë¸ ì„¤ì •]
analysis_model = genai.GenerativeModel(model_name=MODEL_NAME_ANALYSIS, system_instruction=TAGGED_SYSTEM_PROMPT)
search_model = genai.GenerativeModel(model_name=MODEL_NAME_OCR, system_instruction=OCR_SYSTEM_PROMPT, generation_config={"temperature": 0.0})
insight_model = genai.GenerativeModel(model_name=MODEL_NAME_ANALYSIS, system_instruction=INSIGHT_SYSTEM_PROMPT)
concept_model = genai.GenerativeModel(model_name=MODEL_NAME_ANALYSIS, system_instruction=CONCEPT_SYSTEM_PROMPT)

def encode_image_to_base64(image_path):
    with open(image_path, "rb") as f: return base64.b64encode(f.read()).decode('utf-8')

# ==========================================================
# [JSON Helper Functions] - [ë³µêµ¬ ì™„ë£Œ] ì‚­ì œë˜ì—ˆë˜ í•¨ìˆ˜ë“¤ 100% ë³µêµ¬
# ==========================================================
def clean_json_text(text):
    if not text: return ""
    text = re.sub(r'```json\s*', '', text)
    text = re.sub(r'```', '', text)
    start_idx = text.find('{')
    end_idx = text.rfind('}')
    if start_idx != -1 and end_idx != -1:
        text = text[start_idx : end_idx+1]
    return text.strip()

def repair_json_content(text):
    if not text: return "{}"
    text = re.sub(r'(?<!\\)\\(?!["\\/bfnrtu])', r'\\\\', text)
    def replace_newlines_in_string(match):
        content = match.group(1).replace('\n', '\\n').replace('\r', '')
        return f'"{content}"'
    try:
        text = re.sub(r'"\s*:\s*"([^"\\]*(?:\\.[^"\\]*)*)"', replace_newlines_in_string, text, flags=re.DOTALL)
    except: pass
    return text

def try_advanced_parsing(text):
    try: return json.loads(text)
    except:
        try: return json.loads(repair_json_content(text))
        except:
            try:
                py_text = text.replace("true", "True").replace("false", "False").replace("null", "None")
                return ast.literal_eval(py_text)
            except: return None

def parse_broken_json(text):
    # [ë³µêµ¬ ì™„ë£Œ] í˜¹ì‹œ ëª¨ë¥¼ JSON í¬ë§· ì—ëŸ¬ ì‹œ ê°•ì œ ì¶”ì¶œìš© í•¨ìˆ˜
    print("âš  [Warning] JSON íŒŒì‹± ì‹¤íŒ¨. ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ê°•ì œ ì¶”ì¶œì„ ì‹œë„í•©ë‹ˆë‹¤.")
    fallback_data = {
        "db_columns": {"necessity": "", "key_idea": "", "special_point": ""},
        "body_content": {}
    }
    patterns = {
        "necessity": r'"necessity"\s*:\s*"([^"]+)"',
        "key_idea": r'"key_idea"\s*:\s*"([^"]+)"',
        "goal": r'"goal"\s*:\s*"([^"]+)"',
        "ai_solution": r'"ai_solution"\s*:\s*"([^"]+)"'
    }
    for key, pat in patterns.items():
        match = re.search(pat, text)
        if match:
            val = match.group(1)
            if key in ["necessity", "key_idea"]:
                fallback_data["db_columns"][key] = val
            else:
                fallback_data["body_content"][key] = val
    return fallback_data

# ==========================================================
# [The Monster Parser V30] Manual & Verbose Extraction (Restored & Enhanced)
# ==========================================================
def parse_tagged_response(text):
    print("ğŸšœ [Parser V30] ë°ì´í„° ì¶”ì¶œ ì‹œì‘ (Manual & Verbose Mode)...")
    
    data = {
        "db_columns": {"necessity": "", "key_idea": "", "special_point": ""},
        "body_content": {
            "teacher_decoding": [], # [ì‹ ê·œ] ì„ ìƒë‹˜ì˜ ì‹œì„  í†µí•©
            "conditions": [], "special_points": [], "key_ideas": [], # [ë³µêµ¬] Legacy ë¦¬ìŠ¤íŠ¸ í•„ë“œ
            "basic_concepts": [], "practical_concepts": [], 
            "figure_analysis": "", "verbatim_handwriting": "", 
            "ai_solution": "", "instructor_solution": "",
            "strategy_overview": "", "action_protocol": ""
        }
    }

    # [Verbose Extraction Helper] - [ë³µêµ¬ ì™„ë£Œ] ë¡œê·¸ ë° ë¶€ë¶„ ë§¤ì¹­ ê¸°ëŠ¥
    def extract_section(start_tag, end_tag, debug_name):
        base_start = start_tag.replace("[", "").replace("]", "")
        base_end = end_tag.replace("[", "").replace("]", "")
        
        # [í•µì‹¬ ìˆ˜ìˆ ] ê´„í˜¸ [, ], ë³„í‘œ *, ìƒµ # ê¸°í˜¸ê°€ ì„ì—¬ìˆì–´ë„ ë¬´ì¡°ê±´ ì°¾ì•„ë‚´ëŠ” ì •ê·œì‹ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
        pattern = r'[\#\*\s\[\]]*' + base_start + r'[\#\*\s\[\]]*(.*?)[\#\*\s\[\]]*' + base_end + r'[\#\*\s\[\]]*'
        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
        if match: return match.group(1).strip()
        
        # Fallback: ë£¨ì¦ˆ ë§¤ì¹­ (ì¢…ë£Œ íƒœê·¸ë¥¼ ë¹¼ë¨¹ì€ ê²½ìš°)
        pattern_loose = r'[\#\*\s\[\]]*' + base_start + r'[\#\*\s\[\]]*(.*)'
        match_loose = re.search(pattern_loose, text, re.DOTALL | re.IGNORECASE)
        if match_loose:
            content = match_loose.group(1).strip()
            # ë‹¤ìŒ ì‹œì‘ íƒœê·¸ê°€ ë‚˜ì˜¤ê¸° ì „ê¹Œì§€ ì˜ë¼ëƒ„
            next_tag_match = re.search(r'[\#\*\s\[\]]*[A-Z_]+_START[\#\*\s\[\]]*', content, re.IGNORECASE)
            if next_tag_match: return content[:next_tag_match.start()].strip()
            return content
            
        # ìµœí›„ì˜ ë³´ë£¨ (AI_SOLUTION ì „ìš©)
        if "AI_SOLUTION" in start_tag:
            alt_match = re.search(r'#+\s*AI\s*(ì •ì„\s*)?í•´ì„¤(.*?)(?=#+|$)', text, re.DOTALL | re.IGNORECASE)
            if alt_match: return alt_match.group(2).strip()
            
        return ""

    def clean_list(raw_text):
        if not raw_text: return []
        lines = raw_text.split('\n')
        cleaned = []
        for line in lines:
            line = re.sub(r'^[\s\*\-\d\.]+', '', line).strip() 
            if line: cleaned.append(line)
        return cleaned

    # [ë³µêµ¬ ì™„ë£Œ] ì¤‘ë³µ ì œê±° ë° ë³‘í•© ë¡œì§ (ë¦¬ìŠ¤íŠ¸ í•©ì¹˜ê¸°ìš©)
    def merge_and_deduplicate(single_text, list_text):
        items = []
        # 1. Single text ì²˜ë¦¬
        if single_text: items.append(single_text)
        # 2. List text ì²˜ë¦¬
        if list_text:
            lines = clean_list(list_text)
            for line in lines:
                is_duplicate = False
                for existing in items:
                    if line in existing or existing in line: # í¬í•¨ ê´€ê³„ í™•ì¸
                        is_duplicate = True
                        break
                if not is_duplicate:
                    items.append(line)
        return "\n".join(items)

    # [V30 New Logic] íŒŒì´í”„(|) êµ¬ë¶„ Teacher's Decoding íŒŒì„œ
    def parse_teacher_decoding(raw_text, item_type):
        items = []
        if not raw_text: return items
        lines = raw_text.split('\n')
        for line in lines:
            if not line.strip(): continue
            parts = [p.strip() for p in line.split('|')]
            
            symbol = parts[0] if len(parts) > 0 else "Note"
            content = parts[1] if len(parts) > 1 else parts[0]
            ai_comment = parts[2] if len(parts) > 2 else ""
            
            if len(parts) == 1: 
                symbol = "Note"
                content = line
            
            items.append({
                "type": item_type,
                "symbol": symbol,
                "content": content,
                "ai_comment": ai_comment
            })
        return items

    # --- 1. Extraction & Integration (Dual Processing) ---
    print("  >> ì„ ìƒë‹˜ì˜ ì‹œì„ (Decoding) ì¶”ì¶œ ë° ë ˆê±°ì‹œ ë°ì´í„° ë³‘í•© ì¤‘...")
    decoding_list = []
    
    # 1. Necessity
    raw_nec = extract_section("[[NECESSITY_START]]", "[[NECESSITY_END]]", "Necessity")
    decoding_list.extend(parse_teacher_decoding(raw_nec, "í•„ì—°ì„±"))
    data["db_columns"]["necessity"] = raw_nec.replace("|", " ").replace("\n", " ")

    # 2. Key Idea (Merge Single + List Tag) [ë³µêµ¬ëœ ë¡œì§]
    k1 = extract_section("[[KEY_IDEA_START]]", "[[KEY_IDEA_END]]", "KeyIdea(Single)")
    k2 = extract_section("[[KEY_IDEAS_LIST_START]]", "[[KEY_IDEAS_LIST_END]]", "KeyIdea(List)")
    raw_key_merged = merge_and_deduplicate(k1, k2)
    
    # í†µí•©ëœ í…ìŠ¤íŠ¸ë¥¼ Decoding Listì—ë„ ë„£ê³ , DB ì»¬ëŸ¼ì—ë„ ë„£ìŒ (ì´ì¤‘ ì €ì¥)
    decoding_list.extend(parse_teacher_decoding(raw_key_merged, "í•µì‹¬ ì•„ì´ë””ì–´"))
    data["db_columns"]["key_idea"] = raw_key_merged.replace("|", " ").replace("\n", " ")
    data["body_content"]["key_ideas"] = clean_list(k2) # [Legacy List ë³´ì¡´]

    # 3. Special Point (Merge Single + List Tag) [ë³µêµ¬ëœ ë¡œì§]
    s1 = extract_section("[[SPECIAL_POINT_START]]", "[[SPECIAL_POINT_END]]", "SpecialPoint(Single)")
    s2 = extract_section("[[SPECIAL_POINTS_LIST_START]]", "[[SPECIAL_POINTS_LIST_END]]", "SpecialPoint(List)")
    raw_sp_merged = merge_and_deduplicate(s1, s2)
    
    decoding_list.extend(parse_teacher_decoding(raw_sp_merged, "íŠ¹ì´ì "))
    data["db_columns"]["special_point"] = raw_sp_merged.replace("|", " ").replace("\n", " ")
    data["body_content"]["special_points"] = clean_list(s2) # [Legacy List ë³´ì¡´]

    # 4. Goal
    raw_goal = extract_section("[[GOAL_START]]", "[[GOAL_END]]", "Goal")
    decoding_list.extend(parse_teacher_decoding(raw_goal, "êµ¬í•˜ëŠ” ëª©í‘œ"))
    data["body_content"]["goal"] = raw_goal

    # 5. Conditions
    raw_cond = extract_section("[[CONDITIONS_START]]", "[[CONDITIONS_END]]", "Conditions")
    decoding_list.extend(parse_teacher_decoding(raw_cond, "ì¡°ê±´"))
    data["body_content"]["conditions"] = clean_list(raw_cond) # [Legacy List ë³´ì¡´]
    
    # ìµœì¢… í†µí•© ë¦¬ìŠ¤íŠ¸ ì €ì¥
    data["body_content"]["teacher_decoding"] = decoding_list

    # --- 2. Body Content Extraction (Restored All Fields) ---
    print("  >> ë³¸ë¬¸ ì½˜í…ì¸  ë° ê·¸ë˜í”„/ê¸°ë³¸ê°œë… ì¶”ì¶œ ì¤‘...")
    data["body_content"]["verbatim_handwriting"] = extract_section("[[VERBATIM_START]]", "[[VERBATIM_END]]", "Verbatim")
    data["body_content"]["ai_solution"] = extract_section("[[AI_SOLUTION_START]]", "[[AI_SOLUTION_END]]", "AI Solution")
    data["body_content"]["instructor_solution"] = extract_section("[[DEEP_INSIGHT_START]]", "[[DEEP_INSIGHT_END]]", "Insight")
    
    data["body_content"]["strategy_overview"] = extract_section("[[STRATEGY_START]]", "[[STRATEGY_END]]", "Strategy")
    data["body_content"]["action_protocol"] = extract_section("[[ACTION_PROTOCOL_START]]", "[[ACTION_PROTOCOL_END]]", "ActionProtocol")
    
    # [ë³µêµ¬] Figure Analysis & Basic Concepts
    data["body_content"]["figure_analysis"] = extract_section("[[FIGURE_ANALYSIS_START]]", "[[FIGURE_ANALYSIS_END]]", "Figure")
    data["body_content"]["basic_concepts"] = clean_list(extract_section("[[BASIC_CONCEPTS_START]]", "[[BASIC_CONCEPTS_END]]", "BasicConcepts"))

    # ì‹¤ì „ê°œë… ì •ì œ
    pc_raw = extract_section("[[PRACTICAL_CONCEPTS_START]]", "[[PRACTICAL_CONCEPTS_END]]", "PracticalConcepts")
    pc_list = []
    if pc_raw:
        lines = pc_raw.split('\n')
        for line in lines:
            parts = re.split(r'\|\|', line)
            if len(parts) >= 2:
                title_part = parts[0].replace("Title:", "").strip()
                content_part = parts[1].replace("Content:", "").strip()
                if title_part:
                    pc_list.append({"title": title_part, "content": content_part})
    data["body_content"]["practical_concepts"] = pc_list

    return data

def execute_with_key_rotation(model, content, **kwargs):
    if "safety_settings" not in kwargs:
        kwargs["safety_settings"] = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        ]

    max_attempts = len(GOOGLE_API_KEYS) + 1 
    for attempt in range(max_attempts):
        try:
            return model.generate_content(content, **kwargs)
        except (ResourceExhausted, InternalServerError, ServiceUnavailable) as e:
            print(f"âš  Gemini API Error. í‚¤ êµì²´ ì‹œë„... ({e})")
            if rotate_api_key(): time.sleep(2); continue 
            else: raise e
        except Exception as e:
            if "429" in str(e) or "quota" in str(e).lower():
                print(f"âš  429 Rate Limit. í‚¤ êµì²´ ì‹œë„...")
                if rotate_api_key(): time.sleep(2); continue
            raise e 
    raise Exception("All Gemini Keys Exhausted")

def analyze_image_structure(image_path):
    import concept_manager
    raw_concepts = concept_manager.load_concepts()
    concept_list_text = "\n".join([f"- {c['title']}: {c.get('content','...')[:50]}" for c in raw_concepts[:50]])
    
    print("\n--- [Stage 1] êµ¬ì¡° ë¶„ì„ (V30 Forensic Juggernaut Mode) ì‹œì‘ ---")
    
    user_prompt = f"""
    Analyze the image strictly according to the 'Forensic Auditor' protocol.
    Fill all the tags in KOREAN.
    
    [Reference Concept DB Context]
    {concept_list_text}
    """
    
    try:
        response = execute_with_key_rotation(
            analysis_model, 
            [user_prompt, Image.open(image_path)],
            generation_config={"max_output_tokens": 8192, "temperature": 0.1},
            request_options=REQUEST_OPTIONS
        )
        result_data = parse_tagged_response(response.text)
        
        # [ê²€ì¦ ë¡œì§ ë³´ì¡´]
        body = result_data["body_content"]
        if not body["verbatim_handwriting"]:
            if body["strategy_overview"] or body["ai_solution"]:
                print("âš ï¸ [Warn] ì†ê¸€ì”¨(Verbatim)ëŠ” ì—†ì§€ë§Œ, í•´ì„¤/ì „ëµì´ ìˆì–´ í†µê³¼í•©ë‹ˆë‹¤.")
            else:
                print("âŒ [Error] ë¶„ì„ ë°ì´í„° ì „ë¬´ (Verbatim/Strategy/Solution ëª¨ë‘ ì—†ìŒ).")
                raise Exception("Critical Extraction Failed (Empty Data)")
            
        print("âœ… [Plan A] íƒœê·¸ íŒŒì‹± ì„±ê³µ")

    except Exception as e:
        print(f"âš  [Plan A ì‹¤íŒ¨] {e}. [Plan B] GPT ìš©ë³‘ íˆ¬ì… ì—¬ë¶€ í™•ì¸...")
        return None

    print("\n--- [Stage 2] ì‹¬ì¸µ ì¶”ë¡  ì‹œì‘ ---")
    concept_db_text = json.dumps(concept_manager.load_concepts(), ensure_ascii=False, indent=2)
    base_sol = result_data.get("body_content", {}).get("ai_solution", "")
    
    # [Insight ìƒì„± ë¡œì§ ë³´ì¡´]
    # (ì£¼ì˜) íŒŒì„œê°€ ë”ë¯¸ í…ìŠ¤íŠ¸ë¥¼ ì¡ì•„ì™€ì„œ ifë¬¸ì´ Falseê°€ ë˜ëŠ” ì¹˜ëª…ì  ë²„ê·¸ë¥¼ ì›ì²œ ì°¨ë‹¨í•¨.
    # ë”ë¯¸ í…ìŠ¤íŠ¸ê°€ ìˆë“  ì—†ë“ , ë¬´ì¡°ê±´ Stage 2 ë…ë¦½ ì—ì´ì „íŠ¸ë¥¼ ê°€ë™í•˜ì—¬ ë®ì–´ì”Œì›€!
    deep_insight = generate_deep_insight(image_path, base_sol, concept_db_text) 
    result_data["body_content"]["instructor_solution"] = deep_insight
    
    print("âœ… ë¶„ì„ ì™„ë£Œ.")
    return result_data

def generate_deep_insight(image_path, base_solution_text, concept_db_text):
    final_prompt = INSIGHT_SYSTEM_PROMPT.replace("[USER_CONCEPT_DB]", concept_db_text)
    user_prompt = f"[Standard Solution]:\n{base_solution_text}\n\nBased on the image and solution, provide the 1-Tier Insight."
    try:
        dynamic_insight_model = genai.GenerativeModel(model_name=MODEL_NAME_ANALYSIS, system_instruction=final_prompt)
        response = execute_with_key_rotation(
            dynamic_insight_model, [user_prompt, Image.open(image_path)], request_options=REQUEST_OPTIONS
        )
        return response.text.strip()
    except: return "ì‹¬ì¸µ ë¶„ì„ ìƒì„± ì‹¤íŒ¨"

def call_gpt4o_fallback(system_instr, user_instr, image_path):
    return None

def get_pure_ocr_text(image_path):
    try:
        response = execute_with_key_rotation(
            search_model, ["Execute OCR.", Image.open(image_path)], request_options=REQUEST_OPTIONS
        )
        return response.text.replace("```latex", "").replace("```", "").strip()
    except: return None

def extract_concepts_flexible(image_path):
    try:
        response = execute_with_key_rotation(
            concept_model, ["Extract concepts strictly in JSON.", Image.open(image_path)],
            generation_config={"response_mime_type": "application/json"}, request_options=REQUEST_OPTIONS
        )
        cleaned = clean_json_text(response.text)
        return json.loads(cleaned)
    except: return None
    
# [NEW] Aë‹¨ê³„(ë‹¨ìˆœê³„ì‚°) íŒë…ê¸° - Track B ì „ìš©
def check_is_basic_drill(text):
    if not text or len(text) < 5: return False
    try:
        # Flash ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¹ ë¥´ê²Œ íŒë‹¨ (True/False)
        prompt = f"""
        Role: Math Problem Classifier.
        Task: Analyze the text and determine if it is a "Simple Calculation Drill" (A-step,ë‹¨ìˆœ ì—°ì‚°,ê¸°ì´ˆ ë¬¸ì œ).
        Text: {text[:500]}
        Output: Return ONLY 'TRUE' if it is a simple drill, 'FALSE' otherwise.
        """
        resp = execute_with_key_rotation(search_model, [prompt], request_options=REQUEST_OPTIONS)
        return "TRUE" in resp.text.strip().upper()
    except: return False

# [NEW] ì‹œëŒ€ì¸ì¬ê¸‰ ë‚œì´ë„ íŒë…ê¸° (Level 1~4 Classifier)
def analyze_difficulty_level(text):
    if not text: return "ê¸°ë³¸ê°œë…"
    try:
        # Flash ëª¨ë¸ì—ê²Œ 'ë¬¸ì œì˜ ê´€ìƒ(Heuristics)'ì„ ë³´ê³  íŒë‹¨í•˜ë¼ê³  ì§€ì‹œ
        prompt = f"""
        Role: Math Problem Difficulty Classifier.
        Task: Classify the difficulty of the given math problem text into one of 4 levels.
        
        [Criteria]
        1. LEVEL_1 (Basic Concept): Short text (1-3 lines), asks for simple calculation or basic definition.
        2. LEVEL_2 (Entry Semi-Killer): Standard 4-point problem. Has 1-2 conditions. Typical textbook style.
        3. LEVEL_3 (Deep Semi-Killer): Hard 4-point. Keywords: "Defined function g(x)", "Differentiability", "Select all correct (ã„±,ã„´,ã„·)", "Fill in the blank". Requires logical deduction.
        4. LEVEL_4 (Killer): Very long text, complex conditions, new function definitions, finding Max/Min in complex situations. 

        Input Text:
        {text[:800]}

        Output: ONLY return one word: "LEVEL_1", "LEVEL_2", "LEVEL_3", or "LEVEL_4".
        """
        resp = execute_with_key_rotation(search_model, [prompt], request_options=REQUEST_OPTIONS)
        result = resp.text.strip().upper()
        
        if "LEVEL_4" in result: return "í‚¬ëŸ¬"
        if "LEVEL_3" in result: return "ì¤€í‚¬ëŸ¬_ì‹¬í™”"
        if "LEVEL_2" in result: return "ì¤€í‚¬ëŸ¬_ì§„ì…"
        return "ê¸°ë³¸ê°œë…"
    except: return "ê¸°ë³¸ê°œë…"