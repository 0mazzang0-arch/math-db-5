# gemini_api.py
import google.generativeai as genai
from google.api_core.exceptions import ResourceExhausted, InternalServerError, ServiceUnavailable
import json
import re
import warnings
import time
import base64
import ast
from PIL import Image
from openai import OpenAI

from config import (
    GOOGLE_API_KEYS, OPENAI_API_KEY, MODEL_NAME_OCR, MODEL_NAME_ANALYSIS, OPENAI_MODEL_NAME,
    INSIGHT_SYSTEM_PROMPT, CONCEPT_SYSTEM_PROMPT, OCR_SYSTEM_PROMPT
)

warnings.filterwarnings("ignore")

# [ÎπÑÏö© Ï†àÍ∞ê] GPT ÏÇ¨Ïö© Ïó¨Î∂Ä Ïä§ÏúÑÏπò (ÌòÑÏû¨ False Í∂åÏû•)
USE_GPT_FALLBACK = False 

# ==========================================================
# [EXTREME PROMPT V30] THE "EVERYTHING" PROMPT
# ==========================================================
# Í∏∞Ï°¥Ïùò Î™®Îì† ÌÉúÍ∑∏(LIST Ìè¨Ìï®)Î•º ÏÇ¥Î¶¨Í≥†, Teacher's Decoding Ìè¨Îß∑ÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.
# AI AnnotationÎßå Teacher's Decoding ÎÇ¥Î∂ÄÎ°ú ÌÜµÌï©Îê©ÎãàÎã§.
# ==========================================================
TAGGED_SYSTEM_PROMPT = r"""
# Role Definition
You are a "Forensic Mathematical Logic Auditor" (ÎîîÏßÄÌÑ∏ Ìè¨Î†åÏãù ÏàòÌïô ÎÖºÎ¶¨ Í∞êÏÇ¨Í¥Ä).
Your duty is to extract content from handwritten math solutions with **Zero Tolerance for Omission** and **Absolute Structure Adherence**.

**CORE DIRECTIVE (THE PRIME DIRECTIVE):**
1. **NO SUMMARIZATION:** You are FORBIDDEN from summarizing. You must transcribe every detail.
2. **NO INTERPRETATION IN DB:** Do not interpret implied meanings for the Database columns. Extract only what is explicitly marked.
3. **SEPARATION OF CONCERNS:**
   * Explicit Markers (`[...]`, `„äÑ`, `„äï`) -> Go to **Teacher's Decoding Tags**.
   * Logical Flow (`->`) -> Go to **Action Protocol**.
   * Handwriting -> Go to **Verbatim**.

   ### [CRITICAL ADDITION] ANSWER EXTRACTION
* **Target:** You MUST identify the final answer of the problem.
* **Format:** Extract ONLY the final value (e.g., "3", "5", "42", "3\sqrt{2}", "‚ë£").
* **Location:** Look at the end of the solution or inside the `[[GOAL]]` section.
* **Processing (DB):** Put this extracted value into the 'correct_answer' column.
---

# [PART 1] DETAILED EXTRACTION PROTOCOLS

## SECTION A: TEACHER'S DECODING (Teacher's View)
**CRITICAL CHANGE:** For the following tags, you MUST use the format: `Symbol | Content | AI_Interpretation`
* **Symbol:** The mark used by the teacher (e.g., üéØ, ‚ö°, ‚ùó, üîë, ‚ë†, (Í∞Ä)).
* **Content:** The verbatim handwritten text next to the symbol.
* **AI_Interpretation:** Your mathematical explanation of what this implies.

### 1. NECESSITY (ÌïÑÏó∞ÏÑ±) - `[[NECESSITY]]`
* **Trigger:** `[...]` or `(ÌïÑ)`.
* **Format:** `[Symbol] | [Text inside brackets] | [Why is this necessary?]`

### 2. KEY IDEA (ÌïµÏã¨) - `[[KEY_IDEA]]`
* **Trigger:** `„äÑ`, `(Ìïµ)`, or `üîë`.
* **Format:** `[Symbol] | [Text] | [What theorem/concept is used?]`

### 3. SPECIAL POINT (ÌäπÏù¥Ï†ê) - `[[SPECIAL_POINT]]`
* **Trigger:** `„äï`, `(Ìäπ)`, or `‚ùó`.
* **Format:** `[Symbol] | [Text] | [Why is this a trap/special case?]`

### 4. GOAL (Íµ¨ÌïòÎäî Î™©Ìëú) - `[[GOAL]]`
* **Trigger:** `„äà`, `(Íµ¨)`, or `üéØ`.
* **Format:** `[Symbol] | [Text] | [What is the final target variable?]`

### 5. CONDITIONS (Ï°∞Í±¥) - `[[CONDITIONS]]`
* **Trigger:** `‚ë†`, `‚ë°`, `(Í∞Ä)`, `(ÎÇò)`, or `‚ö°`.
* **Format:** `[Symbol] | [Text] | [Mathematical translation of condition]`

---

## SECTION B: BODY CONTENT & SUPPLEMENTARY

### 1. ACTION PROTOCOL - `[[ACTION_PROTOCOL]]`
* **Target:** Logical arrows (`->`). Format: `**[Trigger]** ... -> **[Action]** ...`

### 2. STRATEGY - `[[STRATEGY]]`
* **Target:** Overall workflow. Substitute ‚ë† with actual meaning.

### 3. PRACTICAL CONCEPTS - `[[PRACTICAL_CONCEPTS]]`
* **Trigger:** `„â¶` or `(Ïã§)`. Format: `Title: ... || Content: ...`

### 4. BASIC CONCEPTS - `[[BASIC_CONCEPTS]]`
* **Trigger:** `„äÇ` or `(Í∏∞)`. Basic definitions used.

### 5. FIGURE ANALYSIS - `[[FIGURE_ANALYSIS]]`
* **Target:** Description of graphs or geometric figures.

### 6. VERBATIM - `[[VERBATIM]]`
* **Target:** ALL handwriting. Strict LaTeX. No Korean inside `$`.

### 7. SUPPLEMENTARY LISTS (Safety Net)
* **KEY_IDEAS_LIST:** If multiple key ideas exist, list them here too.
* **SPECIAL_POINTS_LIST:** If multiple special points exist, list them here too.

---

# [PART 2] OUTPUT FORMAT (STRICT TAG SYSTEM)

**Generate output strictly in KOREAN.**

[[NECESSITY_START]]
(Format: Symbol | Content | Interpretation)
[[NECESSITY_END]]

[[KEY_IDEA_START]]
(Format: Symbol | Content | Interpretation)
[[KEY_IDEA_END]]

[[KEY_IDEAS_LIST_START]]
(Supplementary list for safety)
[[KEY_IDEAS_LIST_END]]

[[SPECIAL_POINT_START]]
(Format: Symbol | Content | Interpretation)
[[SPECIAL_POINT_END]]

[[SPECIAL_POINTS_LIST_START]]
(Supplementary list for safety)
[[SPECIAL_POINTS_LIST_END]]

[[GOAL_START]]
(Format: Symbol | Content | Interpretation)
[[GOAL_END]]

[[CONDITIONS_START]]
(Format: Symbol | Content | Interpretation)
[[CONDITIONS_END]]

[[ACTION_PROTOCOL_START]]
(AI's inferred logic)
[[ACTION_PROTOCOL_END]]

[[STRATEGY_START]]
(Workflow summary)
[[STRATEGY_END]]

[[PRACTICAL_CONCEPTS_START]]
(Format: Title: ... || Content: ...)
[[PRACTICAL_CONCEPTS_END]]

[[BASIC_CONCEPTS_START]]
(Basic definitions)
[[BASIC_CONCEPTS_END]]

[[FIGURE_ANALYSIS_START]]
(Graph description)
[[FIGURE_ANALYSIS_END]]

[[VERBATIM_START]]
(Pixel-perfect transcription)
[[VERBATIM_END]]

[[AI_SOLUTION_START]]
(Standard solution)
[[AI_SOLUTION_END]]

[[DEEP_INSIGHT_START]]
(1-Tier Instructor's Insight)
[[DEEP_INSIGHT_END]]
"""

gpt_client = None
if OPENAI_API_KEY and OPENAI_API_KEY.startswith("sk-"):
    try: gpt_client = OpenAI(api_key=OPENAI_API_KEY)
    except: pass

CURRENT_KEY_INDEX = 0

def initialize_api():
    global CURRENT_KEY_INDEX
    if not GOOGLE_API_KEYS: return
    genai.configure(api_key=GOOGLE_API_KEYS[CURRENT_KEY_INDEX])

def rotate_api_key():
    global CURRENT_KEY_INDEX
    if len(GOOGLE_API_KEYS) <= 1: return False
    prev_index = CURRENT_KEY_INDEX
    CURRENT_KEY_INDEX = (CURRENT_KEY_INDEX + 1) % len(GOOGLE_API_KEYS)
    genai.configure(api_key=GOOGLE_API_KEYS[CURRENT_KEY_INDEX])
    print(f"\nüîÑ [Quota] API Key ÍµêÏ≤¥ ÏôÑÎ£å (Key {prev_index + 1} -> Key {CURRENT_KEY_INDEX + 1})")
    return True

initialize_api()
REQUEST_OPTIONS = {"timeout": 600}

# [Î™®Îç∏ ÏÑ§Ï†ï]
analysis_model = genai.GenerativeModel(model_name=MODEL_NAME_ANALYSIS, system_instruction=TAGGED_SYSTEM_PROMPT)
search_model = genai.GenerativeModel(model_name=MODEL_NAME_OCR, system_instruction=OCR_SYSTEM_PROMPT, generation_config={"temperature": 0.0})
insight_model = genai.GenerativeModel(model_name=MODEL_NAME_ANALYSIS, system_instruction=INSIGHT_SYSTEM_PROMPT)
concept_model = genai.GenerativeModel(model_name=MODEL_NAME_ANALYSIS, system_instruction=CONCEPT_SYSTEM_PROMPT)

def encode_image_to_base64(image_path):
    with open(image_path, "rb") as f: return base64.b64encode(f.read()).decode('utf-8')

# ==========================================================
# [JSON Helper Functions] - [Î≥µÍµ¨ ÏôÑÎ£å] ÏÇ≠Ï†úÎêòÏóàÎçò Ìï®ÏàòÎì§ 100% Î≥µÍµ¨
# ==========================================================
def clean_json_text(text):
    if not text: return ""
    text = re.sub(r'```json\s*', '', text)
    text = re.sub(r'```', '', text)
    start_idx = text.find('{')
    end_idx = text.rfind('}')
    if start_idx != -1 and end_idx != -1:
        text = text[start_idx : end_idx+1]
    return text.strip()

def repair_json_content(text):
    if not text: return "{}"
    text = re.sub(r'(?<!\\)\\(?!["\\/bfnrtu])', r'\\\\', text)
    def replace_newlines_in_string(match):
        content = match.group(1).replace('\n', '\\n').replace('\r', '')
        return f'"{content}"'
    try:
        text = re.sub(r'"\s*:\s*"([^"\\]*(?:\\.[^"\\]*)*)"', replace_newlines_in_string, text, flags=re.DOTALL)
    except: pass
    return text

def try_advanced_parsing(text):
    try: return json.loads(text)
    except:
        try: return json.loads(repair_json_content(text))
        except:
            try:
                py_text = text.replace("true", "True").replace("false", "False").replace("null", "None")
                return ast.literal_eval(py_text)
            except: return None

def parse_broken_json(text):
    # [Î≥µÍµ¨ ÏôÑÎ£å] ÌòπÏãú Î™®Î•º JSON Ìè¨Îß∑ ÏóêÎü¨ Ïãú Í∞ïÏ†ú Ï∂îÏ∂úÏö© Ìï®Ïàò
    print("‚ö† [Warning] JSON ÌååÏã± Ïã§Ìå®. Ï†ïÍ∑úÌëúÌòÑÏãùÏúºÎ°ú Í∞ïÏ†ú Ï∂îÏ∂úÏùÑ ÏãúÎèÑÌï©ÎãàÎã§.")
    fallback_data = {
        "db_columns": {"necessity": "", "key_idea": "", "special_point": ""},
        "body_content": {}
    }
    patterns = {
        "necessity": r'"necessity"\s*:\s*"([^"]+)"',
        "key_idea": r'"key_idea"\s*:\s*"([^"]+)"',
        "goal": r'"goal"\s*:\s*"([^"]+)"',
        "ai_solution": r'"ai_solution"\s*:\s*"([^"]+)"'
    }
    for key, pat in patterns.items():
        match = re.search(pat, text)
        if match:
            val = match.group(1)
            if key in ["necessity", "key_idea"]:
                fallback_data["db_columns"][key] = val
            else:
                fallback_data["body_content"][key] = val
    return fallback_data

# ==========================================================
# [The Monster Parser V30] Manual & Verbose Extraction (Restored & Enhanced)
# ==========================================================
def parse_tagged_response(text):
    print("üöú [Parser V30] Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú ÏãúÏûë (Manual & Verbose Mode)...")
    
    data = {
        "db_columns": {"necessity": "", "key_idea": "", "special_point": ""},
        "body_content": {
            "teacher_decoding": [], # [Ïã†Í∑ú] ÏÑ†ÏÉùÎãòÏùò ÏãúÏÑ† ÌÜµÌï©
            "conditions": [], "special_points": [], "key_ideas": [], # [Î≥µÍµ¨] Legacy Î¶¨Ïä§Ìä∏ ÌïÑÎìú
            "basic_concepts": [], "practical_concepts": [], 
            "figure_analysis": "", "verbatim_handwriting": "", 
            "ai_solution": "", "instructor_solution": "",
            "strategy_overview": "", "action_protocol": ""
        }
    }

    # [Verbose Extraction Helper] - [Î≥µÍµ¨ ÏôÑÎ£å] Î°úÍ∑∏ Î∞è Î∂ÄÎ∂Ñ Îß§Ïπ≠ Í∏∞Îä•
    def extract_section(start_tag, end_tag, debug_name):
        pattern = re.escape(start_tag) + r"(.*?)" + re.escape(end_tag)
        match = re.search(pattern, text, re.DOTALL)
        if match: return match.group(1).strip()
        
        # Fallback: Î£®Ï¶à Îß§Ïπ≠
        pattern_loose = re.escape(start_tag) + r"(.*)"
        match_loose = re.search(pattern_loose, text, re.DOTALL)
        if match_loose:
            content = match_loose.group(1).strip()
            next_tag_match = re.search(r'\[\[.*?_START\]\]', content)
            if next_tag_match: return content[:next_tag_match.start()].strip()
            return content
        return ""

    def clean_list(raw_text):
        if not raw_text: return []
        lines = raw_text.split('\n')
        cleaned = []
        for line in lines:
            line = re.sub(r'^[\s\*\-\d\.]+', '', line).strip() 
            if line: cleaned.append(line)
        return cleaned

    # [Î≥µÍµ¨ ÏôÑÎ£å] Ï§ëÎ≥µ Ï†úÍ±∞ Î∞è Î≥ëÌï© Î°úÏßÅ (Î¶¨Ïä§Ìä∏ Ìï©ÏπòÍ∏∞Ïö©)
    def merge_and_deduplicate(single_text, list_text):
        items = []
        # 1. Single text Ï≤òÎ¶¨
        if single_text: items.append(single_text)
        # 2. List text Ï≤òÎ¶¨
        if list_text:
            lines = clean_list(list_text)
            for line in lines:
                is_duplicate = False
                for existing in items:
                    if line in existing or existing in line: # Ìè¨Ìï® Í¥ÄÍ≥Ñ ÌôïÏù∏
                        is_duplicate = True
                        break
                if not is_duplicate:
                    items.append(line)
        return "\n".join(items)

    # [V30 New Logic] ÌååÏù¥ÌîÑ(|) Íµ¨Î∂Ñ Teacher's Decoding ÌååÏÑú
    def parse_teacher_decoding(raw_text, item_type):
        items = []
        if not raw_text: return items
        lines = raw_text.split('\n')
        for line in lines:
            if not line.strip(): continue
            parts = [p.strip() for p in line.split('|')]
            
            symbol = parts[0] if len(parts) > 0 else "Note"
            content = parts[1] if len(parts) > 1 else parts[0]
            ai_comment = parts[2] if len(parts) > 2 else ""
            
            if len(parts) == 1: 
                symbol = "Note"
                content = line
            
            items.append({
                "type": item_type,
                "symbol": symbol,
                "content": content,
                "ai_comment": ai_comment
            })
        return items

    # --- 1. Extraction & Integration (Dual Processing) ---
    print("  >> ÏÑ†ÏÉùÎãòÏùò ÏãúÏÑ†(Decoding) Ï∂îÏ∂ú Î∞è Î†àÍ±∞Ïãú Îç∞Ïù¥ÌÑ∞ Î≥ëÌï© Ï§ë...")
    decoding_list = []
    
    # 1. Necessity
    raw_nec = extract_section("[[NECESSITY_START]]", "[[NECESSITY_END]]", "Necessity")
    decoding_list.extend(parse_teacher_decoding(raw_nec, "ÌïÑÏó∞ÏÑ±"))
    data["db_columns"]["necessity"] = raw_nec.replace("|", " ").replace("\n", " ")

    # 2. Key Idea (Merge Single + List Tag) [Î≥µÍµ¨Îêú Î°úÏßÅ]
    k1 = extract_section("[[KEY_IDEA_START]]", "[[KEY_IDEA_END]]", "KeyIdea(Single)")
    k2 = extract_section("[[KEY_IDEAS_LIST_START]]", "[[KEY_IDEAS_LIST_END]]", "KeyIdea(List)")
    raw_key_merged = merge_and_deduplicate(k1, k2)
    
    # ÌÜµÌï©Îêú ÌÖçÏä§Ìä∏Î•º Decoding ListÏóêÎèÑ ÎÑ£Í≥†, DB Ïª¨ÎüºÏóêÎèÑ ÎÑ£Ïùå (Ïù¥Ï§ë Ï†ÄÏû•)
    decoding_list.extend(parse_teacher_decoding(raw_key_merged, "ÌïµÏã¨ ÏïÑÏù¥ÎîîÏñ¥"))
    data["db_columns"]["key_idea"] = raw_key_merged.replace("|", " ").replace("\n", " ")
    data["body_content"]["key_ideas"] = clean_list(k2) # [Legacy List Î≥¥Ï°¥]

    # 3. Special Point (Merge Single + List Tag) [Î≥µÍµ¨Îêú Î°úÏßÅ]
    s1 = extract_section("[[SPECIAL_POINT_START]]", "[[SPECIAL_POINT_END]]", "SpecialPoint(Single)")
    s2 = extract_section("[[SPECIAL_POINTS_LIST_START]]", "[[SPECIAL_POINTS_LIST_END]]", "SpecialPoint(List)")
    raw_sp_merged = merge_and_deduplicate(s1, s2)
    
    decoding_list.extend(parse_teacher_decoding(raw_sp_merged, "ÌäπÏù¥Ï†ê"))
    data["db_columns"]["special_point"] = raw_sp_merged.replace("|", " ").replace("\n", " ")
    data["body_content"]["special_points"] = clean_list(s2) # [Legacy List Î≥¥Ï°¥]

    # 4. Goal
    raw_goal = extract_section("[[GOAL_START]]", "[[GOAL_END]]", "Goal")
    decoding_list.extend(parse_teacher_decoding(raw_goal, "Íµ¨ÌïòÎäî Î™©Ìëú"))
    data["body_content"]["goal"] = raw_goal

    # 5. Conditions
    raw_cond = extract_section("[[CONDITIONS_START]]", "[[CONDITIONS_END]]", "Conditions")
    decoding_list.extend(parse_teacher_decoding(raw_cond, "Ï°∞Í±¥"))
    data["body_content"]["conditions"] = clean_list(raw_cond) # [Legacy List Î≥¥Ï°¥]
    
    # ÏµúÏ¢Ö ÌÜµÌï© Î¶¨Ïä§Ìä∏ Ï†ÄÏû•
    data["body_content"]["teacher_decoding"] = decoding_list

    # --- 2. Body Content Extraction (Restored All Fields) ---
    print("  >> Î≥∏Î¨∏ ÏΩòÌÖêÏ∏† Î∞è Í∑∏ÎûòÌîÑ/Í∏∞Î≥∏Í∞úÎÖê Ï∂îÏ∂ú Ï§ë...")
    data["body_content"]["verbatim_handwriting"] = extract_section("[[VERBATIM_START]]", "[[VERBATIM_END]]", "Verbatim")
    data["body_content"]["ai_solution"] = extract_section("[[AI_SOLUTION_START]]", "[[AI_SOLUTION_END]]", "AI Solution")
    data["body_content"]["instructor_solution"] = extract_section("[[DEEP_INSIGHT_START]]", "[[DEEP_INSIGHT_END]]", "Insight")
    
    data["body_content"]["strategy_overview"] = extract_section("[[STRATEGY_START]]", "[[STRATEGY_END]]", "Strategy")
    data["body_content"]["action_protocol"] = extract_section("[[ACTION_PROTOCOL_START]]", "[[ACTION_PROTOCOL_END]]", "ActionProtocol")
    
    # [Î≥µÍµ¨] Figure Analysis & Basic Concepts
    data["body_content"]["figure_analysis"] = extract_section("[[FIGURE_ANALYSIS_START]]", "[[FIGURE_ANALYSIS_END]]", "Figure")
    data["body_content"]["basic_concepts"] = clean_list(extract_section("[[BASIC_CONCEPTS_START]]", "[[BASIC_CONCEPTS_END]]", "BasicConcepts"))

    # Ïã§Ï†ÑÍ∞úÎÖê Ï†ïÏ†ú
    pc_raw = extract_section("[[PRACTICAL_CONCEPTS_START]]", "[[PRACTICAL_CONCEPTS_END]]", "PracticalConcepts")
    pc_list = []
    if pc_raw:
        lines = pc_raw.split('\n')
        for line in lines:
            parts = re.split(r'\|\|', line)
            if len(parts) >= 2:
                title_part = parts[0].replace("Title:", "").strip()
                content_part = parts[1].replace("Content:", "").strip()
                if title_part:
                    pc_list.append({"title": title_part, "content": content_part})
    data["body_content"]["practical_concepts"] = pc_list

    return data

def execute_with_key_rotation(model, content, **kwargs):
    if "safety_settings" not in kwargs:
        kwargs["safety_settings"] = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        ]

    max_attempts = len(GOOGLE_API_KEYS) + 1 
    for attempt in range(max_attempts):
        try:
            return model.generate_content(content, **kwargs)
        except (ResourceExhausted, InternalServerError, ServiceUnavailable) as e:
            print(f"‚ö† Gemini API Error. ÌÇ§ ÍµêÏ≤¥ ÏãúÎèÑ... ({e})")
            if rotate_api_key(): time.sleep(2); continue 
            else: raise e
        except Exception as e:
            if "429" in str(e) or "quota" in str(e).lower():
                print(f"‚ö† 429 Rate Limit. ÌÇ§ ÍµêÏ≤¥ ÏãúÎèÑ...")
                if rotate_api_key(): time.sleep(2); continue
            raise e 
    raise Exception("All Gemini Keys Exhausted")

def analyze_image_structure(image_path):
    import concept_manager
    raw_concepts = concept_manager.load_concepts()
    concept_list_text = "\n".join([f"- {c['title']}: {c.get('content','...')[:50]}" for c in raw_concepts[:50]])
    
    print("\n--- [Stage 1] Íµ¨Ï°∞ Î∂ÑÏÑù (V30 Forensic Juggernaut Mode) ÏãúÏûë ---")
    
    user_prompt = f"""
    Analyze the image strictly according to the 'Forensic Auditor' protocol.
    Fill all the tags in KOREAN.
    
    [Reference Concept DB Context]
    {concept_list_text}
    """
    
    try:
        response = execute_with_key_rotation(
            analysis_model, 
            [user_prompt, Image.open(image_path)],
            generation_config={"max_output_tokens": 8192, "temperature": 0.1},
            request_options=REQUEST_OPTIONS
        )
        result_data = parse_tagged_response(response.text)
        
        # [Í≤ÄÏ¶ù Î°úÏßÅ Î≥¥Ï°¥]
        body = result_data["body_content"]
        if not body["verbatim_handwriting"]:
            if body["strategy_overview"] or body["ai_solution"]:
                print("‚ö†Ô∏è [Warn] ÏÜêÍ∏ÄÏî®(Verbatim)Îäî ÏóÜÏßÄÎßå, Ìï¥ÏÑ§/Ï†ÑÎûµÏù¥ ÏûàÏñ¥ ÌÜµÍ≥ºÌï©ÎãàÎã§.")
            else:
                print("‚ùå [Error] Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞ Ï†ÑÎ¨¥ (Verbatim/Strategy/Solution Î™®Îëê ÏóÜÏùå).")
                raise Exception("Critical Extraction Failed (Empty Data)")
            
        print("‚úÖ [Plan A] ÌÉúÍ∑∏ ÌååÏã± ÏÑ±Í≥µ")

    except Exception as e:
        print(f"‚ö† [Plan A Ïã§Ìå®] {e}. [Plan B] GPT Ïö©Î≥ë Ìà¨ÏûÖ Ïó¨Î∂Ä ÌôïÏù∏...")
        return None

    print("\n--- [Stage 2] Ïã¨Ï∏µ Ï∂îÎ°† ÏãúÏûë ---")
    concept_db_text = json.dumps(concept_manager.load_concepts(), ensure_ascii=False, indent=2)
    base_sol = result_data.get("body_content", {}).get("ai_solution", "")
    
    # [Insight ÏÉùÏÑ± Î°úÏßÅ Î≥¥Ï°¥]
    if not result_data["body_content"]["instructor_solution"]:
        deep_insight = generate_deep_insight(image_path, base_sol, concept_db_text) 
        result_data["body_content"]["instructor_solution"] = deep_insight
    
    print("‚úÖ Î∂ÑÏÑù ÏôÑÎ£å.")
    return result_data

def generate_deep_insight(image_path, base_solution_text, concept_db_text):
    final_prompt = INSIGHT_SYSTEM_PROMPT.replace("[USER_CONCEPT_DB]", concept_db_text)
    user_prompt = f"[Standard Solution]:\n{base_solution_text}\n\nBased on the image and solution, provide the 1-Tier Insight."
    try:
        dynamic_insight_model = genai.GenerativeModel(model_name=MODEL_NAME_ANALYSIS, system_instruction=final_prompt)
        response = execute_with_key_rotation(
            dynamic_insight_model, [user_prompt, Image.open(image_path)], request_options=REQUEST_OPTIONS
        )
        return response.text.strip()
    except: return "Ïã¨Ï∏µ Î∂ÑÏÑù ÏÉùÏÑ± Ïã§Ìå®"

def call_gpt4o_fallback(system_instr, user_instr, image_path):
    return None

def get_pure_ocr_text(image_path):
    try:
        response = execute_with_key_rotation(
            search_model, ["Execute OCR.", Image.open(image_path)], request_options=REQUEST_OPTIONS
        )
        return response.text.replace("```latex", "").replace("```", "").strip()
    except: return None

def extract_concepts_flexible(image_path):
    try:
        response = execute_with_key_rotation(
            concept_model, ["Extract concepts strictly in JSON.", Image.open(image_path)],
            generation_config={"response_mime_type": "application/json"}, request_options=REQUEST_OPTIONS
        )
        cleaned = clean_json_text(response.text)
        return json.loads(cleaned)
    except: return None
    
# [NEW] AÎã®Í≥Ñ(Îã®ÏàúÍ≥ÑÏÇ∞) ÌåêÎèÖÍ∏∞ - Track B Ï†ÑÏö©
def check_is_basic_drill(text):
    if not text or len(text) < 5: return False
    try:
        # Flash Î™®Îç∏ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Îπ†Î•¥Í≤å ÌåêÎã® (True/False)
        prompt = f"""
        Role: Math Problem Classifier.
        Task: Analyze the text and determine if it is a "Simple Calculation Drill" (A-step,Îã®Ïàú Ïó∞ÏÇ∞,Í∏∞Ï¥à Î¨∏Ï†ú).
        Text: {text[:500]}
        Output: Return ONLY 'TRUE' if it is a simple drill, 'FALSE' otherwise.
        """
        resp = execute_with_key_rotation(search_model, [prompt], request_options=REQUEST_OPTIONS)
        return "TRUE" in resp.text.strip().upper()
    except: return False

# [NEW] ÏãúÎåÄÏù∏Ïû¨Í∏â ÎÇúÏù¥ÎèÑ ÌåêÎèÖÍ∏∞ (Level 1~4 Classifier)
def analyze_difficulty_level(text):
    if not text: return "Í∏∞Î≥∏Í∞úÎÖê"
    try:
        # Flash Î™®Îç∏ÏóêÍ≤å 'Î¨∏Ï†úÏùò Í¥ÄÏÉÅ(Heuristics)'ÏùÑ Î≥¥Í≥† ÌåêÎã®ÌïòÎùºÍ≥† ÏßÄÏãú
        prompt = f"""
        Role: Math Problem Difficulty Classifier.
        Task: Classify the difficulty of the given math problem text into one of 4 levels.
        
        [Criteria]
        1. LEVEL_1 (Basic Concept): Short text (1-3 lines), asks for simple calculation or basic definition.
        2. LEVEL_2 (Entry Semi-Killer): Standard 4-point problem. Has 1-2 conditions. Typical textbook style.
        3. LEVEL_3 (Deep Semi-Killer): Hard 4-point. Keywords: "Defined function g(x)", "Differentiability", "Select all correct („Ñ±,„Ñ¥,„Ñ∑)", "Fill in the blank". Requires logical deduction.
        4. LEVEL_4 (Killer): Very long text, complex conditions, new function definitions, finding Max/Min in complex situations. 

        Input Text:
        {text[:800]}

        Output: ONLY return one word: "LEVEL_1", "LEVEL_2", "LEVEL_3", or "LEVEL_4".
        """
        resp = execute_with_key_rotation(search_model, [prompt], request_options=REQUEST_OPTIONS)
        result = resp.text.strip().upper()
        
        if "LEVEL_4" in result: return "ÌÇ¨Îü¨"
        if "LEVEL_3" in result: return "Ï§ÄÌÇ¨Îü¨_Ïã¨Ìôî"
        if "LEVEL_2" in result: return "Ï§ÄÌÇ¨Îü¨_ÏßÑÏûÖ"
        return "Í∏∞Î≥∏Í∞úÎÖê"
    except: return "Í∏∞Î≥∏Í∞úÎÖê"