# gemini_api.py
import google.generativeai as genai
from google.api_core.exceptions import ResourceExhausted, InternalServerError, ServiceUnavailable
import json
import re
import warnings
import time
import base64
import ast
from PIL import Image
from openai import OpenAI

from config import (
    GOOGLE_API_KEYS, OPENAI_API_KEY, MODEL_NAME_OCR, MODEL_NAME_ANALYSIS, OPENAI_MODEL_NAME,
    INSIGHT_SYSTEM_PROMPT, CONCEPT_SYSTEM_PROMPT, OCR_SYSTEM_PROMPT
)

warnings.filterwarnings("ignore")

# [ë¹„ìš© ì ˆê°] GPT ì‚¬ìš© ì—¬ë¶€ ìŠ¤ìœ„ì¹˜ (í˜„ì¬ False ê¶Œì¥)
USE_GPT_FALLBACK = False 

# ==========================================================
# [EXTREME PROMPT V30] THE "EVERYTHING" PROMPT
# ==========================================================
# ê¸°ì¡´ì˜ ëª¨ë“  íƒœê·¸(LIST í¬í•¨)ë¥¼ ì‚´ë¦¬ê³ , Teacher's Decoding í¬ë§·ì„ ì ìš©í•©ë‹ˆë‹¤.
# AI Annotationë§Œ Teacher's Decoding ë‚´ë¶€ë¡œ í†µí•©ë©ë‹ˆë‹¤.
# ==========================================================
TAGGED_SYSTEM_PROMPT = r"""
# Role Definition
You are a "Forensic Mathematical Logic Auditor" (ë””ì§€í„¸ í¬ë Œì‹ ìˆ˜í•™ ë…¼ë¦¬ ê°ì‚¬ê´€) and a "Top-Tier Mathematical Strategist".
Your duty is to extract content from handwritten math solutions with **Zero Tolerance for Omission**, and then independently generate universal mathematical strategies.

**CORE DIRECTIVE (THE PRIME DIRECTIVE):**
1. **NO SUMMARIZATION:** You are FORBIDDEN from summarizing. You must transcribe every detail.
2. **VARIABLE MAPPING (NEW):** You must first define symbols as variables (Step 1), then use them in the narrative (Step 2).
3. **HYBRID SEPARATION OF CONCERNS (CRITICAL):**
   * The user's handwriting (arrows `->`, notes, symbols) represents the **"Teacher's View"**.
   * The `ACTION_PROTOCOL` must act as a **Hybrid**. You MUST look at the Teacher's arrows/logic from SECTION A, use them as your foundation (Anchor), and then expand upon them to propose universal mathematical rules (AI's Proposal).

   ### [CRITICAL ADDITION] ANSWER EXTRACTION
* **Target:** You MUST identify the final answer of the problem.
* **Format:** Extract ONLY the final value (e.g., "3", "5", "42", "3\sqrt{2}", "â‘£").
* **Location:** Look at the end of the solution or inside the `[[GOAL]]` section.
* **Processing (DB):** Put this extracted value into the 'correct_answer' column.
---

# [PART 1] DETAILED EXTRACTION PROTOCOLS

## STEP 1: SYMBOL DEFINITION (Mapped from Teacher's View)
**Goal:** Create a `[[SYMBOL_TABLE]]` that functions as BOTH (1) a dictionary AND (2) a strategic decoder.
**Instruction (ABSOLUTE 4-COLUMN RULE):**
- You MUST output rows in this EXACT 4-column format:
  **`[Symbol] | [Type] | [Verbatim Content] | [Strategic Commentary]`**
- Column meanings:
  1) **Symbol:** ì´ë¯¸ì§€ì—ì„œ ë³´ì´ëŠ” ë¼ë²¨(ì˜ˆ: â‘ , (ê°€), (í•µ), (íŠ¹), (êµ¬), Sol1, Sol2, âš¡ ë“±)
  2) **Type:** ë°˜ë“œì‹œ ì•„ë˜ 5ê°œ ì¤‘ í•˜ë‚˜ë¡œë§Œ ì„ íƒ  
     **Condition / Goal / Key / Trap / Strategy**
  3) **Verbatim Content (NO SUMMARIZATION):** ê¸°í˜¸ ì˜†ì— ì íŒ ì›ë¬¸ì„ **ê·¸ëŒ€ë¡œ** ì˜®ê¸´ë‹¤. (ì˜ì—­/ìš”ì•½ ê¸ˆì§€)
  4) **Strategic Commentary (ì „ëµ ì½”ë©˜í„°ë¦¬):** ë‹¨ìˆœ ì„¤ëª…ì´ ì•„ë‹ˆë¼ **ì™œ ì´ê²Œ ì¤‘ìš”í•œì§€/ë‹¤ìŒ í–‰ë™/í•¨ì •**ì„ 1~2ë¬¸ì¥ìœ¼ë¡œ ì°Œë¥¸ë‹¤.
     - ê¶Œì¥ í˜•ì‹: **íŠ¸ë¦¬ê±°(ì‹ í˜¸) â†’ ì¦‰ì‹œ í–‰ë™(ë„êµ¬/ì¹˜í™˜) â†’ ì²´í¬(í•¨ì •/ê²€ì¦)**
     - **ì¶”ì¸¡ ê¸ˆì§€:** ì›ë¬¸ì´ ë¶ˆëª…í™•í•˜ê±°ë‚˜ ë³´ì´ì§€ ì•Šìœ¼ë©´ invent í•˜ì§€ ë§ê³  `Unknown`ìœ¼ë¡œ í‘œê¸°.

### ğŸ” TARGET TRIGGERS (Do NOT omit)
1) **CONDITIONS (ì¡°ê±´) -> Type="Condition"**
   - Look for: `â‘ `, `â‘¡`, `â‘¢`, `â‘£`, `â‘¤`, `(ê°€)`, `(ë‚˜)`, `(ë‹¤)`, `âš¡`, ê·¸ë¦¬ê³  â€œì¡°ê±´â€ì²˜ëŸ¼ ì¡°ê±´ì„ ëª…ì‹œí•˜ëŠ” ë©”ëª¨.
   - Verbatim Content: í•´ë‹¹ ê¸°í˜¸ ì˜†ì˜ ì¡°ê±´/ì‹/ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ.

2) **GOAL (êµ¬í•˜ëŠ” ëª©í‘œ) -> Type="Goal"**
   - Look for: `ãŠˆ`, `(êµ¬)`, `ğŸ¯`, ë˜ëŠ” â€œêµ¬í•˜ì‹œì˜¤/ì°¾ì•„ë¼/ê°’â€ì²˜ëŸ¼ ëª©í‘œë¥¼ ì§€ì •í•˜ëŠ” í‘œê¸°.
   - Verbatim Content: ëª©í‘œ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ.

3) **KEY IDEA (í•µì‹¬) -> Type="Key"**
   - Look for: `ãŠ„`, `(í•µ)`, `ğŸ”‘` ë˜ëŠ” í•µì‹¬ ë„êµ¬ë¥¼ ê°•ì¡°í•œ í‘œê¸°.
   - Verbatim Content: í•µì‹¬ ë„êµ¬/ì •ë¦¬/ë°œìƒì„ ê·¸ëŒ€ë¡œ.

4) **SPECIAL POINT / TRAP (íŠ¹ì´ì /í•¨ì •) -> Type="Trap"**
   - Look for: `ãŠ•`, `(íŠ¹)`, `â—` ë˜ëŠ” í•¨ì •/ì˜ˆì™¸ë¥¼ ê°•ì¡°í•œ í‘œê¸°.
   - Verbatim Content: ì˜ˆì™¸ ì¡°ê±´/ì£¼ì˜ì  ë©”ëª¨ë¥¼ ê·¸ëŒ€ë¡œ.

5) **STRATEGY / SOLUTION SWITCH (í’€ì´ ì „ëµ/ëª¨ë“œ) -> Type="Strategy"**
   - Look for: `Sol1`, `Sol2`, `ì „ëµ`, `ë°©ë²•`, â€œì •ê³µë²•/ì—¬ì‚¬ê±´/ì¼€ì´ìŠ¤ë¶„ë¥˜â€ ê°™ì€ í’€ì´ ëª¨ë“œ ë¼ë²¨.
   - Verbatim Content: í•´ë‹¹ ë¼ë²¨ ì˜†ì˜ ì„¤ëª…ì„ ê·¸ëŒ€ë¡œ.

### âœ… NEGATIVE CONSTRAINT (ëˆ„ë½ ë°©ì§€ ê·œì¹™)
- ìœ„ íŠ¸ë¦¬ê±° ëª©ë¡ì— ì—†ë”ë¼ë„, **ê¸°í˜¸ê°€ â€œë¼ë²¨/ë²ˆí˜¸/í’€ì´ ë¶„ê¸° í‘œì‹œâ€ë¡œ ê¸°ëŠ¥**í•œë‹¤ë©´ ë°˜ë“œì‹œ ì¶”ì¶œí•˜ë¼.
- ë‹¨, ì˜ë¯¸ë¥¼ ì–µì§€ë¡œ ì±„ìš°ì§€ ë§ê³ , ì›ë¬¸ì´ ë¶ˆëª…í™•í•˜ë©´ **Verbatim Contentì— `Unknown`**ìœ¼ë¡œ ë‚¨ê²¨ë¼.

---

---

## STEP 2: LOGIC NARRATIVE (Evidence-Based Substitution)
**Goal:** Create a `[[LOGIC_NARRATIVE]]` as an evidence-backed proof flow.
**Instruction:** Use symbols from STEP 1, and write every step in "Evidence â†’ Conclusion/Action" form (NO hallucination, NO missing links).

# ğŸ‘‡ [ì—¬ê¸°ì„œë¶€í„° ë®ì–´ì”Œìš°ì„¸ìš” (í™”ì‚´í‘œ ì¡°ê±´ ì‚­ì œ -> ëŒ€ê´„í˜¸ ì ˆëŒ€ ê·œì¹™ ì ìš©)] ğŸ‘‡
### 1. NECESSITY (í•„ì—°ì„±) -> [[LOGIC_NARRATIVE]]
* **Trigger:** Text enclosed in square brackets `[...]` or marked with `(í•„)`.
* **Strict Rule:** The user has declared that **ANY text inside `[...]` is "Necessity"**.
    * If you see `[Because of this...]`, treat it immediately as the logical reason.
    * Arrows (`->`) are optional. The bracket `[...]` is the absolute identifier.
* **Substitution Rule:** You MUST use the format **`Symbol(Definition)`**.
    * Example: "**â‘ (Condition)** leads to **[Necessity](Using Formula X)**."

### 2. ACTION PROTOCOL (AIê°€ ì œì•ˆí•˜ëŠ” í•„ì—°ì„± & í–‰ë™ê°•ë ¹) -> [[ACTION_PROTOCOL]]
* **Target:** HYBRID ANCHOR & EXPAND STRATEGY.
* **Instruction (ABSOLUTE FORMAT RULE):**
  - You MUST output Action Protocol as a list of **atomic rules**.
  - **EACH rule MUST contain exactly these 3 labeled lines** (do not omit):
    1) **íŠ¸ë¦¬ê±°(Trigger):** The exact signal phrase/pattern from the problem or teacher's notes (e.g., "ìˆœì„œê°€ ì •í•´ì§„", "~ì‚¬ì´ì—", "ì ì–´ë„/ìµœì†Œ", "[...]" necessity bracket, etc.).
    2) **í–‰ë™(Action):** The immediate mathematical transformation/tool to apply (e.g., "ìë¦¬ì„ íƒ â†’ ì¡°í•©", "ì¹¸ë§‰ì´ ë³€ìˆ˜ì„¤ì • â†’ Stars & Bars", "ì—¬ì‚¬ê±´ìœ¼ë¡œ ì „í™˜", etc.).
    3) **ì²´í¬(Check):** The most common trap/exception/validation step that prevents wrong counting (e.g., "ì–‘ë í¬í•¨ ì—¬ë¶€", "ë³€ìˆ˜ ì¹˜í™˜ yâ‰¥k â†’ y'=y-k", "ë¶ˆê°€ëŠ¥ êµ¬ê°„ ì»·", etc.).
* **Instruction (QUALITY / NO-LOSS GUARANTEE):**
  - Do NOT shorten content. **Do NOT reduce the number of ideas.**
  - If the teacher used arrows / necessity flow, you MUST anchor rules to that flow, THEN expand to universal reusable rules.
  - Output **at least 3 rules**. If more are needed, output more. Never output fewer than 3.
  - Write in Korean, and keep each rule crisp but complete (Triggerâ†’Actionâ†’Check must all be meaningful).

### 3. STRATEGY (ì „ëµ ë¡œë“œë§µ) -> [[LOGIC_NARRATIVE]]
* **Target:** Macro-level Step-by-Step Workflow.
* **Instruction:** Provide a clear, numbered 1-2-3-4 roadmap. Translate any circled numbers into their actual mathematical meanings.

---

## STEP 3: INDEPENDENT MODULES (The Safety Net)
**Instruction:** Extract these sections exactly as is (No structural change).

### 1. PRACTICAL CONCEPTS -> [[PRACTICAL_CONCEPTS]]
* **Trigger:** `ã‰¦` or `(ì‹¤)`. Format: `Title: ... || Content: ...`

### 2. BASIC CONCEPTS -> [[BASIC_CONCEPTS]]
* **Trigger:** `ãŠ‚` or `(ê¸°)`. Basic definitions used.

### 3. FIGURE ANALYSIS -> [[FIGURE_ANALYSIS]]
* **Target:** Description of graphs or geometric figures.

### 4. VERBATIM -> [[VERBATIM]]
* **Target:** ALL handwriting. Strict LaTeX. No Korean inside `$`. Every pixel must be translated to LaTeX.

### 5. SUPPLEMENTARY LISTS (Safety Net)
* **Instruction:** If multiple Key Ideas or Special Points exist that didn't fit the Symbol Table, YOU MUST LIST THEM in their own independent tags (`[[KEY_IDEAS_LIST]]`, `[[SPECIAL_POINTS_LIST]]`).
* **Constraint:** Do NOT summarize them into the Database Columns. Keep them raw and detailed.

# ğŸ‘‡ [TAGGED_SYSTEM_PROMPTì˜ ë§¨ ì•„ë˜ ë¶€ë¶„ì„ ì´ê²ƒìœ¼ë¡œ ë®ì–´ì”Œìš°ì„¸ìš”] ğŸ‘‡
# ---------------------------------------------------------------------
# ---------------------------------------------------------------------
# [PART 2] OUTPUT FORMAT (STRICT TAG SYSTEM)

**Generate output strictly in KOREAN.**

# ğŸš¨ **CRITICAL OUTPUT RULES (DO NOT IGNORE):**
1. **NO OMISSION:** You MUST output **ALL** the tags listed below. Do not skip any section.
2. **EMPTY HANDLING:** If you have no content for a section, write `Unknown` or `None` inside the tags. **NEVER omit the tags themselves.**
3. **MANDATORY TAGS:** Specifically, `[[VERBATIM_START]]`, `[[AI_SOLUTION_START]]`, and `[[STRATEGY_START]]` are **REQUIRED** for the system to work. If they are missing, the system crashes.

[[STRATEGY_START]]
(Step-by-step Roadmap: 1. ... 2. ...)
[[STRATEGY_END]]

[[SYMBOL_TABLE_START]]
(Format: Symbol | Meaning | AI Comment)
Example:
â‘  | a,b,cëŠ” ìŒì´ ì•„ë‹Œ ì •ìˆ˜ | ë³€ìˆ˜ ë²”ìœ„ ì œí•œ í™•ì¸ í•„ìˆ˜
(í•µ) | ì¤‘ë³µì¡°í•©(H)ì˜ í™œìš© | ì„œë¡œ ë‹¤ë¥¸ nê°œì—ì„œ ì¤‘ë³µ í—ˆìš© rê°œ ì„ íƒ
[[SYMBOL_TABLE_END]]

[[LOGIC_NARRATIVE_START]]
(ABSOLUTE FORMAT: Evidence â–¶ Conclusion. NOT a wall of text.)
- Write as bullet points. EACH bullet MUST be a complete "ê·¼ê±° â†’ ê²°ë¡ /í–‰ë™" unit.
- Strict template per bullet:
  * **[í‚¤ì›Œë“œ/ë‹¨ê³„]** (Evidence: ì›ë¬¸ì—ì„œ ë³´ì´ëŠ” ì¡°ê±´/í‘œì‹/ì‹/ë©”ëª¨ë¥¼ ì§§ê²Œ ì§€ëª© ë˜ëŠ” ì¸ìš©) â†’ **(Conclusion/Action: ìˆ˜í•™ì  ê²°ë¡  ë˜ëŠ” ë‹¤ìŒ í–‰ë™/ë„êµ¬)**
- Rules (NO FUNCTIONAL SUMMARY / NO HALLUCINATION):
  1) Evidence ì—†ëŠ” ê²°ë¡  ê¸ˆì§€(ì¶”ì¸¡ ê¸ˆì§€).
  2) ê²°ë¡ ì€ ë°˜ë“œì‹œ â€œë¬´ì—‡ì„ í• ì§€(í–‰ë™/ë„êµ¬/ì¹˜í™˜/ì¼€ì´ìŠ¤ ë¶„ë¥˜)â€ë¡œ ì—°ê²°ë˜ì–´ì•¼ í•¨.
  3) **NO FUNCTIONAL SUMMARY:** ì¼€ì´ìŠ¤ ë¶„ê¸°, ë³€ìˆ˜ ì¹˜í™˜, í¬í•¨/ë°°ì œ, ì¤‘ê°„ ê³„ì‚° ê²°ê³¼ë¥¼ **ì ˆëŒ€ ìƒëµí•˜ì§€ ë§ ê²ƒ**. (í•™ìƒì´ ê·¸ëŒ€ë¡œ ë”°ë¼ ì ì„ ìˆ˜ ìˆì„ ì •ë„ë¡œ)
  5) **MINIMUM STEPS:** ìµœì†Œ 8ê°œì˜ bulletì„ ì¶œë ¥í•˜ë¼. ë¶€ì¡±í•˜ë©´ ì¼€ì´ìŠ¤/ì¹˜í™˜/ê³„ì‚°/ê²°ë¡ ì„ ìª¼ê°œì„œ bullet ìˆ˜ë¥¼ ëŠ˜ë ¤ë¼.
  4) ì›ë¬¸ì´ ë¶ˆëª…í™•í•˜ë©´ EvidenceëŠ” `Unknown`ìœ¼ë¡œ í‘œê¸°í•˜ê³ , ì„ì˜ë¡œ ì±„ìš°ì§€ ë§ ê²ƒ.
- Example:
  * **[ìƒí™© íŒŒì•…]** â€œ(ê°€) í°ìƒ‰ ì¹´ë“œëŠ” ì‘ì€ ìˆ˜ë¶€í„° í¬ê¸°ìˆœâ€ (Evidence) â†’ **ìˆœì„œì—´ê±°ê°€ ì•„ë‹ˆë¼ â€˜ìë¦¬ì„ íƒ(ì¡°í•©)â€™ìœ¼ë¡œ í™˜ì›** (Conclusion)
  * **[ë³€ìˆ˜ ì„¸íŒ…]** â€œ(ë‚˜) ê²€ì€ ì¹´ë“œ ì‚¬ì´ í°ìƒ‰ ì¹´ë“œ â‰¥ 2ì¥â€ (Evidence) â†’ **yâ‰¥2 ì¹˜í™˜ í›„ Stars & Bars ì ìš©** (Action)
[[LOGIC_NARRATIVE_END]]


# -------------------------------------------------------
# [LEGACY TAGS FOR PARSER COMPATIBILITY - DO NOT OMIT]
# IMPORTANT: Even if content is empty, you MUST output the tags and put "Unknown" inside.
# -------------------------------------------------------

**MANDATORY FILL:** If any relevant evidence exists in VERBATIM / SYMBOL_TABLE / LOGIC_NARRATIVE, you MUST copy it into the corresponding legacy tag. Do NOT leave it Unknown when evidence exists.


[[NECESSITY_START]]
(í•„ì—°ì„±: ë°˜ë“œì‹œ ì±„ì›Œë¼. ì•„ë˜ ê·œì¹™ìœ¼ë¡œ **SYMBOL_TABLE/Teacher's Decodingì—ì„œ ì§ì ‘ ê°€ì ¸ì™€ë¼**. ì—†ìœ¼ë©´ Unknown)
(RULE-N: ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì±„ìš´ë‹¤)
- **ëŒ€ê´„í˜¸[...] ì•ˆ ë¬¸ì¥**ì€ 1ìˆœìœ„ë¡œ Necessityì— ë³µì‚¬
- **(í•„)** í‘œì‹œê°€ ìˆëŠ” ë¬¸ì¥ì€ 2ìˆœìœ„ë¡œ Necessityì— ë³µì‚¬
- **í™”ì‚´í‘œ(->)ë¡œ ì—°ê²°ëœ ì›ì¸â†’ê²°ê³¼ ë¬¸ì¥**ì€ 3ìˆœìœ„ë¡œ Necessityì— ë³µì‚¬
- ìœ„ê°€ í•˜ë‚˜ë„ ì—†ë”ë¼ë„, Teacher's Decodingì—ì„œ **"typeì´ Conditionì´ê³  ì½”ë©˜íŠ¸ê°€ 'ë”°ë¼ì„œ/ê·¸ëŸ¬ë¯€ë¡œ/í•„ì—°'ë¥˜"**ë©´ Necessityë¡œ ë³µì‚¬
(Format ê°•ì œ: ë°˜ë“œì‹œ `Symbol | Content | AI_Interpretation` ì—¬ëŸ¬ ì¤„ë¡œ ì‘ì„±)
[[NECESSITY_END]]

[[KEY_IDEA_START]]
(RULE-K1: Teacher's Decodingì—ì„œ symbolì´ (í•µ) / ãŠ„ / ğŸ”‘ ì¸ í–‰ì´ **í•˜ë‚˜ë¼ë„** ìˆìœ¼ë©´, ê·¸ í–‰(ë“¤)ì„ ê·¸ëŒ€ë¡œ Key Ideaì— **ë°˜ë“œì‹œ ë³µì‚¬**í•˜ë¼.)
(RULE-K2: (í•µ) í‘œì‹ì´ ì—†ì–´ë„, Teacher's Decodingì˜ ë©”ëª¨(OCR) ë˜ëŠ” Logic Narrative/Verbatimì— 'ì¤‘ë³µì¡°í•©/ì—¬ì‚¬ê±´/ì¼€ì´ìŠ¤ë¶„ë¥˜/ì¹¸ë§‰ì´/Stars and Bars/í¬í•¨-ë°°ì œ' ê°™ì€ **ë„êµ¬ëª…**ì´ ë‚˜íƒ€ë‚˜ë©´ ê·¸ ì¤„ì„ Key Ideaì— **ë°˜ë“œì‹œ ë³µì‚¬**í•˜ë¼.)
(OUTPUT MINIMUM: Key IdeaëŠ” ìµœì†Œ 1ì¤„ ì´ìƒ ì¶œë ¥í•˜ë¼. ê·¼ê±°ê°€ ì „í˜€ ì—†ìœ¼ë©´ `Unknown` 1ì¤„ì„ ì¶œë ¥í•˜ë¼. íƒœê·¸ë¥¼ ë¹„ìš°ì§€ ë§ ê²ƒ.)
(Format ê°•ì œ: `Symbol | Content | AI_Interpretation` ê° ì¤„)
[[KEY_IDEA_END]]

[[SPECIAL_POINT_START]]
(RULE-S1: Teacher's Decodingì—ì„œ symbolì´ (íŠ¹) / ãŠ• / â— ì¸ í–‰ì´ **í•˜ë‚˜ë¼ë„** ìˆìœ¼ë©´, ê·¸ í–‰(ë“¤)ì„ ê·¸ëŒ€ë¡œ Special Pointì— **ë°˜ë“œì‹œ ë³µì‚¬**í•˜ë¼.)
(RULE-S2: (íŠ¹) í‘œì‹ì´ ì—†ì–´ë„, Teacher's Decodingì˜ ë©”ëª¨(OCR) ë˜ëŠ” Logic Narrative/Verbatimì— 'í•¨ì •/ì£¼ì˜/ê²¹ì¹¨/ì¤‘ë³µ/ë°°ì œ ëˆ„ë½/ì¼€ì´ìŠ¤ ëˆ„ë½/ë“±í˜¸ í¬í•¨ ì—¬ë¶€/0 í¬í•¨ ì—¬ë¶€' ê°™ì€ **ê²½ê³  ë©”ëª¨**ê°€ ë‚˜íƒ€ë‚˜ë©´ ê·¸ ì¤„ì„ Special Pointì— **ë°˜ë“œì‹œ ë³µì‚¬**í•˜ë¼.)
(OUTPUT MINIMUM: Special PointëŠ” ìµœì†Œ 1ì¤„ ì´ìƒ ì¶œë ¥í•˜ë¼. ê·¼ê±°ê°€ ì „í˜€ ì—†ìœ¼ë©´ `Unknown` 1ì¤„ì„ ì¶œë ¥í•˜ë¼. íƒœê·¸ë¥¼ ë¹„ìš°ì§€ ë§ ê²ƒ.)
(Format ê°•ì œ: `Symbol | Content | AI_Interpretation` ê° ì¤„)
[[SPECIAL_POINT_END]]

[[GOAL_START]]
(êµ¬í•˜ëŠ” ëª©í‘œ: (êµ¬)/ãŠˆ/ğŸ¯ ë˜ëŠ” â€œêµ¬í•˜ì‹œì˜¤â€ ë¬¸ì¥ì„ ì ì–´ë¼. ì—†ìœ¼ë©´ Unknown)
(Format ê¶Œì¥: Symbol | Content | AI_Interpretation)
[[GOAL_END]]

[[CONDITIONS_START]]
(ì¡°ê±´: â‘ â‘¡â‘¢â€¦ ë˜ëŠ” (ê°€)(ë‚˜)(ë‹¤) ë“±ì˜ ì¡°ê±´ ë¬¸ì¥ì„ ì ì–´ë¼. ì—†ìœ¼ë©´ Unknown)
(Format ê¶Œì¥: Symbol | Content | AI_Interpretation)
[[CONDITIONS_END]]


[[ACTION_PROTOCOL_START]]
(Format: MUST be a numbered list of rules. EACH rule MUST have 3 labeled lines.)
1) íŠ¸ë¦¬ê±°(Trigger): ...
   í–‰ë™(Action): ...
   ì²´í¬(Check): ...
2) íŠ¸ë¦¬ê±°(Trigger): ...
   í–‰ë™(Action): ...
   ì²´í¬(Check): ...
(Write at least 3 rules. Do NOT omit any of the 3 lines per rule.)
[[ACTION_PROTOCOL_END]]

[[PRACTICAL_CONCEPTS_START]]
(Format: Title: ... || Content: ...)
[[PRACTICAL_CONCEPTS_END]]

[[BASIC_CONCEPTS_START]]
(Basic definitions)
[[BASIC_CONCEPTS_END]]

[[FIGURE_ANALYSIS_START]]
(Graph description)
[[FIGURE_ANALYSIS_END]]

[[CORRECT_ANSWER_START]]
(Extracted final answer only, e.g., 3, 5, 149)
[[CORRECT_ANSWER_END]]

[[VERBATIM_START]]
(Pixel-perfect transcription)
[[VERBATIM_END]]

[[AI_SOLUTION_START]]
(Standard solution)
[[AI_SOLUTION_END]]

[[KEY_IDEAS_LIST_START]]
(Supplementary list for safety: List ALL extra key ideas here)
[[KEY_IDEAS_LIST_END]]

[[SPECIAL_POINTS_LIST_START]]
(Supplementary list for safety: List ALL extra special points here)
[[SPECIAL_POINTS_LIST_END]]

[[DB_COLUMNS_START]]
ABSOLUTE RULE: DB_COLUMNSëŠ” **ìš”ì•½ì´ ì•„ë‹ˆë¼ ë³µì‚¬**ë‹¤. ì•„ë˜ ë ˆê±°ì‹œ íƒœê·¸ì˜ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ë³µë¶™í•˜ë¼. (Do NOT paraphrase. Do NOT shorten.)
- necessity := [[NECESSITY_START]] ë‚´ë¶€ ë‚´ìš© ê·¸ëŒ€ë¡œ (íƒœê·¸ ì•ˆì´ Unknownì´ë©´ Unknown ê·¸ëŒ€ë¡œ)
- key_idea := [[KEY_IDEA_START]] ë‚´ë¶€ ë‚´ìš© ê·¸ëŒ€ë¡œ (Unknownì´ë©´ Unknown ê·¸ëŒ€ë¡œ)
- special_point := [[SPECIAL_POINT_START]] ë‚´ë¶€ ë‚´ìš© ê·¸ëŒ€ë¡œ (Unknownì´ë©´ Unknown ê·¸ëŒ€ë¡œ)

necessity: (ì—¬ê¸°ì— necessityë¥¼ ìœ„ ê·œì¹™ëŒ€ë¡œ ê·¸ëŒ€ë¡œ ë³µì‚¬)
key_idea: (ì—¬ê¸°ì— key_ideaë¥¼ ìœ„ ê·œì¹™ëŒ€ë¡œ ê·¸ëŒ€ë¡œ ë³µì‚¬)
special_point: (ì—¬ê¸°ì— special_pointë¥¼ ìœ„ ê·œì¹™ëŒ€ë¡œ ê·¸ëŒ€ë¡œ ë³µì‚¬)
correct_answer: (ì •ë‹µì´ ìˆìœ¼ë©´ ì •ë‹µ, ì—†ìœ¼ë©´ Unknown)
[[DB_COLUMNS_END]]

[[DEEP_INSIGHT_START]]
(Leave empty)
[[DEEP_INSIGHT_END]]
"""

gpt_client = None
if OPENAI_API_KEY and OPENAI_API_KEY.startswith("sk-"):
    try: gpt_client = OpenAI(api_key=OPENAI_API_KEY)
    except: pass

CURRENT_KEY_INDEX = 0

def initialize_api():
    global CURRENT_KEY_INDEX
    if not GOOGLE_API_KEYS: return
    genai.configure(api_key=GOOGLE_API_KEYS[CURRENT_KEY_INDEX])

def rotate_api_key():
    global CURRENT_KEY_INDEX
    if len(GOOGLE_API_KEYS) <= 1: return False
    prev_index = CURRENT_KEY_INDEX
    CURRENT_KEY_INDEX = (CURRENT_KEY_INDEX + 1) % len(GOOGLE_API_KEYS)
    genai.configure(api_key=GOOGLE_API_KEYS[CURRENT_KEY_INDEX])
    print(f"\nğŸ”„ [Quota] API Key êµì²´ ì™„ë£Œ (Key {prev_index + 1} -> Key {CURRENT_KEY_INDEX + 1})")
    return True

initialize_api()
REQUEST_OPTIONS = {"timeout": 600}

# [ëª¨ë¸ ì„¤ì •]
analysis_model = genai.GenerativeModel(model_name=MODEL_NAME_ANALYSIS, system_instruction=TAGGED_SYSTEM_PROMPT)
search_model = genai.GenerativeModel(model_name=MODEL_NAME_OCR, system_instruction=OCR_SYSTEM_PROMPT, generation_config={"temperature": 0.0})
insight_model = genai.GenerativeModel(model_name=MODEL_NAME_ANALYSIS, system_instruction=INSIGHT_SYSTEM_PROMPT)
concept_model = genai.GenerativeModel(model_name=MODEL_NAME_ANALYSIS, system_instruction=CONCEPT_SYSTEM_PROMPT)

def encode_image_to_base64(image_path):
    with open(image_path, "rb") as f: return base64.b64encode(f.read()).decode('utf-8')

# ==========================================================
# [JSON Helper Functions] - [ë³µêµ¬ ì™„ë£Œ] ì‚­ì œë˜ì—ˆë˜ í•¨ìˆ˜ë“¤ 100% ë³µêµ¬
# ==========================================================
def clean_json_text(text):
    if not text: return ""
    text = re.sub(r'```json\s*', '', text)
    text = re.sub(r'```', '', text)
    start_idx = text.find('{')
    end_idx = text.rfind('}')
    if start_idx != -1 and end_idx != -1:
        text = text[start_idx : end_idx+1]
    return text.strip()

def repair_json_content(text):
    if not text: return "{}"
    text = re.sub(r'(?<!\\)\\(?!["\\/bfnrtu])', r'\\\\', text)
    def replace_newlines_in_string(match):
        content = match.group(1).replace('\n', '\\n').replace('\r', '')
        return f'"{content}"'
    try:
        text = re.sub(r'"\s*:\s*"([^"\\]*(?:\\.[^"\\]*)*)"', replace_newlines_in_string, text, flags=re.DOTALL)
    except: pass
    return text

def try_advanced_parsing(text):
    try: return json.loads(text)
    except:
        try: return json.loads(repair_json_content(text))
        except:
            try:
                py_text = text.replace("true", "True").replace("false", "False").replace("null", "None")
                return ast.literal_eval(py_text)
            except: return None

def parse_broken_json(text):
    # [ë³µêµ¬ ì™„ë£Œ] í˜¹ì‹œ ëª¨ë¥¼ JSON í¬ë§· ì—ëŸ¬ ì‹œ ê°•ì œ ì¶”ì¶œìš© í•¨ìˆ˜
    print("âš  [Warning] JSON íŒŒì‹± ì‹¤íŒ¨. ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ê°•ì œ ì¶”ì¶œì„ ì‹œë„í•©ë‹ˆë‹¤.")
    fallback_data = {
        "db_columns": {"necessity": "", "key_idea": "", "special_point": ""},
        "body_content": {}
    }
    patterns = {
        "necessity": r'"necessity"\s*:\s*"([^"]+)"',
        "key_idea": r'"key_idea"\s*:\s*"([^"]+)"',
        "goal": r'"goal"\s*:\s*"([^"]+)"',
        "ai_solution": r'"ai_solution"\s*:\s*"([^"]+)"'
    }
    for key, pat in patterns.items():
        match = re.search(pat, text)
        if match:
            val = match.group(1)
            if key in ["necessity", "key_idea"]:
                fallback_data["db_columns"][key] = val
            else:
                fallback_data["body_content"][key] = val
    return fallback_data

# ==========================================================
# [The Monster Parser V30] Manual & Verbose Extraction (Restored & Enhanced)
# ==========================================================
def parse_tagged_response(text):
    print("ğŸšœ [Parser V30] ë°ì´í„° ì¶”ì¶œ ì‹œì‘ (Manual & Verbose Mode)...")
    
    data = {
        "db_columns": {"necessity": "", "key_idea": "", "special_point": "", "correct_answer": ""},
        "body_content": {
            "symbol_table": [],      # [V35] ê¸°í˜¸ ì •ì˜ í…Œì´ë¸”
            "logic_narrative": [],   # [V35] ë…¼ë¦¬ ì„œìˆ  ë¦¬ìŠ¤íŠ¸
            
            "key_ideas_list": [],    # [Safety] ì¶”ê°€ í•µì‹¬ ë¦¬ìŠ¤íŠ¸
            "special_points_list": [], # [Safety] ì¶”ê°€ íŠ¹ì´ì  ë¦¬ìŠ¤íŠ¸
            
            "practical_concepts": [], "basic_concepts": [],     
            "figure_analysis": "",    "verbatim_handwriting": "", 
            "ai_solution": "",        "instructor_solution": "",
            "conditions": [], "goal": "" # Legacy í˜¸í™˜ìš©
        }
    }

    # [Verbose Extraction Helper] - [ë³µêµ¬ ì™„ë£Œ] ë¡œê·¸ ë° ë¶€ë¶„ ë§¤ì¹­ ê¸°ëŠ¥
    def extract_section(start_tag, end_tag, debug_name=None):
        # 1. íƒœê·¸ ì •ê·œí™”
        base_start = start_tag.replace("[", "").replace("]", "")
        base_end = end_tag.replace("[", "").replace("]", "")
        
        # 2. [Core] ì •ì„ ë§¤ì¹­ (ê´„í˜¸, ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ë¬´ì‹œí•˜ê³  íƒœê·¸ ì°¾ê¸°)
        pattern = r'[\#\*\s\[\]]*' + base_start + r'[\#\*\s\[\]]*(.*?)[\#\*\s\[\]]*' + base_end + r'[\#\*\s\[\]]*'
        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
        if match: return match.group(1).strip()
        
        # 3. [Safety Net 1] Fallback (ì¢…ë£Œ íƒœê·¸ë¥¼ AIê°€ ë¹¼ë¨¹ì—ˆì„ ë•Œ)
        # ì‹œì‘ íƒœê·¸ë¶€í„°... ë‹¤ìŒ 'START' íƒœê·¸ê°€ ë‚˜ì˜¤ê¸° ì „ê¹Œì§€ ëª½ë•… ê¸ì–´ì˜´
        pattern_loose = r'[\#\*\s\[\]]*' + base_start + r'[\#\*\s\[\]]*(.*)'
        match_loose = re.search(pattern_loose, text, re.DOTALL | re.IGNORECASE)
        if match_loose:
            content = match_loose.group(1).strip()
            # ë‹¤ìŒ ì„¹ì…˜ì˜ ì‹œì‘ íƒœê·¸ê°€ ë³´ì´ë©´ ê±°ê¸°ì„œ ìë¥¸ë‹¤.
            next_tag_match = re.search(r'[\#\*\s\[\]]*[A-Z_]+_START[\#\*\s\[\]]*', content, re.IGNORECASE)
            if next_tag_match: return content[:next_tag_match.start()].strip()
            return content
            
        # 4. [Safety Net 2] Last Resort (AI í•´ì„¤ ì „ìš©)
        # íƒœê·¸ê°€ ì™„ì „íˆ ê¹¨ì¡Œì„ ë•Œ í•œê¸€ í‚¤ì›Œë“œ 'AI í•´ì„¤'ë¡œ ì°¾ê¸°
        if "AI_SOLUTION" in start_tag:
            alt_match = re.search(r'#+\s*AI\s*(ì •ì„\s*)?í•´ì„¤(.*?)(?=#+|$)', text, re.DOTALL | re.IGNORECASE)
            if alt_match: return alt_match.group(2).strip()
            
        return ""

# [Helper] ë¦¬ìŠ¤íŠ¸ íŒŒì‹± (ë¶ˆë¦¿ í¬ì¸íŠ¸ ì œê±° + ê¸°í˜¸ ë³´ì¡´ Fix)
    # [Helper] ë¦¬ìŠ¤íŠ¸ íŒŒì‹± (í•¨ìˆ˜ëª… ë³€ê²½ ë° ë¡œì§ í™•ì •)
    def parse_list(raw_text):
        if not raw_text: return []
        lines = raw_text.split('\n')
        cleaned = []
        for line in lines:
            # [í•€ì…‹ ìˆ˜ì •] â‘ , (ê°€) ë³´ì¡´ (ë¶ˆë¦¿ê³¼ 1. 2. ê°™ì€ ë²ˆí˜¸ë§Œ ì œê±°)
            line = re.sub(r'^\s*([\*\-]\s*|\d+\.\s*)', '', line).strip()
            if line: cleaned.append(line)
        return cleaned

    # [ë³µêµ¬ ì™„ë£Œ] ì¤‘ë³µ ì œê±° ë° ë³‘í•© ë¡œì§ (ë¦¬ìŠ¤íŠ¸ í•©ì¹˜ê¸°ìš©)
    def merge_and_deduplicate(single_text, list_text):
        items = []
        # 1. Single text ì²˜ë¦¬
        if single_text: items.append(single_text)
        # 2. List text ì²˜ë¦¬
        if list_text:
            lines = clean_list(list_text)
            for line in lines:
                is_duplicate = False
                for existing in items:
                    if line in existing or existing in line: # í¬í•¨ ê´€ê³„ í™•ì¸
                        is_duplicate = True
                        break
                if not is_duplicate:
                    items.append(line)
        return "\n".join(items)

    # [V30 New Logic] íŒŒì´í”„(|) êµ¬ë¶„ Teacher's Decoding íŒŒì„œ
    def parse_teacher_decoding(raw_text, item_type):
        items = []
        if not raw_text: return items
        lines = raw_text.split('\n')
        for line in lines:
            if not line.strip(): continue
            parts = [p.strip() for p in line.split('|')]
            
            symbol = parts[0] if len(parts) > 0 else "Note"
            content = parts[1] if len(parts) > 1 else parts[0]
            ai_comment = parts[2] if len(parts) > 2 else ""
            
            if len(parts) == 1: 
                symbol = "Note"
                content = line
            
            items.append({
                "type": item_type,
                "symbol": symbol,
                "content": content,
                "ai_comment": ai_comment
            })
        return items

# 1. [STEP 1] Symbol Table íŒŒì‹±
    raw_symbols = extract_section("SYMBOL_TABLE_START", "SYMBOL_TABLE_END")
    symbol_list = []

    if raw_symbols:
        for line in raw_symbols.splitlines():
            line = line.strip()
            if not line or line.startswith("Example") or line.startswith("("):
                continue
            if "|" not in line:
                # í¬ë§· ê¹¨ì§„ ì¤„ë„ ë³´ì¡´ (ëˆ„ë½ ë°©ì§€)
                symbol_list.append({
                    "symbol": "Unknown",
                    "type": "Trap",
                    "content": line,
                    "ai_comment": "(SYMBOL_TABLE í¬ë§· ì˜¤ë¥˜: '|' ì—†ìŒ)"
                })
                continue

            parts = [p.strip() for p in line.split("|")]

            # 4ì—´ ê¸°ëŒ€: Symbol | Type | Verbatim Content | Strategic Commentary
            sym = parts[0] if len(parts) > 0 else ""
            dtype = parts[1] if len(parts) > 1 else ""
            content = parts[2] if len(parts) > 2 else ""
            comment = parts[3] if len(parts) > 3 else ""

            # ëˆ„ë½ ë°©ì§€: ìµœì†Œí•œì´ë¼ë„ ì±„ì›€
            if not dtype:
                dtype = "Condition"
            if not content:
                content = "Unknown"

            symbol_list.append({
                "symbol": sym,
                "type": dtype,
                "content": content,
                "ai_comment": comment
            })

    # âœ… í•µì‹¬: ì´ì œë¶€í„° Notionì€ teacher_decodingë§Œ ë³´ë©´ 4ì—´ì´ í•­ìƒ ë§ëŠ”ë‹¤
    data["body_content"]["teacher_decoding"] = symbol_list

    # (ì„ íƒ) êµ¬ë²„ì „ í˜¸í™˜ì„ ìœ„í•´ symbol_tableë„ ë‚¨ê¸°ê³  ì‹¶ìœ¼ë©´ ì•„ë˜ì²˜ëŸ¼ ì €ì¥
    data["body_content"]["symbol_table"] = [
        {"symbol": x["symbol"], "meaning": x["content"], "comment": x["ai_comment"], "type": x["type"]}
        for x in symbol_list
    ]

    # [ì‹ ê·œ ë³µêµ¬] ì „ëµ ë¡œë“œë§µ & í–‰ë™ ê°•ë ¹ ì¶”ì¶œ
    data["body_content"]["strategy_overview"] = extract_section("STRATEGY_START", "STRATEGY_END")
    data["body_content"]["action_protocol"] = extract_section("ACTION_PROTOCOL_START", "ACTION_PROTOCOL_END")

    # 2. [STEP 2] Logic Narrative íŒŒì‹±
    raw_logic = extract_section("LOGIC_NARRATIVE_START", "LOGIC_NARRATIVE_END")
    data["body_content"]["logic_narrative"] = parse_list(raw_logic)

        # -------------------------------------------------------
    # [V30+ Critical Fix] Legacy Tags -> DB Columns (NO SUMMARY, COPY ONLY)
    # -------------------------------------------------------
    legacy_necessity = extract_section("NECESSITY_START", "NECESSITY_END", "LegacyNecessity")
    legacy_key_idea = extract_section("KEY_IDEA_START", "KEY_IDEA_END", "LegacyKeyIdea")
    legacy_special = extract_section("SPECIAL_POINT_START", "SPECIAL_POINT_END", "LegacySpecial")

    def _clean_legacy_block(s: str) -> str:
        if not s:
            return ""
        s = s.strip()
        # íƒœê·¸ ì•ˆì— ì•ˆë‚´ ë¬¸êµ¬ë§Œ ìˆê³  ì‹¤ì œ ë‚´ìš©ì´ ì—†ëŠ” ê²½ìš°ë¥¼ ë°©ì§€
        # (ê´„í˜¸ë¡œ ì‹œì‘í•˜ëŠ” ì•ˆë‚´ ë¼ì¸ë“¤ ì œê±°ëŠ” "ìš”ì•½"ì´ ì•„ë‹ˆë¼ "í”„ë¡¬í”„íŠ¸ ì•ˆë‚´ë¬¸ ì œê±°"ì„
        lines = []
        for line in s.splitlines():
            t = line.strip()
            if not t:
                continue
            # í”„ë¡¬í”„íŠ¸ ì•ˆë‚´ë¬¸ íŒ¨í„´ ì œê±° (í•„ìš” ìµœì†Œ)
            if t.startswith("(") and t.endswith(")"):
                continue
            if t.lower().startswith("rule-"):
                continue
            lines.append(t)
        return "\n".join(lines).strip()

    legacy_necessity = _clean_legacy_block(legacy_necessity)
    legacy_key_idea = _clean_legacy_block(legacy_key_idea)
    legacy_special = _clean_legacy_block(legacy_special)

    data["body_content"]["legacy_necessity_raw"] = legacy_necessity
    data["body_content"]["legacy_key_idea_raw"] = legacy_key_idea
    data["body_content"]["legacy_special_point_raw"] = legacy_special

    # Unknown ì²˜ë¦¬ í‘œì¤€í™” (ë¹ˆì¹¸ ë°©ì§€)
    def _normalize_unknown(s: str) -> str:
        if not s or not s.strip():
            return "Unknown"
        ss = s.strip()
        if ss.lower() == "unknown":
            return "Unknown"
        return ss

    def _to_db_index_string(items, max_len: int = 180) -> str:
        pieces = []
        for item in items:
            t = (item or "").strip()
            if not t:
                continue
            pieces.append(t)

        one_line = " / ".join(pieces).strip()
        one_line = _normalize_unknown(one_line)
        if one_line == "Unknown":
            return "Unknown"
        if len(one_line) > max_len:
            return one_line[:max_len - 3].rstrip() + "..."
        return one_line

    strict_key_contents = []
    strict_trap_contents = []
    strict_necessity_contents = []
    for row in symbol_list:
        dtype = (row.get("type") or "").strip().lower()
        content = (row.get("content") or "")
        ai_comment = (row.get("ai_comment") or "")

        if dtype == "key":
            if content and content.strip():
                strict_key_contents.append(content)
            if ai_comment and ai_comment.strip():
                strict_key_contents.append(ai_comment)
        if dtype == "trap":
            if content and content.strip():
                strict_trap_contents.append(content)
            if ai_comment and ai_comment.strip():
                strict_trap_contents.append(ai_comment)

        for source_text in (content, ai_comment):
            if not source_text:
                continue
            for bracket_text in re.findall(r"\[[^\[\]]+\]", source_text):
                strict_necessity_contents.append(bracket_text)

    # âœ… DB ì»¬ëŸ¼ì€ teacher_decoding ì¦ê±° ê¸°ë°˜ Strict ê·œì¹™ìœ¼ë¡œë§Œ ì €ì¥
    data["db_columns"]["necessity"] = _to_db_index_string(strict_necessity_contents)
    data["db_columns"]["key_idea"] = _to_db_index_string(strict_key_contents)
    data["db_columns"]["special_point"] = _to_db_index_string(strict_trap_contents)


    # 3. [Safety Nets] ë…ë¦½ ë¦¬ìŠ¤íŠ¸ íŒŒì‹±
    data["body_content"]["key_ideas_list"] = parse_list(extract_section("KEY_IDEAS_LIST_START", "KEY_IDEAS_LIST_END", "KeyList"))

    data["body_content"]["special_points_list"] = parse_list(extract_section("SPECIAL_POINTS_LIST_START", "SPECIAL_POINTS_LIST_END", "SpecList"))
    # 4. [Independent Modules] ì‹¤ì „ê°œë…, ê¸°ë³¸ê°œë…, ê·¸ë˜í”„, ì •ë‹µ, ì›ë¬¸
    pc_raw = extract_section("PRACTICAL_CONCEPTS_START", "PRACTICAL_CONCEPTS_END", "PracConcept")
    pc_list = []
    if pc_raw:
        for line in pc_raw.split('\n'):
            parts = re.split(r'\|\|', line)
            if len(parts) >= 2:
                pc_list.append({"title": parts[0].replace("Title:", "").strip(), "content": parts[1].replace("Content:", "").strip()})
    data["body_content"]["practical_concepts"] = pc_list

    data["body_content"]["basic_concepts"] = parse_list(extract_section("BASIC_CONCEPTS_START", "BASIC_CONCEPTS_END", "BasicConcept"))
    data["body_content"]["figure_analysis"] = extract_section("FIGURE_ANALYSIS_START", "FIGURE_ANALYSIS_END", "Figure")
    data["db_columns"]["correct_answer"] = extract_section("CORRECT_ANSWER_START", "CORRECT_ANSWER_END", "Answer")
    data["body_content"]["verbatim_handwriting"] = extract_section("VERBATIM_START", "VERBATIM_END", "Verbatim")
    data["body_content"]["ai_solution"] = extract_section("AI_SOLUTION_START", "AI_SOLUTION_END", "AISolution")
    data["body_content"]["instructor_solution"] = extract_section("DEEP_INSIGHT_START", "DEEP_INSIGHT_END", "Insight")

    return data

def execute_with_key_rotation(model, content, **kwargs):
    if "safety_settings" not in kwargs:
        kwargs["safety_settings"] = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        ]

    max_attempts = len(GOOGLE_API_KEYS) + 1 
    for attempt in range(max_attempts):
        try:
            return model.generate_content(content, **kwargs)
        except (ResourceExhausted, InternalServerError, ServiceUnavailable) as e:
            print(f"âš  Gemini API Error. í‚¤ êµì²´ ì‹œë„... ({e})")
            if rotate_api_key(): time.sleep(2); continue 
            else: raise e
        except Exception as e:
            if "429" in str(e) or "quota" in str(e).lower():
                print(f"âš  429 Rate Limit. í‚¤ êµì²´ ì‹œë„...")
                if rotate_api_key(): time.sleep(2); continue
            raise e 
    raise Exception("All Gemini Keys Exhausted")

def analyze_image_structure(image_path):
    import concept_manager
    raw_concepts = concept_manager.load_concepts()
    concept_list_text = "\n".join([f"- {c['title']}: {c.get('content','...')[:50]}" for c in raw_concepts[:50]])
    
    print("\n--- [Stage 1] êµ¬ì¡° ë¶„ì„ (V30 Forensic Juggernaut Mode) ì‹œì‘ ---")
    
    user_prompt = f"""
    Analyze the image strictly according to the 'Forensic Auditor' protocol.
    Fill all the tags in KOREAN.
    
    [Reference Concept DB Context]
    {concept_list_text}
    """
    
    try:
        response = execute_with_key_rotation(
            analysis_model, 
            [user_prompt, Image.open(image_path)],
            generation_config={"max_output_tokens": 8192, "temperature": 0.1},
            request_options=REQUEST_OPTIONS
        )
        result_data = parse_tagged_response(response.text)
        
        # [ê²€ì¦ ë¡œì§ ë³´ì¡´]
        body = result_data["body_content"]
        if not body["verbatim_handwriting"]:
            if body["strategy_overview"] or body["ai_solution"]:
                print("âš ï¸ [Warn] ì†ê¸€ì”¨(Verbatim)ëŠ” ì—†ì§€ë§Œ, í•´ì„¤/ì „ëµì´ ìˆì–´ í†µê³¼í•©ë‹ˆë‹¤.")
            else:
                print("âŒ [Error] ë¶„ì„ ë°ì´í„° ì „ë¬´ (Verbatim/Strategy/Solution ëª¨ë‘ ì—†ìŒ).")
                raise Exception("Critical Extraction Failed (Empty Data)")
            
        print("âœ… [Plan A] íƒœê·¸ íŒŒì‹± ì„±ê³µ")

    except Exception as e:
        print(f"âš  [Plan A ì‹¤íŒ¨] {e}. [Plan B] GPT ìš©ë³‘ íˆ¬ì… ì—¬ë¶€ í™•ì¸...")
        return None

    print("\n--- [Stage 2] ì‹¬ì¸µ ì¶”ë¡  ì‹œì‘ ---")
    concept_db_text = json.dumps(concept_manager.load_concepts(), ensure_ascii=False, indent=2)
    base_sol = result_data.get("body_content", {}).get("ai_solution", "")
    
    # [Insight ìƒì„± ë¡œì§ ë³´ì¡´]
    # (ì£¼ì˜) íŒŒì„œê°€ ë”ë¯¸ í…ìŠ¤íŠ¸ë¥¼ ì¡ì•„ì™€ì„œ ifë¬¸ì´ Falseê°€ ë˜ëŠ” ì¹˜ëª…ì  ë²„ê·¸ë¥¼ ì›ì²œ ì°¨ë‹¨í•¨.
    # ë”ë¯¸ í…ìŠ¤íŠ¸ê°€ ìˆë“  ì—†ë“ , ë¬´ì¡°ê±´ Stage 2 ë…ë¦½ ì—ì´ì „íŠ¸ë¥¼ ê°€ë™í•˜ì—¬ ë®ì–´ì”Œì›€!
    deep_insight = generate_deep_insight(image_path, base_sol, concept_db_text) 
    result_data["body_content"]["instructor_solution"] = deep_insight
    
    print("âœ… ë¶„ì„ ì™„ë£Œ.")
    return result_data

def generate_deep_insight(image_path, base_solution_text, concept_db_text):
    final_prompt = INSIGHT_SYSTEM_PROMPT.replace("[USER_CONCEPT_DB]", concept_db_text)
    user_prompt = f"[Standard Solution]:\n{base_solution_text}\n\nBased on the image and solution, provide the 1-Tier Insight."
    try:
        dynamic_insight_model = genai.GenerativeModel(model_name=MODEL_NAME_ANALYSIS, system_instruction=final_prompt)
        response = execute_with_key_rotation(
            dynamic_insight_model, [user_prompt, Image.open(image_path)], request_options=REQUEST_OPTIONS
        )
        return response.text.strip()
    except: return "ì‹¬ì¸µ ë¶„ì„ ìƒì„± ì‹¤íŒ¨"

def call_gpt4o_fallback(system_instr, user_instr, image_path):
    return None

def get_pure_ocr_text(image_path):
    try:
        response = execute_with_key_rotation(
            search_model, ["Execute OCR.", Image.open(image_path)], request_options=REQUEST_OPTIONS
        )
        return response.text.replace("```latex", "").replace("```", "").strip()
    except: return None

def extract_concepts_flexible(image_path):
    try:
        response = execute_with_key_rotation(
            concept_model, ["Extract concepts strictly in JSON.", Image.open(image_path)],
            generation_config={"response_mime_type": "application/json"}, request_options=REQUEST_OPTIONS
        )
        cleaned = clean_json_text(response.text)
        return json.loads(cleaned)
    except: return None
    
# [NEW] Aë‹¨ê³„(ë‹¨ìˆœê³„ì‚°) íŒë…ê¸° - Track B ì „ìš©
def check_is_basic_drill(text):
    if not text or len(text) < 5: return False
    try:
        # Flash ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¹ ë¥´ê²Œ íŒë‹¨ (True/False)
        prompt = f"""
        Role: Math Problem Classifier.
        Task: Analyze the text and determine if it is a "Simple Calculation Drill" (A-step,ë‹¨ìˆœ ì—°ì‚°,ê¸°ì´ˆ ë¬¸ì œ).
        Text: {text[:500]}
        Output: Return ONLY 'TRUE' if it is a simple drill, 'FALSE' otherwise.
        """
        resp = execute_with_key_rotation(search_model, [prompt], request_options=REQUEST_OPTIONS)
        return "TRUE" in resp.text.strip().upper()
    except: return False

# [NEW] ì‹œëŒ€ì¸ì¬ê¸‰ ë‚œì´ë„ íŒë…ê¸° (Level 1~4 Classifier)
def analyze_difficulty_level(text):
    if not text: return "ê¸°ë³¸ê°œë…"
    try:
        # Flash ëª¨ë¸ì—ê²Œ 'ë¬¸ì œì˜ ê´€ìƒ(Heuristics)'ì„ ë³´ê³  íŒë‹¨í•˜ë¼ê³  ì§€ì‹œ
        prompt = f"""
        Role: Math Problem Difficulty Classifier.
        Task: Classify the difficulty of the given math problem text into one of 4 levels.
        
        [Criteria]
        1. LEVEL_1 (Basic Concept): Short text (1-3 lines), asks for simple calculation or basic definition.
        2. LEVEL_2 (Entry Semi-Killer): Standard 4-point problem. Has 1-2 conditions. Typical textbook style.
        3. LEVEL_3 (Deep Semi-Killer): Hard 4-point. Keywords: "Defined function g(x)", "Differentiability", "Select all correct (ã„±,ã„´,ã„·)", "Fill in the blank". Requires logical deduction.
        4. LEVEL_4 (Killer): Very long text, complex conditions, new function definitions, finding Max/Min in complex situations. 

        Input Text:
        {text[:800]}

        Output: ONLY return one word: "LEVEL_1", "LEVEL_2", "LEVEL_3", or "LEVEL_4".
        """
        resp = execute_with_key_rotation(search_model, [prompt], request_options=REQUEST_OPTIONS)
        result = resp.text.strip().upper()
        
        if "LEVEL_4" in result: return "í‚¬ëŸ¬"
        if "LEVEL_3" in result: return "ì¤€í‚¬ëŸ¬_ì‹¬í™”"
        if "LEVEL_2" in result: return "ì¤€í‚¬ëŸ¬_ì§„ì…"
        return "ê¸°ë³¸ê°œë…"
    except: return "ê¸°ë³¸ê°œë…"
