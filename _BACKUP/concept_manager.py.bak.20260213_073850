# concept_manager.py
import json
import os
import shutil
import re
import difflib
import time
import logging
import threading
from datetime import datetime
from config import MD_DIR_PATH


FILE_LOCK = threading.RLock()
# ==========================================================
# [Configuration] ê²½ë¡œ ë° ìƒìˆ˜ (ì ˆëŒ€ íƒ€í˜‘ ì—†ìŒ)
# ==========================================================
DB_PATH = os.path.join(MD_DIR_PATH, "concept_book.json")
BACKUP_DIR = os.path.join(MD_DIR_PATH, "concept_history")
TEMP_DB_PATH = os.path.join(MD_DIR_PATH, "concept_book.tmp")
WHITELIST_PATH = os.path.join(MD_DIR_PATH, "concept_whitelist.json")

# [ìœ ì‚¬ë„ ì„ê³„ê°’ - ì—„ê²©]
SIMILARITY_THRESHOLD_HIGH = 0.85  # ì´ ì´ìƒì´ë©´ ë¬´ì¡°ê±´ ë³‘í•© (Append)
SIMILARITY_THRESHOLD_WARN = 0.40  # ì´ ì´ìƒì´ë©´ (ì¤‘ë³µì˜ì‹¬) íƒœê·¸ ë¶€ì°©

# ë¡œê¹… ì„¤ì • (ë³€íƒœì ìœ¼ë¡œ ìƒì„¸í•˜ê²Œ)
logging.basicConfig(
    filename='concept_manager.log',
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    encoding='utf-8'
)

def log_debug(msg): logging.debug(msg)
def log_info(msg): 
    print(f"â„¹ï¸ [Manager] {msg}")
    logging.info(msg)
def log_warn(msg): 
    print(f"âš ï¸ [Manager] {msg}")
    logging.warning(msg)
def log_error(msg): 
    print(f"âŒ [Manager] {msg}")
    logging.error(msg)

# ==========================================================
# [Core Logic 0] Whitelist (ë©´ì£„ë¶€ ì‹œìŠ¤í…œ)
# ==========================================================
def load_whitelist():
    """ì‚¬ìš©ìê°€ 'ì¤‘ë³µ ì•„ë‹˜'ìœ¼ë¡œ ì§€ì •í•œ ìŒì„ ë¡œë“œ"""
    if not os.path.exists(WHITELIST_PATH): return []
    try:
        with open(WHITELIST_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except: return []

def add_to_whitelist(title_a, title_b):
    """(A, B)ëŠ” ì„œë¡œ ë‹¤ë¥¸ ê°œë…ì„ì„ ì˜êµ¬ ê¸°ë¡"""
    data = load_whitelist()
    # ìˆœì„œ ë¬´ê´€í•˜ê²Œ ì €ì¥ (í•­ìƒ ì •ë ¬í•´ì„œ ì €ì¥)
    pair = sorted([title_a, title_b])
    if pair not in data:
        data.append(pair)
        try:
            with open(WHITELIST_PATH, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            log_info(f"ğŸ³ï¸ [Whitelist] '{title_a}' vs '{title_b}' ë¬´ì‹œ ëª©ë¡ ë“±ë¡.")
        except Exception as e:
            log_error(f"Whitelist ì €ì¥ ì‹¤íŒ¨: {e}")

def is_whitelisted(title_a, title_b):
    """ì´ ë‘ ê°œê°€ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ”ì§€ í™•ì¸"""
    data = load_whitelist()
    pair = sorted([title_a, title_b])
    return pair in data

# ==========================================================
# [Core Logic 1] ë³€íƒœì  ì •ê·œí™” (Fingerprint)
# ==========================================================
def normalize_fingerprint(text):
    """
    í…ìŠ¤íŠ¸ì˜ ì˜í˜¼ë§Œ ì¶”ì¶œ.
    1. ì†Œë¬¸ìí™” + ì–‘ì˜† ê³µë°± ì œê±°
    2. (ì¤‘ë³µì˜ì‹¬) [xx%] íƒœê·¸ ì œê±° (ìˆœìˆ˜ ì œëª©ë§Œ ë¹„êµ ìœ„í•´)
    3. ë…¸ì´ì¦ˆ ë‹¨ì–´ ì œê±° (ìˆ˜í•™, ê°œë… ë“±)
    4. íŠ¹ìˆ˜ë¬¸ì ì „ë©¸ì‹œí‚´
    """
    if not text: return ""
    
    # 1. íƒœê·¸ ì œê±° (ê¸°ì¡´ì— ë¶™ì€ íƒœê·¸ ë¬´ì‹œí•˜ê³  ì•Œë§¹ì´ë§Œ ë¹„êµ)
    text = re.sub(r'^\(ì¤‘ë³µì˜ì‹¬\)\s*\[\d+%\]\s*', '', text)
    
    # 2. ê¸°ë³¸ ì •ì œ
    text = text.lower().strip()
    
    # 3. ë…¸ì´ì¦ˆ ë‹¨ì–´ ì œê±° (ê¸´ ê²ƒë¶€í„°)
    noise_words = ["ì‹¤ì „ê°œë…", "ê¸°ë³¸ê°œë…", "ìˆ˜í•™ê°œë…", "ê³µì‹ì •ë¦¬", "ê°œë…ì •ë¦¬", "ìˆ˜í•™", "ê°œë…", "ê³µì‹", "ì •ë¦¬"]
    for word in noise_words:
        text = text.replace(word, "")
    
    # 4. íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ì ì™¸ ì œê±°)
    text = re.sub(r'[^a-z0-9ê°€-í£]', '', text)
    
    return text

# ==========================================================
# [Core Logic 2] ìœ ì‚¬ë„ ê³„ì‚° (Sim Radar)
# ==========================================================
def calculate_similarity(s1, s2):
    if not s1 or not s2: return 0.0
    norm1 = normalize_fingerprint(s1)
    norm2 = normalize_fingerprint(s2)
    if not norm1 or not norm2: return 0.0
    return difflib.SequenceMatcher(None, norm1, norm2).ratio()

# ==========================================================
# [File I/O] ì•ˆì „ ì œì¼ (Safety First)
# ==========================================================
def ensure_backup_dir():
    if not os.path.exists(BACKUP_DIR):
        try: os.makedirs(BACKUP_DIR)
        except: pass

def create_snapshot():
    if not os.path.exists(DB_PATH): return
    ensure_backup_dir()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_filename = f"concept_book_backup_{timestamp}.json"
    try: shutil.copy2(DB_PATH, os.path.join(BACKUP_DIR, backup_filename))
    except: pass

def load_concepts():
    """
    [Thread-Safe] JSON DB ë¡œë“œ.
    íŒŒì¼ì„ ì½ëŠ” ë„ì¤‘ì— ë‹¤ë¥¸ ìŠ¤ë ˆë“œê°€ ì“°ì§€ ëª»í•˜ë„ë¡ ë½ì„ ê²ë‹ˆë‹¤.
    """
    with FILE_LOCK:
        if not os.path.exists(DB_PATH): return []
        try:
            with open(DB_PATH, "r", encoding="utf-8") as f:
                data = json.load(f)
                return data if isinstance(data, list) else []
        except: return []

def save_all_concepts(data):
    """Atomic Save: ì“°ë‹¤ê°€ ì£½ì–´ë„ DBëŠ” ê¹¨ì§€ì§€ ì•ŠëŠ”ë‹¤."""
    with FILE_LOCK:
        try:
            with open(TEMP_DB_PATH, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            if os.path.exists(DB_PATH): os.remove(DB_PATH)
            os.rename(TEMP_DB_PATH, DB_PATH)
            return True
        except Exception as e:
            log_error(f"CRITICAL: DB ì €ì¥ ì‹¤íŒ¨ {e}")
            return False

# ==========================================================
# [Main Logic] ê°œë… ì €ì¥ (The Fortress Gatekeeper)
# ==========================================================
def save_concept(new_concept):
    """
    [ì•Œê³ ë¦¬ì¦˜: Fortress V2 - Tag & Append]
    1. Fingerprint ì™„ì „ ì¼ì¹˜ -> ë¬´ì¡°ê±´ ë³‘í•©
    2. ìœ ì‚¬ë„ > 85% -> ë¬´ì¡°ê±´ ë³‘í•© (Smart Format)
    3. ìœ ì‚¬ë„ > 40% -> (ì¤‘ë³µì˜ì‹¬) íƒœê·¸ ë¶™ì—¬ì„œ ìƒì„± (ë‹¨, í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ìˆìœ¼ë©´ íŒ¨ìŠ¤)
    4. ë‚˜ë¨¸ì§€ -> ì‹ ê·œ ìƒì„±
    """
    if not new_concept or "title" not in new_concept: return

    raw_title = new_concept['title'].strip()
    raw_content = new_concept.get('content', "").strip()
    
    # [ë°©ì–´] ë‚´ìš© ë¶€ì‹¤(10ì ë¯¸ë§Œ) ì°¨ë‹¨
    if len(raw_content) < 10:
        log_info(f"ë‚´ìš© ë¶€ì‹¤ë¡œ ì €ì¥ ê±°ë¶€: {raw_title}")
        return

    create_snapshot()
    data = load_concepts()
    
    # -------------------------------------------------------
    # 1. ì „ìˆ˜ ì¡°ì‚¬ (Full Scan)
    # -------------------------------------------------------
    best_match_idx = -1
    highest_sim = 0.0
    match_type = "NONE" # EXACT, HIGH, MID, NONE
    
    target_fingerprint = normalize_fingerprint(raw_title)

    for idx, item in enumerate(data):
        existing_title = item.get('title', "")
        
        # 0. í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ í™•ì¸ (ë©´ì£„ë¶€)
        if is_whitelisted(raw_title, existing_title):
            continue # ì´ ë…€ì„ê³¼ëŠ” ë¹„êµí•˜ì§€ ì•ŠëŠ”ë‹¤
            
        existing_fingerprint = normalize_fingerprint(existing_title)
        
        # 1-1. ì™„ì „ ì¼ì¹˜ (Priority 1)
        if target_fingerprint == existing_fingerprint:
            best_match_idx = idx
            highest_sim = 1.0
            match_type = "EXACT"
            break 
        
        # 1-2. ìœ ì‚¬ë„ ê³„ì‚°
        sim = calculate_similarity(raw_title, existing_title)
        if sim > highest_sim:
            highest_sim = sim
            best_match_idx = idx

    # -------------------------------------------------------
    # 2. íŒì • ë° ì‹¤í–‰ (Decision)
    # -------------------------------------------------------
    
    # [CASE A] ë³‘í•© (Append) - ì™„ì „ ì¼ì¹˜ or 85% ì´ìƒ
    if match_type == "EXACT" or highest_sim >= SIMILARITY_THRESHOLD_HIGH:
        target_item = data[best_match_idx]
        target_title = target_item.get('title')
        old_content = target_item.get('content', "")
        
        # ë‚´ìš© ì¤‘ë³µ ì²´í¬ (ë‹¨ìˆœ í¬í•¨)
        if normalize_fingerprint(raw_content) in normalize_fingerprint(old_content):
            log_info(f"ğŸ›¡ï¸ [Skip] '{raw_title}' ë‚´ìš©ì€ ì´ë¯¸ '{target_title}'ì— ìˆìŒ.")
            return

        # [Smart Formatting]
        today = datetime.now().strftime("%Y-%m-%d")
        # ì´ë¯¸ì§€ URLì´ë‚˜ ì¶œì²˜ê°€ ìˆìœ¼ë©´ ì¢‹ê² ì§€ë§Œ, í˜„ì¬ëŠ” ë‚ ì§œë¡œ êµ¬ë¶„
        append_header = f"\n\n\n--- ğŸ“… [ì¶”ê°€: {today}] (ìœ ì‚¬ë„ {int(highest_sim*100)}%) ---\n"
        
        target_item['content'] = old_content + append_header + raw_content
        target_item['last_updated'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        save_all_concepts(data)
        log_info(f"ğŸ”— [Merged] '{raw_title}' -> '{target_title}' ë³‘í•© ì™„ë£Œ.")
        return

    # [CASE B] ì¤‘ë³µ ì˜ì‹¬ (Tagging) - 40% ~ 84%
    elif highest_sim >= SIMILARITY_THRESHOLD_WARN:
        sim_percent = int(highest_sim * 100)
        origin_title = data[best_match_idx]['title']
        
        # íƒœê·¸ ë¶€ì°©: "(ì¤‘ë³µì˜ì‹¬) [82%] ì›ë˜ì œëª©"
        tagged_title = f"(ì¤‘ë³µì˜ì‹¬) [{sim_percent}%] {raw_title}"
        
        log_warn(f"âš ï¸ [Suspect] '{raw_title}' vs '{origin_title}' ({sim_percent}%). íƒœê·¸ ë¶€ì°© ì €ì¥.")
        
        new_concept['title'] = tagged_title
        new_concept['created_at'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        new_concept['last_updated'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # ë‚´ìš© ìƒë‹¨ì—ë„ ê²½ê³  ë¬¸êµ¬ ì‚½ì…
        warn_msg = f"> âš ï¸ **ì‹œìŠ¤í…œ ê²½ê³ :** ì´ ê°œë…ì€ '{origin_title}'ê³¼ {sim_percent}% ìœ ì‚¬í•©ë‹ˆë‹¤.\n\n"
        new_concept['content'] = warn_msg + raw_content
        
        data.append(new_concept)
        save_all_concepts(data)
        return

    # [CASE C] ì‹ ê·œ ìƒì„± (New)
    new_concept['created_at'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    new_concept['last_updated'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    data.append(new_concept)
    save_all_concepts(data)
    log_info(f"âœ¨ [New] '{raw_title}' ì‹ ê·œ ë“±ë¡.")

# ==========================================================
# [Helper Tools] UI ì—°ë™ìš© ë„êµ¬ë“¤
# ==========================================================
def delete_concept(target_title):
    create_snapshot()
    data = load_concepts()
    new_data = [d for d in data if d['title'] != target_title]
    if len(data) != len(new_data):
        save_all_concepts(new_data)
        return True
    return False

def manual_update_concept(target_title, new_content):
    create_snapshot()
    data = load_concepts()
    for item in data:
        if item['title'] == target_title:
            item['content'] = new_content
            item['last_updated'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            save_all_concepts(data)
            return True
    return False

def merge_concepts_manual(master_title, slave_titles):
    """ì‚¬ìš©ìê°€ UIì—ì„œ ì„ íƒí•œ ê²ƒë“¤ ê°•ì œ ë³‘í•©"""
    create_snapshot()
    data = load_concepts()
    
    master_item = next((d for d in data if d['title'] == master_title), None)
    if not master_item: return False
    
    slaves = [d for d in data if d['title'] in slave_titles]
    
    today = datetime.now().strftime("%Y-%m-%d")
    merged_content = master_item.get('content', "")
    
    for slave in slaves:
        s_title = slave.get('title')
        s_content = slave.get('content', "")
        header = f"\n\n--- ğŸ”— [ë³‘í•©ë¨: {s_title} | {today}] ---\n"
        merged_content += header + s_content
    
    master_item['content'] = merged_content
    master_item['last_updated'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # Slave ì‚­ì œ (ì œëª©ì—ì„œ íƒœê·¸ ë–¼ê³  ë¹„êµí•˜ëŠ” ë¡œì§ ë“± ë¶ˆí•„ìš”, ì •í™•í•œ ì œëª©ìœ¼ë¡œ ì‚­ì œ)
    slave_set = set(slave_titles)
    new_data = [d for d in data if d['title'] not in slave_set]
    
    save_all_concepts(new_data)
    return True

def remove_suspect_tag(target_title):
    """(ì¤‘ë³µì˜ì‹¬) íƒœê·¸ ì œê±° (ì²­ì†Œ ê¸°ëŠ¥)"""
    # ì •ê·œì‹ìœ¼ë¡œ íƒœê·¸ ë¶€ë¶„ë§Œ ë‚ ë¦¼
    clean_title = re.sub(r'^\(ì¤‘ë³µì˜ì‹¬\)\s*\[\d+%\]\s*', '', target_title)
    
    if clean_title == target_title: return False # íƒœê·¸ ì—†ìŒ
    
    create_snapshot()
    data = load_concepts()
    
    # í˜¹ì‹œ íƒœê·¸ ë—€ ì´ë¦„ì´ ì´ë¯¸ ì¡´ì¬í•˜ë©´? -> ë³‘í•©í•´ì•¼ í•¨ (ë³µì¡ë„ ì¦ê°€)
    # ì—¬ê¸°ì„œëŠ” "ì´ë¯¸ ì¡´ì¬í•˜ë©´ ì‹¤íŒ¨ ì²˜ë¦¬"í•˜ê³  ì‚¬ìš©ìì—ê²Œ "ë³‘í•©í•˜ì„¸ìš”"ë¼ê³  í•˜ëŠ” ê²Œ ì•ˆì „í•¨
    if any(d['title'] == clean_title for d in data):
        log_warn(f"íƒœê·¸ ì œê±° ë¶ˆê°€: '{clean_title}'ì´ ì´ë¯¸ ì¡´ì¬í•¨. ë³‘í•© ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        return "EXISTS" # íŠ¹ìˆ˜ ë¦¬í„´
    
    for item in data:
        if item['title'] == target_title:
            item['title'] = clean_title
            # ë‚´ìš© ìƒë‹¨ì˜ ê²½ê³  ë¬¸êµ¬ë„ ì œê±° ì‹œë„
            content = item.get('content', "")
            content = re.sub(r'> âš ï¸ \*\*ì‹œìŠ¤í…œ ê²½ê³ :\*\*.*?\n\n', '', content, flags=re.DOTALL)
            item['content'] = content
            save_all_concepts(data)
            return True
            
    return False

def get_similarity_clusters():
    """
    [UI ì •ë ¬ìš©] ì „ì²´ ê°œë…ì„ N*N ë¹„êµí•˜ì—¬ ìœ ì‚¬í•œ ê²ƒë¼ë¦¬ ë¬¶ì€ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    (ì„±ëŠ¥ ë¬´ì‹œ, ê²°ê³¼ ì§€í–¥)
    """
    data = load_concepts()
    if not data: return []
    
    # 1. (ì œëª©, ì •ê·œí™”ëœì œëª©) ë¦¬ìŠ¤íŠ¸ ìƒì„±
    items = []
    for d in data:
        items.append({
            'title': d['title'],
            'norm': normalize_fingerprint(d['title']),
            'visited': False
        })
        
    clusters = []
    
    # 2. í´ëŸ¬ìŠ¤í„°ë§ (Greedy)
    for i in range(len(items)):
        if items[i]['visited']: continue
        
        # ìƒˆë¡œìš´ í´ëŸ¬ìŠ¤í„° ì‹œì‘
        current_cluster = [items[i]['title']]
        items[i]['visited'] = True
        base_norm = items[i]['norm']
        
        for j in range(i+1, len(items)):
            if items[j]['visited']: continue
            
            # ìœ ì‚¬ë„ ë¹„êµ (ê¸°ì¤€: 0.4 ì´ìƒì´ë©´ ê°™ì€ ê·¸ë£¹ìœ¼ë¡œ ê°„ì£¼)
            sim = difflib.SequenceMatcher(None, base_norm, items[j]['norm']).ratio()
            if sim >= 0.4:
                current_cluster.append(items[j]['title'])
                items[j]['visited'] = True
        
        clusters.append(current_cluster)
        
    # 3. í”Œë« ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (í´ëŸ¬ìŠ¤í„° ê°„ êµ¬ë¶„ì€ UIì—ì„œ ì²˜ë¦¬í•˜ë“  ê·¸ëƒ¥ ë‚˜ì—´í•˜ë“ )
    # ì—¬ê¸°ì„œëŠ” ìœ ì‚¬í•œ ê²ƒë¼ë¦¬ ì¸ì ‘í•˜ê²Œ ë°°ì¹˜ëœ ë‹¨ì¼ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    sorted_titles = []
    for cl in clusters:
        # í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ëŠ” ê°€ë‚˜ë‹¤ìˆœ ì •ë ¬
        cl.sort()
        sorted_titles.extend(cl)
        
    return sorted_titles