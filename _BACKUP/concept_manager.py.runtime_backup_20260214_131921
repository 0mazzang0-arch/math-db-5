# concept_manager.py
import json
import os
import shutil
import re
import difflib
import logging
import threading
import unicodedata
from datetime import datetime

from config import MD_DIR_PATH
import database_manager


FILE_LOCK = threading.RLock()
# ==========================================================
# [Configuration] ê²½ë¡œ ë° ìƒìˆ˜ (ì ˆëŒ€ íƒ€í˜‘ ì—†ìŒ)
# ==========================================================
DB_PATH = os.path.join(MD_DIR_PATH, "mathbot.sqlite3")
BACKUP_DIR = os.path.join(MD_DIR_PATH, "concept_history")
SOURCE_BACKUP_DIR = os.path.join(os.path.dirname(__file__), "_BACKUP")
WHITELIST_PATH = os.path.join(MD_DIR_PATH, "concept_whitelist.json")

# [ìœ ì‚¬ë„ ì„ê³„ê°’ - ì—„ê²©]
SIMILARITY_THRESHOLD_HIGH = 0.85  # ì´ ì´ìƒì´ë©´ ë¬´ì¡°ê±´ ë³‘í•© (Append)
SIMILARITY_THRESHOLD_WARN = 0.40  # ì´ ì´ìƒì´ë©´ (ì¤‘ë³µì˜ì‹¬) íƒœê·¸ ë¶€ì°©

# [Global State] find_page_id ì§€ì›ìš© ìºì‹œ
CONCEPT_CACHE = []
FAST_LOOKUP_MAP = {}  # { "ì •ê·œí™”ëœì œëª©": "concept_id" }
GHOST_MAP = {}        # { "ì •ê·œí™”ëœì œëª©(í™•í†µì œê±°)": "concept_id" }

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    filename='concept_manager.log',
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    encoding='utf-8'
)


def log_debug(msg): logging.debug(msg)


def log_info(msg):
    print(f"â„¹ï¸ [Manager] {msg}")
    logging.info(msg)
    try:
        database_manager.write_log("INFO", msg)
    except Exception:
        pass


def log_warn(msg):
    print(f"âš ï¸ [Manager] {msg}")
    logging.warning(msg)
    try:
        database_manager.write_log("WARN", msg)
    except Exception:
        pass


def log_error(msg):
    print(f"âŒ [Manager] {msg}")
    logging.error(msg)
    try:
        database_manager.write_log("ERROR", msg)
    except Exception:
        pass


# ==========================================================
# [Defensive Change] ì†ŒìŠ¤ íŒŒì¼ ìë™ ë°±ì—…
# ==========================================================
def backup_current_source_file():
    """ë®ì–´ì“°ê¸° ì´ì „ ë³´ì¡´ ê´€ë¡€ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•œ ì†ŒìŠ¤ ë°±ì—… ë£¨í‹´."""
    try:
        os.makedirs(SOURCE_BACKUP_DIR, exist_ok=True)
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        dst = os.path.join(SOURCE_BACKUP_DIR, f"concept_manager.py.runtime_backup_{ts}")
        shutil.copy2(__file__, dst)
    except Exception:
        pass


# ==========================================================
# [Core Logic 0] Whitelist (ë©´ì£„ë¶€ ì‹œìŠ¤í…œ)
# ==========================================================
def load_whitelist():
    """ì‚¬ìš©ìê°€ 'ì¤‘ë³µ ì•„ë‹˜'ìœ¼ë¡œ ì§€ì •í•œ ìŒì„ ë¡œë“œ"""
    if not os.path.exists(WHITELIST_PATH):
        return []
    try:
        with open(WHITELIST_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return []


def add_to_whitelist(title_a, title_b):
    """(A, B)ëŠ” ì„œë¡œ ë‹¤ë¥¸ ê°œë…ì„ì„ ì˜êµ¬ ê¸°ë¡"""
    data = load_whitelist()
    pair = sorted([title_a, title_b])
    if pair not in data:
        data.append(pair)
        try:
            os.makedirs(os.path.dirname(WHITELIST_PATH), exist_ok=True)
            with open(WHITELIST_PATH, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            log_info(f"ğŸ³ï¸ [Whitelist] '{title_a}' vs '{title_b}' ë¬´ì‹œ ëª©ë¡ ë“±ë¡.")
        except Exception as e:
            log_error(f"Whitelist ì €ì¥ ì‹¤íŒ¨: {e}")


def is_whitelisted(title_a, title_b):
    """ì´ ë‘ ê°œê°€ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ”ì§€ í™•ì¸"""
    data = load_whitelist()
    pair = sorted([title_a, title_b])
    return pair in data


# ==========================================================
# [Core Logic 1] ë³€íƒœì  ì •ê·œí™” (Fingerprint)
# ==========================================================
def normalize_fingerprint(text):
    """
    í…ìŠ¤íŠ¸ì˜ ì˜í˜¼ë§Œ ì¶”ì¶œ.
    1. ì†Œë¬¸ìí™” + ì–‘ì˜† ê³µë°± ì œê±°
    2. (ì¤‘ë³µì˜ì‹¬) [xx%] íƒœê·¸ ì œê±° (ìˆœìˆ˜ ì œëª©ë§Œ ë¹„êµ ìœ„í•´)
    3. ë…¸ì´ì¦ˆ ë‹¨ì–´ ì œê±° (ìˆ˜í•™, ê°œë… ë“±)
    4. íŠ¹ìˆ˜ë¬¸ì ì „ë©¸ì‹œí‚´
    """
    if not text:
        return ""

    text = re.sub(r'^\(ì¤‘ë³µì˜ì‹¬\)\s*\[\d+%\]\s*', '', str(text))
    text = text.lower().strip()
    noise_words = ["ì‹¤ì „ê°œë…", "ê¸°ë³¸ê°œë…", "ìˆ˜í•™ê°œë…", "ê³µì‹ì •ë¦¬", "ê°œë…ì •ë¦¬", "ìˆ˜í•™", "ê°œë…", "ê³µì‹", "ì •ë¦¬"]
    for word in noise_words:
        text = text.replace(word, "")
    text = re.sub(r'[^a-z0-9ê°€-í£]', '', text)
    return text


def calculate_similarity(s1, s2):
    if not s1 or not s2:
        return 0.0
    norm1 = normalize_fingerprint(s1)
    norm2 = normalize_fingerprint(s2)
    if not norm1 or not norm2:
        return 0.0
    return difflib.SequenceMatcher(None, norm1, norm2).ratio()


# ==========================================================
# [Ghost Map ë¡œì§] íŒŒì¼ëª… ë§¤ì¹­ ì •ê·œí™”
# ==========================================================
def normalize_aggressive(text):
    if not text:
        return ""

    text = unicodedata.normalize('NFC', str(text))
    text = text.replace("ê³µí†µë²”ìœ„", "").replace("ê³µí†µ", "")

    score_match = re.search(r'(\[\d+\.\d+ì \])', text)
    if score_match:
        split_idx = score_match.start()
        front = text[:split_idx]
        back = text[split_idx:]
        back = back.replace("ë¬¸ê³¼", "").replace("ì´ê³¼", "").replace("ì˜ˆì²´ëŠ¥", "")
        text = front + back
    else:
        text = re.sub(r'(ë¬¸ê³¼|ì´ê³¼|ì˜ˆì²´ëŠ¥)\s*$', '', text)

    text = re.sub(r'\[.*?\]', '', text)
    text = re.sub(r'\(.*?\)', '', text)
    for _ in range(3):
        text = re.sub(r'(_img\d*|_\d+)\s*$', '', text)

    text = re.sub(r'[^0-9a-zA-Zê°€-í£]', '', text).lower()
    return text.strip()


# ==========================================================
# [SQLite I/O] ì•ˆì „ ì œì¼ (Safety First)
# ==========================================================
def ensure_backup_dir():
    if not os.path.exists(BACKUP_DIR):
        try:
            os.makedirs(BACKUP_DIR)
        except Exception:
            pass


def create_snapshot():
    if not os.path.exists(DB_PATH):
        return
    ensure_backup_dir()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_filename = f"mathbot_backup_{timestamp}.sqlite3"
    try:
        shutil.copy2(DB_PATH, os.path.join(BACKUP_DIR, backup_filename))
    except Exception:
        pass


def _row_to_legacy_item(row):
    return {
        "id": row.get("id"),
        "title": row.get("title", ""),
        "content": row.get("content", ""),
        "fingerprint": row.get("fingerprint", ""),
        "notion_page_id": row.get("notion_page_id"),
        "created_at": row.get("created_at"),
        "last_updated": row.get("last_updated"),
    }


def load_concepts():
    """
    [Thread-Safe] SQLite DB ë¡œë“œ.
    main.py í˜¸í™˜ì„ ìœ„í•´ ê¸°ì¡´ JSON ë¦¬ìŠ¤íŠ¸ êµ¬ì¡°ë¡œ ë°˜í™˜.
    """
    with FILE_LOCK:
        try:
            database_manager.init_db()
            rows = database_manager.fetch_all_concepts()
            return [_row_to_legacy_item(r) for r in rows]
        except Exception as e:
            log_error(f"DB ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []


def save_all_concepts(data):
    """Atomic Replace: ì“°ë‹¤ê°€ ì£½ì–´ë„ íŠ¸ëœì­ì…˜ ë¡¤ë°±"""
    with FILE_LOCK:
        try:
            normalized = []
            for item in data:
                title = item.get("title", "")
                normalized.append({
                    "title": title,
                    "content": item.get("content", ""),
                    "fingerprint": item.get("fingerprint") or normalize_fingerprint(title),
                    "notion_page_id": item.get("notion_page_id"),
                    "created_at": item.get("created_at") or datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "last_updated": item.get("last_updated"),
                })
            database_manager.replace_all_concepts(normalized)
            return True
        except Exception as e:
            log_error(f"CRITICAL: DB ì €ì¥ ì‹¤íŒ¨ {e}")
            return False


# ==========================================================
# [Main Logic] ê°œë… ì €ì¥ (The Fortress Gatekeeper)
# ==========================================================
def save_concept(new_concept):
    """
    [ì•Œê³ ë¦¬ì¦˜: Fortress V2 - Tag & Append]
    1. Fingerprint ì™„ì „ ì¼ì¹˜ -> ë¬´ì¡°ê±´ ë³‘í•©
    2. ìœ ì‚¬ë„ > 85% -> ë¬´ì¡°ê±´ ë³‘í•© (Smart Format)
    3. ìœ ì‚¬ë„ > 40% -> (ì¤‘ë³µì˜ì‹¬) íƒœê·¸ ë¶™ì—¬ì„œ ìƒì„± (ë‹¨, í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ìˆìœ¼ë©´ íŒ¨ìŠ¤)
    4. ë‚˜ë¨¸ì§€ -> ì‹ ê·œ ìƒì„±
    """
    if not new_concept or "title" not in new_concept:
        return

    raw_title = new_concept['title'].strip()
    raw_content = new_concept.get('content', "").strip()

    if len(raw_content) < 10:
        log_info(f"ë‚´ìš© ë¶€ì‹¤ë¡œ ì €ì¥ ê±°ë¶€: {raw_title}")
        return

    create_snapshot()
    data = load_concepts()

    best_match_idx = -1
    highest_sim = 0.0
    match_type = "NONE"

    target_fingerprint = normalize_fingerprint(raw_title)

    for idx, item in enumerate(data):
        existing_title = item.get('title', "")

        if is_whitelisted(raw_title, existing_title):
            continue

        existing_fingerprint = normalize_fingerprint(existing_title)

        if target_fingerprint == existing_fingerprint:
            best_match_idx = idx
            highest_sim = 1.0
            match_type = "EXACT"
            break

        sim = calculate_similarity(raw_title, existing_title)
        if sim > highest_sim:
            highest_sim = sim
            best_match_idx = idx

    if match_type == "EXACT" or highest_sim >= SIMILARITY_THRESHOLD_HIGH:
        target_item = data[best_match_idx]
        target_title = target_item.get('title')
        old_content = target_item.get('content', "")

        if normalize_fingerprint(raw_content) in normalize_fingerprint(old_content):
            log_info(f"ğŸ›¡ï¸ [Skip] '{raw_title}' ë‚´ìš©ì€ ì´ë¯¸ '{target_title}'ì— ìˆìŒ.")
            return

        today = datetime.now().strftime("%Y-%m-%d")
        append_header = f"\n\n\n--- ğŸ“… [ì¶”ê°€: {today}] (ìœ ì‚¬ë„ {int(highest_sim*100)}%) ---\n"

        target_item['content'] = old_content + append_header + raw_content
        target_item['last_updated'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        target_item['fingerprint'] = normalize_fingerprint(target_item.get('title', ''))

        save_all_concepts(data)
        log_info(f"ğŸ”— [Merged] '{raw_title}' -> '{target_title}' ë³‘í•© ì™„ë£Œ.")
        sync_db_to_memory(lambda _: None)
        return

    elif highest_sim >= SIMILARITY_THRESHOLD_WARN:
        sim_percent = int(highest_sim * 100)
        origin_title = data[best_match_idx]['title'] if best_match_idx >= 0 else "Unknown"

        tagged_title = f"(ì¤‘ë³µì˜ì‹¬) [{sim_percent}%] {raw_title}"

        log_warn(f"âš ï¸ [Suspect] '{raw_title}' vs '{origin_title}' ({sim_percent}%). íƒœê·¸ ë¶€ì°© ì €ì¥.")

        new_concept['title'] = tagged_title
        new_concept['created_at'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        new_concept['last_updated'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        warn_msg = f"> âš ï¸ **ì‹œìŠ¤í…œ ê²½ê³ :** ì´ ê°œë…ì€ '{origin_title}'ê³¼ {sim_percent}% ìœ ì‚¬í•©ë‹ˆë‹¤.\n\n"
        new_concept['content'] = warn_msg + raw_content
        new_concept['fingerprint'] = normalize_fingerprint(tagged_title)

        data.append(new_concept)
        save_all_concepts(data)
        sync_db_to_memory(lambda _: None)
        return

    new_concept['created_at'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    new_concept['last_updated'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    new_concept['fingerprint'] = normalize_fingerprint(raw_title)
    data.append(new_concept)
    save_all_concepts(data)
    log_info(f"âœ¨ [New] '{raw_title}' ì‹ ê·œ ë“±ë¡.")
    sync_db_to_memory(lambda _: None)


# ==========================================================
# [Notion ID Matching Support] sync/find
# ==========================================================
def sync_db_to_memory(log_func=print):
    """
    SQLite concepts ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œí•˜ì—¬ FAST_LOOKUP_MAP/GHOST_MAP ì¬êµ¬ì¶•.
    """
    global CONCEPT_CACHE, FAST_LOOKUP_MAP, GHOST_MAP
    data = load_concepts()

    CONCEPT_CACHE = data
    FAST_LOOKUP_MAP = {}
    GHOST_MAP = {}

    for item in data:
        raw_title = item.get("title", "")
        concept_id = str(item.get("notion_page_id") or item.get("id") or "")
        if not raw_title or not concept_id:
            continue

        norm_key = normalize_aggressive(raw_title)
        if norm_key:
            FAST_LOOKUP_MAP[norm_key] = concept_id

        if "í™•ë¥ ê³¼ í†µê³„" in raw_title or "í™•ë¥ ê³¼í†µê³„" in raw_title:
            stripped_src = raw_title.replace("í™•ë¥ ê³¼ í†µê³„", "").replace("í™•ë¥ ê³¼í†µê³„", "")
            stripped_key = normalize_aggressive(stripped_src)
            if stripped_key:
                GHOST_MAP[stripped_key] = concept_id

    try:
        log_func(f"âœ… [ConceptDB] ë©”ëª¨ë¦¬ ë™ê¸°í™” ì™„ë£Œ ({len(CONCEPT_CACHE)}ê°œ)")
    except Exception:
        pass
    return len(CONCEPT_CACHE)


def find_page_id(filename, debug=False):
    """
    íŒŒì¼ëª…ì„ ì •ê·œí™”í•˜ì—¬ concept/notion page idë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    1. Direct Match
    2. Ghost Match (í™•í†µ ê°•ì œ ë§¤ì¹­)
    """
    global FAST_LOOKUP_MAP, CONCEPT_CACHE, GHOST_MAP

    if not FAST_LOOKUP_MAP and not CONCEPT_CACHE:
        sync_db_to_memory(lambda _: None)
        if not FAST_LOOKUP_MAP:
            return None, "DB_CACHE_EMPTY"

    name_body = os.path.splitext(filename)[0]
    target_norm = normalize_aggressive(name_body)

    if target_norm in FAST_LOOKUP_MAP:
        if debug:
            print(f"ğŸš€ [HML Match] 100% ì¼ì¹˜: {filename}")
        return FAST_LOOKUP_MAP[target_norm], None

    year_match = re.search(r'(\d{4})ë…„', name_body)
    year = int(year_match.group(1)) if year_match else 0

    clean_name = re.sub(r'\[.*?\]', '', name_body)
    nums = re.findall(r'(\d+)', clean_name)
    q_num = 0
    if nums:
        for n in reversed(nums):
            if int(n) < 100:
                q_num = int(n)
                break

    is_high3 = "ê³ 3" in name_body

    if year >= 2021 and is_high3 and 23 <= q_num <= 30:
        if target_norm in GHOST_MAP:
            if debug:
                print(f"ğŸ‘» [Ghost Match] í™•í†µ ê°•ì œ ë§¤ì¹­: {filename}")
            return GHOST_MAP[target_norm], None

    return None, "NO_MATCH"


# ==========================================================
# [Helper Tools] UI ì—°ë™ìš© ë„êµ¬ë“¤
# ==========================================================
def delete_concept(target_title):
    create_snapshot()
    data = load_concepts()
    new_data = [d for d in data if d['title'] != target_title]
    if len(data) != len(new_data):
        save_all_concepts(new_data)
        sync_db_to_memory(lambda _: None)
        return True
    return False


def manual_update_concept(target_title, new_content):
    create_snapshot()
    data = load_concepts()
    for item in data:
        if item['title'] == target_title:
            item['content'] = new_content
            item['last_updated'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            item['fingerprint'] = normalize_fingerprint(item.get('title', ''))
            save_all_concepts(data)
            sync_db_to_memory(lambda _: None)
            return True
    return False


def merge_concepts_manual(master_title, slave_titles):
    """ì‚¬ìš©ìê°€ UIì—ì„œ ì„ íƒí•œ ê²ƒë“¤ ê°•ì œ ë³‘í•©"""
    create_snapshot()
    data = load_concepts()

    master_item = next((d for d in data if d['title'] == master_title), None)
    if not master_item:
        return False

    slaves = [d for d in data if d['title'] in slave_titles]

    today = datetime.now().strftime("%Y-%m-%d")
    merged_content = master_item.get('content', "")

    for slave in slaves:
        s_title = slave.get('title')
        s_content = slave.get('content', "")
        header = f"\n\n--- ğŸ”— [ë³‘í•©ë¨: {s_title} | {today}] ---\n"
        merged_content += header + s_content

    master_item['content'] = merged_content
    master_item['last_updated'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    master_item['fingerprint'] = normalize_fingerprint(master_item.get('title', ''))

    slave_set = set(slave_titles)
    new_data = [d for d in data if d['title'] not in slave_set]

    save_all_concepts(new_data)
    sync_db_to_memory(lambda _: None)
    return True


def remove_suspect_tag(target_title):
    """(ì¤‘ë³µì˜ì‹¬) íƒœê·¸ ì œê±° (ì²­ì†Œ ê¸°ëŠ¥)"""
    clean_title = re.sub(r'^\(ì¤‘ë³µì˜ì‹¬\)\s*\[\d+%\]\s*', '', target_title)

    if clean_title == target_title:
        return False

    create_snapshot()
    data = load_concepts()

    if any(d['title'] == clean_title for d in data):
        log_warn(f"íƒœê·¸ ì œê±° ë¶ˆê°€: '{clean_title}'ì´ ì´ë¯¸ ì¡´ì¬í•¨. ë³‘í•© ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        return "EXISTS"

    for item in data:
        if item['title'] == target_title:
            item['title'] = clean_title
            item['fingerprint'] = normalize_fingerprint(clean_title)
            content = item.get('content', "")
            content = re.sub(r'> âš ï¸ \*\*ì‹œìŠ¤í…œ ê²½ê³ :\*\*.*?\n\n', '', content, flags=re.DOTALL)
            item['content'] = content
            save_all_concepts(data)
            sync_db_to_memory(lambda _: None)
            return True

    return False


def get_similarity_clusters():
    """
    [UI ì •ë ¬ìš©] ì „ì²´ ê°œë…ì„ N*N ë¹„êµí•˜ì—¬ ìœ ì‚¬í•œ ê²ƒë¼ë¦¬ ë¬¶ì€ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    (ì„±ëŠ¥ ë¬´ì‹œ, ê²°ê³¼ ì§€í–¥)
    """
    data = load_concepts()
    if not data:
        return []

    items = []
    for d in data:
        items.append({
            'title': d['title'],
            'norm': normalize_fingerprint(d['title']),
            'visited': False
        })

    clusters = []

    for i in range(len(items)):
        if items[i]['visited']:
            continue

        current_cluster = [items[i]['title']]
        items[i]['visited'] = True
        base_norm = items[i]['norm']

        for j in range(i + 1, len(items)):
            if items[j]['visited']:
                continue

            sim = difflib.SequenceMatcher(None, base_norm, items[j]['norm']).ratio()
            if sim >= 0.4:
                current_cluster.append(items[j]['title'])
                items[j]['visited'] = True

        clusters.append(current_cluster)

    sorted_titles = []
    for cl in clusters:
        cl.sort()
        sorted_titles.extend(cl)

    return sorted_titles


# ì´ˆê¸°í™” ë£¨í‹´
try:
    backup_current_source_file()
    database_manager.init_db()
    sync_db_to_memory(lambda _: None)
except Exception:
    pass
