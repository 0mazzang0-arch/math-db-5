# notion_api.py
import requests
import json
import re
import os
import time
import difflib
import unicodedata
from datetime import datetime
from config import NOTION_API_KEY, NOTION_DATABASE_ID, MD_DIR_PATH

# ==========================================================
# [Configuration] í—¤ë” ë° ìƒìˆ˜ ì„¤ì • (ì ˆëŒ€ íƒ€í˜‘ ì—†ìŒ)
# ==========================================================
HEADERS = {
    "Authorization": f"Bearer {NOTION_API_KEY}",
    "Notion-Version": "2022-06-28",
    "Content-Type": "application/json"
}

CACHE_FILE_PATH = os.path.join(MD_DIR_PATH, "notion_db_cache.json")

# [Global State] ë©”ëª¨ë¦¬ ìºì‹œ ë° ê²€ìƒ‰ ìµœì í™” ë§µ
NOTION_CACHE = [] 
IS_CACHE_READY = False
FAST_LOOKUP_MAP = {} # { "ì •ê·œí™”ëœì œëª©": "page_id" }
GHOST_MAP = {}       # { "ì •ê·œí™”ëœì œëª©(í™•í†µì œê±°)": "page_id" } - í™•í†µ ê³¼ëª© ë§¤ì¹­ìš©

# ==========================================================
# [Core Logic 1] í†µì‹  ì•ˆì „ì¥ì¹˜ (Robust Request System)
# ==========================================================
def robust_request(method, url, payload=None, retries=5):
    """
    [ê¸°ì¡´ ê¸°ëŠ¥ ë³´ì¡´] ë„¤íŠ¸ì›Œí¬ ë¶ˆì•ˆì •, API ì†ë„ ì œí•œ(429), ì„œë²„ ì˜¤ë¥˜(5xx)ë¥¼ 
    5íšŒê¹Œì§€ ì¬ì‹œë„í•˜ë©° ë°©ì–´í•˜ëŠ” í†µì‹  í•¨ìˆ˜ì…ë‹ˆë‹¤.
    """
    last_error = None
    for attempt in range(retries):
        try:
            if method == "POST":
                res = requests.post(url, headers=HEADERS, json=payload, timeout=20)
            elif method == "PATCH":
                res = requests.patch(url, headers=HEADERS, json=payload, timeout=20)
            else:
                res = requests.get(url, headers=HEADERS, timeout=20)
            
            # 200 OK: ì„±ê³µ ì‹œ ì¦‰ì‹œ ë°˜í™˜
            if res.status_code == 200:
                return res
            
            # 429 Too Many Requests: ì§€ìˆ˜ ë°±ì˜¤í”„(Exponential Backoff)ë¡œ ëŒ€ê¸°
            if res.status_code == 429:
                wait_time = 2 ** attempt
                print(f"âš ï¸ [Notion 429] ì†ë„ ì œí•œ ê°ì§€! {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                time.sleep(wait_time)
                continue
                
            # 5xx Server Error: ë…¸ì…˜ ì„œë²„ ë¬¸ì œ, ì ì‹œ ëŒ€ê¸° í›„ ì¬ì‹œë„
            if 500 <= res.status_code < 600:
                print(f"âš ï¸ [Notion {res.status_code}] ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜. ì¬ì‹œë„ {attempt+1}/{retries}")
                time.sleep(1)
                continue
                
            # 409 Conflict / 502 Bad Gateway / 503 Service Unavailable / 504 Gateway Timeout
            if res.status_code in [409, 502, 503, 504]:
                print(f"âš ï¸ [Notion {res.status_code}] ì¼ì‹œì  ì˜¤ë¥˜. ì¬ì‹œë„ {attempt+1}/{retries}")
                time.sleep(1)
                continue
                
            # ê·¸ ì™¸ 4xx ì—ëŸ¬ëŠ” ì¬ì‹œë„í•´ë„ ì†Œìš©ì—†ìœ¼ë¯€ë¡œ ì—ëŸ¬ ë©”ì‹œì§€ ì €ì¥ í›„ ë°˜í™˜
            last_error = res.text
            return res 
            
        except Exception as e:
            last_error = str(e)
            print(f"ğŸ’¥ í†µì‹  ì˜ˆì™¸ ë°œìƒ (ì‹œë„ {attempt+1}/{retries}): {e}")
            time.sleep(1)
            
    print(f"âŒ [Final Fail] 5íšŒ ì¬ì‹œë„ ëª¨ë‘ ì‹¤íŒ¨. Last Error: {last_error}")
    return None

# ==========================================================
# [Core Logic 2] ì •ê·œí™” ë° ì§€ë¬¸ ì¶”ì¶œ (Forensic Text Analysis)
# ==========================================================
def normalize_aggressive(text):
    """
    [ê¸°ì¡´ ê¸°ëŠ¥ ë³´ì¡´] HML V229 ë¡œì§ ì´ì‹.
    ì œëª©ì˜ ë³¸ì§ˆ(ì•Œë§¹ì´)ë§Œ ë‚¨ê¸°ê³  ê»ë°ê¸°ë¥¼ ë²—ê²¨ë‚´ì–´ ë§¤ì¹­ë¥ ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.
    """
    if not text: return ""
    
    # 1. ìœ ë‹ˆì½”ë“œ ì •ê·œí™” (NFC)
    text = unicodedata.normalize('NFC', text)
    
    # 2. ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ ì œê±° (ê³µí†µë²”ìœ„ ë“±)
    text = text.replace("ê³µí†µë²”ìœ„", "").replace("ê³µí†µ", "")
    
    # 3. ì ìˆ˜ í‘œê¸°([3.00ì ]) ì²˜ë¦¬: ì ìˆ˜ ë’¤ì— ë¶™ì€ ë¬¸ê³¼/ì´ê³¼/ì˜ˆì²´ëŠ¥ ì œê±°
    score_match = re.search(r'(\[\d+\.\d+ì \])', text)
    if score_match:
        split_idx = score_match.start()
        front = text[:split_idx]
        back = text[split_idx:]
        back = back.replace("ë¬¸ê³¼", "").replace("ì´ê³¼", "").replace("ì˜ˆì²´ëŠ¥", "")
        text = front + back
    else:
        text = re.sub(r'(ë¬¸ê³¼|ì´ê³¼|ì˜ˆì²´ëŠ¥)\s*$', '', text)

    # 4. ì¡ë‹¤í•œ ê´„í˜¸ ë° ì´ë¯¸ì§€ íƒœê·¸ ì œê±°
    text = re.sub(r'\[.*?\]', '', text)
    text = re.sub(r'\(.*?\)', '', text)
    for _ in range(3): text = re.sub(r'(_img\d*|_\d+)\s*$', '', text)
    
    # 5. ìµœì¢… í•„í„°: ìˆ«ì, ì˜ì–´, í•œê¸€ë§Œ ë‚¨ê¹€ (íŠ¹ìˆ˜ë¬¸ì/ê³µë°± ì œê±°)
    text = re.sub(r'[^0-9a-zA-Zê°€-í£]', '', text).lower()
    return text.strip()

def extract_fingerprint(text):
    """
    [ê¸°ì¡´ ê¸°ëŠ¥ ë³´ì¡´] ë¬¸ì œì˜ 6ê°€ì§€ ì‹ ì› ì •ë³´(ì§€ë¬¸)ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ì´ ì •ë³´ëŠ” ìºì‹± ì‹œìŠ¤í…œì—ì„œ ë¬¸ì œì˜ ë™ì¼ì„±ì„ íŒë‹¨í•˜ëŠ” ë³´ì¡° ì§€í‘œë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
    """
    fingerprint = {
        "year": None,
        "month": None,
        "number": None,
        "subject": set(),
        "authority": set(),
        "grade": set()
    }
    
    # 1. ì—°ë„ (Year)
    year_match = re.search(r'(20\d{2})', text)
    if year_match: fingerprint["year"] = int(year_match.group(1))

    # 2. ì›” (Month)
    if "ìˆ˜ëŠ¥" in text or "ëŒ€í•™ìˆ˜í•™ëŠ¥ë ¥ì‹œí—˜" in text: fingerprint["month"] = 11
    else:
        month_match = re.search(r'(\d{1,2})ì›”', text)
        if month_match: fingerprint["month"] = int(month_match.group(1))

    # 3. ë¬¸ì œ ë²ˆí˜¸ (Number)
    clean_text_for_num = re.sub(r'\d+(\.\d+)?ì ', '', text)
    nums = re.findall(r'\d+', clean_text_for_num)
    if nums:
        # ë’¤ì—ì„œë¶€í„° ì°¾ë˜ 30ë²ˆ ì´í•˜ì¸ ìˆ«ìë¥¼ ë¬¸ì œ ë²ˆí˜¸ë¡œ ê°„ì£¼
        for n in reversed(nums):
            val = int(n)
            if 1 <= val <= 30:
                fingerprint["number"] = val
                break
    
    # 4. ê³¼ëª© (Subject)
    if "ê°€í˜•" in text or "ì´ê³¼" in text: fingerprint["subject"].add("ê°€í˜•")
    if "ë‚˜í˜•" in text or "ë¬¸ê³¼" in text: fingerprint["subject"].add("ë‚˜í˜•")
    if "ë¯¸ì " in text: fingerprint["subject"].add("ë¯¸ì ")
    if "ê¸°í•˜" in text: fingerprint["subject"].add("ê¸°í•˜")
    if "í™•í†µ" in text or "í™•ë¥ " in text: fingerprint["subject"].add("í™•í†µ")
    if "ê³µí†µ" in text: fingerprint["subject"].add("ê³µí†µ")

    # 5. ì¶œì œ ê¸°ê´€ (Authority)
    if "ì‚¬ê´€" in text: fingerprint["authority"].add("ì‚¬ê´€")
    if "ê²½ì°°" in text: fingerprint["authority"].add("ê²½ì°°")
    if "êµìœ¡ì²­" in text or "í•™í‰" in text or "í•™ë ¥" in text: fingerprint["authority"].add("êµìœ¡ì²­")
    if "í‰ê°€ì›" in text or "ëª¨ì˜" in text: fingerprint["authority"].add("í‰ê°€ì›")
    if "ìˆ˜ëŠ¥" in text or "ëŒ€í•™ìˆ˜í•™ëŠ¥ë ¥" in text: fingerprint["authority"].add("ìˆ˜ëŠ¥")

    # 6. í•™ë…„ (Grade)
    if "ê³ 1" in text: fingerprint["grade"].add("ê³ 1")
    if "ê³ 2" in text: fingerprint["grade"].add("ê³ 2")
    if "ê³ 3" in text: fingerprint["grade"].add("ê³ 3")

    return fingerprint

# ==========================================================
# [Caching System] ë¡œì»¬ íŒŒì¼ ê¸°ë°˜ ì¦ë¶„ ë™ê¸°í™” (Sync & Cache)
# ==========================================================
class SetEncoder(json.JSONEncoder):
    """[ê¸°ì¡´ ê¸°ëŠ¥ ë³´ì¡´] JSON ì €ì¥ ì‹œ Set ìë£Œí˜•ì„ Listë¡œ ë³€í™˜"""
    def default(self, obj):
        if isinstance(obj, set):
            return list(obj)
        return super().default(obj)

def load_local_cache():
    """[ê¸°ì¡´ ê¸°ëŠ¥ ë³´ì¡´] ë¡œì»¬ JSON ìºì‹œ íŒŒì¼ ë¡œë“œ"""
    if not os.path.exists(CACHE_FILE_PATH): return None
    try:
        with open(CACHE_FILE_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except: return None

def save_local_cache(data):
    """[ê¸°ì¡´ ê¸°ëŠ¥ ë³´ì¡´] ë¡œì»¬ JSON ìºì‹œ íŒŒì¼ ì €ì¥"""
    try:
        with open(CACHE_FILE_PATH, "w", encoding="utf-8") as f:
            json.dump(data, f, cls=SetEncoder, ensure_ascii=False, indent=2)
    except Exception as e: print(f"âš  ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

def sync_db_to_memory(log_func=print):
    """
    [ê¸°ì¡´ ê¸°ëŠ¥ ë³´ì¡´] Notion DBì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ê¸ì–´ì™€ ë©”ëª¨ë¦¬ì— ì˜¬ë¦½ë‹ˆë‹¤.
    ë¡œì»¬ íŒŒì¼ì´ ìˆìœ¼ë©´ ê·¸ê±¸ ë¨¼ì € ë¡œë“œí•˜ê³ , ë³€ê²½ëœ ë¶€ë¶„ë§Œ APIë¡œ ê°€ì ¸ì˜¤ëŠ” 'ì¦ë¶„ ì—…ë°ì´íŠ¸'ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    global NOTION_CACHE, IS_CACHE_READY, FAST_LOOKUP_MAP, GHOST_MAP
    
    local_data = load_local_cache()
    existing_map = {} 
    last_synced_time = None
    
    # 1. ë¡œì»¬ ìºì‹œ ë¡œë“œ ë° ë§µí•‘ êµ¬ì¶•
    if local_data:
        # log_func(f"ğŸ“‚ [System] ë¡œì»¬ ìºì‹œ ë¡œë“œ ({len(local_data)}ê°œ).")
        for item in local_data:
            existing_map[item["id"]] = item
            
            # Fast Lookup Map êµ¬ì¶• (ì •ê·œí™”ëœ ì œëª© -> PageID)
            raw_title = item.get("title", "")
            norm_key = normalize_aggressive(raw_title)
            FAST_LOOKUP_MAP[norm_key] = item["id"]
            
            # Ghost Map êµ¬ì¶• (í™•í†µ ê³¼ëª© íŠ¹í™” ë§¤ì¹­)
            if "í™•ë¥ ê³¼ í†µê³„" in raw_title or "í™•ë¥ ê³¼í†µê³„" in raw_title:
                stripped_src = raw_title.replace("í™•ë¥ ê³¼ í†µê³„", "").replace("í™•ë¥ ê³¼í†µê³„", "")
                stripped_key = normalize_aggressive(stripped_src)
                GHOST_MAP[stripped_key] = item["id"]
            
            # ë§ˆì§€ë§‰ ìˆ˜ì • ì‹œê°„ ì¶”ì 
            item_time = item.get("last_edited_time", "")
            if item_time:
                if not last_synced_time or item_time > last_synced_time:
                    last_synced_time = item_time
    else:
        print("âœ¨ [System] ë¡œì»¬ ìºì‹œ ì—†ìŒ. ì „ì²´ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")

    # 2. Notion API í˜¸ì¶œ (ì¦ë¶„ ì—…ë°ì´íŠ¸)
    url = f"https://api.notion.com/v1/databases/{NOTION_DATABASE_ID}/query"
    payload = {"page_size": 100}
    
    # ë§ˆì§€ë§‰ ë™ê¸°í™” ì‹œê°„ ì´í›„ì— ë³€ê²½ëœ ë°ì´í„°ë§Œ ê°€ì ¸ì˜¤ë„ë¡ í•„í„° ì„¤ì •
    if last_synced_time:
        payload["filter"] = {"timestamp": "last_edited_time", "last_edited_time": {"after": last_synced_time}}
    
    has_more = True
    next_cursor = None
    fetched_count = 0
    
    while has_more:
        if next_cursor: payload["start_cursor"] = next_cursor
        try:
            res = requests.post(url, headers=HEADERS, json=payload)
            if res.status_code != 200: 
                time.sleep(2)
                continue
            
            data = res.json()
            results = data.get("results", [])
            
            for page in results:
                try:
                    page_id = page["id"]
                    edited_time = page["last_edited_time"]
                    props = page["properties"]
                    
# ----------------------------------------------------------------------------------------------------
                    # [ìˆ˜ìˆ  ë¶€ìœ„] ê³µë°± ì œëª© ë¬´ì‹œ ë° ì¶œì²˜ ì»¬ëŸ¼ ìŠ¹ê²© ë¡œì§ (Invisible Wall ë°©ì–´)
                    # ----------------------------------------------------------------------------------------------------
                    title_obj = props.get("ë¬¸ì œ&í’€ì´", {})
                    t_list = title_obj.get("title", []) or title_obj.get("rich_text", [])
                    
                    # í•µì‹¬: .strip()ì„ ì¶”ê°€í•˜ì—¬ ê³µë°±ë§Œ ìˆëŠ” ì¢€ë¹„ ì œëª©ì„ ë¹ˆ ë¬¸ìì—´("")ë¡œ ì²˜ë¦¬
                    raw_title = t_list[0].get("plain_text", "").strip() if t_list else ""

                    # ì œëª©ì´ í…… ë¹„ì—ˆë‹¤ë©´(ê³µë°± í¬í•¨) 'ì¶œì²˜' ì»¬ëŸ¼ì„ ì œëª©ìœ¼ë¡œ ìŠ¹ê²©
                    if not raw_title:
                        src_obj = props.get("ì¶œì²˜", {})
                        s_list = src_obj.get("rich_text", []) or src_obj.get("title", [])
                        if s_list: raw_title = s_list[0].get("plain_text", "").strip()

                    if raw_title:
                    # ----------------------------------------------------------------------------------------------------
                        norm_key = normalize_aggressive(raw_title)
                        FAST_LOOKUP_MAP[norm_key] = page_id
                        
                        if "í™•ë¥ ê³¼ í†µê³„" in raw_title or "í™•ë¥ ê³¼í†µê³„" in raw_title:
                            stripped_src = raw_title.replace("í™•ë¥ ê³¼ í†µê³„", "").replace("í™•ë¥ ê³¼í†µê³„", "")
                            stripped_key = normalize_aggressive(stripped_src)
                            GHOST_MAP[stripped_key] = page_id

                        # ë°ì´í„° ê°ì²´ ìƒì„± ë° ì €ì¥
                        new_item = {
                            "id": page_id, "title": raw_title, "last_edited_time": edited_time,
                            "norm": norm_key, "fp": extract_fingerprint(raw_title)
                        }
                        existing_map[page_id] = new_item
                except: continue
                
            fetched_count += len(results)
            has_more = data.get("has_more", False)
            next_cursor = data.get("next_cursor")
        except: time.sleep(2)

    # ìµœì¢… ìºì‹œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    NOTION_CACHE = list(existing_map.values())
    IS_CACHE_READY = True
    
    # ë³€ê²½ì‚¬í•­ì´ ìˆê±°ë‚˜ ë¡œì»¬ ë°ì´í„°ê°€ ì—†ì—ˆìœ¼ë©´ íŒŒì¼ë¡œ ì €ì¥
    if fetched_count > 0 or not local_data:
        # print(f"ğŸ’¾ [System] ìºì‹œ íŒŒì¼ ì—…ë°ì´íŠ¸... (ì´ {len(NOTION_CACHE)}ê°œ)")
        save_local_cache(NOTION_CACHE)
    
    return len(NOTION_CACHE)

def find_page_id(filename, debug=False):
    """
    [ê¸°ì¡´ ê¸°ëŠ¥ ë³´ì¡´] ì œëª©ìœ¼ë¡œ Notion Page IDë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    1. Exact Match (ì •ê·œí™” í›„ ë¹„êµ)
    2. Ghost Match (í™•í†µ ê³¼ëª© íŠ¹ìˆ˜ ì²˜ë¦¬)
    """
    global FAST_LOOKUP_MAP, GHOST_MAP
    
    # ìºì‹œê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ë¡œë“œ ì‹œë„
    if not FAST_LOOKUP_MAP and not NOTION_CACHE: 
        sync_db_to_memory()
        if not FAST_LOOKUP_MAP: return None, "DB_CACHE_EMPTY"

    name_body = os.path.splitext(filename)[0]
    target_norm = normalize_aggressive(name_body)
    
    # 1. Direct Match (1:1 Map Lookup)
    if target_norm in FAST_LOOKUP_MAP:
        if debug: print(f"ğŸš€ [HML Match] 100% ì¼ì¹˜: {filename}")
        return FAST_LOOKUP_MAP[target_norm], None
        
    # 2. Forced Match (Ghost Map for Prob/Stat)
    # ì¡°ê±´: 2021ë…„ ì´í›„ + ê³ 3 + 23~30ë²ˆ ë¬¸ì œì¸ ê²½ìš°
    year_match = re.search(r'(\d{4})ë…„', name_body)
    year = int(year_match.group(1)) if year_match else 0
    
    clean_name = re.sub(r'\[.*?\]', '', name_body)
    nums = re.findall(r'(\d+)', clean_name)
    q_num = 0
    if nums:
        for n in reversed(nums):
            if int(n) < 100: 
                q_num = int(n)
                break
                
    is_high3 = "ê³ 3" in name_body
    
    if year >= 2021 and is_high3 and 23 <= q_num <= 30:
        if target_norm in GHOST_MAP:
            if debug: print(f"ğŸ‘» [Ghost Match] í™•í†µ ê°•ì œ ë§¤ì¹­: {filename}")
            return GHOST_MAP[target_norm], None

    return None, "NO_MATCH"

# ==========================================================
# [Helpers] LaTeX & Block Rendering (í•œê¸€ ìˆ˜ì‹ ë³µêµ¬ ê¸°ëŠ¥ í¬í•¨)
# ==========================================================
def make_rich_text_list(content):
    if not content: return []
    content = str(content)
    content = content.replace("\\\\", "\\")
    # [ìˆ˜ì •] Notionì´ ëª» ì½ëŠ” LaTeX ë¬¸ì„œ íƒœê·¸ ì œê±° ë° ë³€í™˜ (ì²­ì†Œ ì‘ì—…)
    content = re.sub(r'\\begin\{itemize\}', '', content)
    content = re.sub(r'\\end\{itemize\}', '', content)
    content = re.sub(r'\\begin\{enumerate\}', '', content)
    content = re.sub(r'\\end\{enumerate\}', '', content)
    content = re.sub(r'\\item\s*', '\nâ€¢ ', content)        
    content = re.sub(r'\\textbf\{([^}]+)\}', r'\1', content) 
    content = re.sub(r'\\underline\{([^}]+)\}', r'\1', content) 
    content = re.sub(r'\\textcircled\{([^}]+)\}', r'(\1)', content) 
    content = re.sub(r'\\quad', '  ', content)              
    
    # 1. ì´ì¤‘ ë°±ìŠ¬ë˜ì‹œ ê³¼ë‹¤ ì´ìŠ¤ì¼€ì´í”„ ì •ë¦¬
    content = content.replace("\\\\\\\\", "\\\\")
    
    # ì •ê·œì‹ìœ¼ë¡œ ìˆ˜ì‹ ë¸”ë¡ ë¶„ë¦¬
    pattern = r'(\$\$[\s\S]+?\$\$|\\\[[\s\S]+?\\\]|\$[\s\S]+?\$|\\\([\s\S]+?\\\))'
    tokens = re.split(pattern, content)
    rich_text = []
    
    # [Helper] í•œê¸€ ë˜í•‘ í•¨ìˆ˜ (ìˆ˜ì‹ ë‚´ í•œê¸€ ê¹¨ì§ ë°©ì§€)
    def wrap_korean(match):
        return f"\\text{{{match.group(0)}}}"

    for token in tokens:
        if not token: continue
        token_strip = token.strip()
        
        if not token_strip:
            rich_text.append({"type": "text", "text": {"content": token}})
            continue

        is_equation = False
        expr = ""
        
        # ìˆ˜ì‹ íƒœê·¸ ê°ì§€ ë° ê»ë°ê¸° ë²—ê¸°ê¸°
        if token_strip.startswith("$$") and token_strip.endswith("$$"):
            expr = token_strip[2:-2].strip(); is_equation = True
        elif token_strip.startswith("\\[") and token_strip.endswith("\\]"):
            expr = token_strip[2:-2].strip(); is_equation = True
        elif token_strip.startswith("$") and token_strip.endswith("$"):
            expr = token_strip[1:-1].strip(); is_equation = True
        elif token_strip.startswith("\\(") and token_strip.endswith("\\)"):
            expr = token_strip[2:-2].strip(); is_equation = True
            
        if is_equation:
            # ë¹ˆ ìˆ˜ì‹ ë°©ì–´ (Notion 400 Error ë°©ì§€)
            if not expr or expr.strip() == "":
                rich_text.append({"type": "text", "text": {"content": " "}})
            else:
                # í•œê¸€ ì²˜ë¦¬ ë¡œì§
                if re.search(r'[ê°€-í£]+', expr) and not "\\text" in expr:
                    expr = re.sub(r'([ê°€-í£]+)', wrap_korean, expr)
                
                # ì¬ê²€ì‚¬
                if not expr.strip():
                     rich_text.append({"type": "text", "text": {"content": " "}})
                else:
                    rich_text.append({"type": "equation", "equation": {"expression": expr}})
        else:
            # ì¼ë°˜ í…ìŠ¤íŠ¸ 2000ì ì œí•œ ì²˜ë¦¬
            if len(token) > 1900:
                chunks = [token[i:i+1900] for i in range(0, len(token), 1900)]
                for c in chunks:
                    rich_text.append({"type": "text", "text": {"content": c}})
            else:
                rich_text.append({"type": "text", "text": {"content": token}})
            
    return rich_text

def create_block(type, content, color="default", icon=None):
    """[ê¸°ì¡´ ê¸°ëŠ¥ ë³´ì¡´] ë¸”ë¡ ìƒì„± í—¬í¼ í•¨ìˆ˜"""
    if not content: return []
    
    # ë¸”ë¡ ìˆ˜ì‹ ($$...$$) ë‹¨ë… ì²˜ë¦¬
    if content.strip().startswith("$$") and content.strip().endswith("$$"):
        expr = content.strip().replace("$$", "").strip()
        if expr: return [{"object": "block", "type": "equation", "equation": {"expression": expr}}]
        
    full_rich_text = make_rich_text_list(content)
    if not full_rich_text: return []
    
    # 2000ì ì œí•œ ë°©ì§€ (ì²­í¬ ë¶„í• )
    chunks = [full_rich_text[i:i + 100] for i in range(0, len(full_rich_text), 100)]
    blocks = []
    for chunk in chunks:
        block = {"object": "block", "type": type, type: {"rich_text": chunk}}
        if color != "default" and type != "paragraph": block[type]["color"] = color
        if type == "callout" and icon: block[type]["icon"] = {"emoji": icon}
        blocks.append(block)
    return blocks

# ==========================================================
# [Core Logic 3] í˜ì´ì§€ ìƒì„± ë° ì†ì„± ì—…ë°ì´íŠ¸ (Properties)
# ==========================================================
def create_new_problem_page(title, db_data, concept_ids=None):
    """
    [ê¸°ì¡´ ê¸°ëŠ¥ ë³´ì¡´] Notion DBì— ìƒˆ í˜ì´ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ëª¨ë“  ë©”íƒ€ë°ì´í„°(ë‚œì´ë„, ë“±ê¸‰, ìœ í˜•, ì¶œì²˜ ë“±)ë¥¼ ë§¤í•‘í•©ë‹ˆë‹¤.
    """
    url = "https://api.notion.com/v1/pages"
    
    props = {
        "ì´ë¦„": {"title": [{"text": {"content": title}}]}
    }
    
    # ë©”íƒ€ë°ì´í„° ë§¤í•‘
    if db_data.get("main_category"):
        props["ëŒ€ë¶„ë¥˜"] = {"select": {"name": str(db_data["main_category"])}}
    if db_data.get("sub_category"):
        props["ì¤‘ë¶„ë¥˜"] = {"select": {"name": str(db_data["sub_category"])}}
    if db_data.get("difficulty"):
        props["ë‚œì´ë„"] = {"select": {"name": str(db_data["difficulty"])}}
    if db_data.get("grade"):
        props["ë“±ê¸‰"] = {"select": {"name": str(db_data["grade"])}}
    if db_data.get("type"):
        props["ìœ í˜•"] = {"select": {"name": str(db_data["type"])}}

    # ì¶œì²˜ ì²˜ë¦¬ (Safe Logic: í…ìŠ¤íŠ¸ë¡œ ì…ë ¥í•˜ì—¬ ì˜¤ë¥˜ ë°©ì§€)
    if db_data.get("source"):
        src_val = str(db_data["source"])
        props["ì¶œì²˜"] = {"rich_text": [{"text": {"content": src_val}}]}
    
    # í•„ì—°ì„±/í•µì‹¬/íŠ¹ì´ì  (Legacy Support for DB Filtering)
    if db_data.get("necessity"):
        props["í•„ì—°ì„±"] = {"rich_text": [{"text": {"content": str(db_data["necessity"])[:2000]}}]}
    if db_data.get("key_idea"):
        props["í•µì‹¬ ì•„ì´ë””ì–´"] = {"rich_text": [{"text": {"content": str(db_data["key_idea"])[:2000]}}]}
    if db_data.get("special_point"):
        props["íŠ¹ì´ì "] = {"rich_text": [{"text": {"content": str(db_data["special_point"])[:2000]}}]}
    # [NEW] ì •ë‹µ (Correct Answer) - ì•ˆì „ì¥ì¹˜ ì ìš© (Over-engineering)
    if db_data.get("correct_answer"):
        # ì •ë‹µì´ ë„ˆë¬´ ê¸¸ë©´(í•´ì„¤ì´ ë”¸ë ¤ì˜¤ë©´) 100ìë¡œ ìë¥´ëŠ” ë°©ì–´ ë¡œì§ ì ìš©
        ans_val = str(db_data["correct_answer"]).strip()
        if len(ans_val) > 100: ans_val = ans_val[:100]
        props["ì •ë‹µ"] = {"rich_text": [{"text": {"content": ans_val}}]}
    # íƒœê·¸ ì²˜ë¦¬ (Multi-select)
    if db_data.get("tags"):
        tag_list = []
        raw_tags = db_data["tags"]
        if isinstance(raw_tags, str): raw_tags = [t.strip() for t in raw_tags.split(',')]
        for t in raw_tags: tag_list.append({"name": str(t)})
        if tag_list: props["íƒœê·¸"] = {"multi_select": tag_list}

    # ê°œë… ì—°ê²° (Relation)
    if concept_ids and isinstance(concept_ids, list):
        relation_list = [{"id": cid} for cid in concept_ids]
        props["ì‹¤ì „ê°œë…"] = {"relation": relation_list}

    payload = {"parent": {"database_id": NOTION_DATABASE_ID}, "properties": props}
    
    res = robust_request("POST", url, payload)
    if res and res.status_code == 200:
        page_data = res.json()
        page_id = page_data["id"]
        
        # ìºì‹œ ê°±ì‹  (ì¦‰ì‹œ ê²€ìƒ‰ ê°€ëŠ¥í•˜ë„ë¡)
        norm_key = normalize_aggressive(title)
        FAST_LOOKUP_MAP[norm_key] = page_id
        return page_id, "Success"
    else:
        err_msg = res.text if res else "No Response"
        return None, f"Create Failed: {err_msg}"

def update_page_properties(page_id, db_data, concept_ids=None):
    """[ê¸°ì¡´ ê¸°ëŠ¥ ë³´ì¡´] ê¸°ì¡´ í˜ì´ì§€ ì†ì„± ì—…ë°ì´íŠ¸"""
    nec = db_data.get("necessity") or ""
    key = db_data.get("key_idea") or ""
    spe = db_data.get("special_point") or ""
    
    properties = {
        "í•„ì—°ì„±": {"rich_text": [{"type": "text", "text": {"content": str(nec)}}]},
        "í•µì‹¬ ì•„ì´ë””ì–´": {"rich_text": [{"type": "text", "text": {"content": str(key)}}]},
        "íŠ¹ì´ì ": {"rich_text": [{"type": "text", "text": {"content": str(spe)}}]},
    }
    
    if concept_ids and isinstance(concept_ids, list):
        relation_list = [{"id": cid} for cid in concept_ids]
        properties["ì‹¤ì „ê°œë…"] = {"relation": relation_list}
        
    payload = {"properties": properties}
    url = f"https://api.notion.com/v1/pages/{page_id}"
    
    res = robust_request("PATCH", url, payload)
    if res and res.status_code == 200: return True, "ì„±ê³µ"
    return False, res.text if res else "Update Failed"
def make_heading_2(text, color="default"):
    return {"object": "block", "type": "heading_2", "heading_2": {"rich_text": make_rich_text_list(text), "color": color}}

def make_text_block(text):
    return {"object": "block", "type": "paragraph", "paragraph": {"rich_text": make_rich_text_list(text)}}

# ----------------------------------------------------------------------------------------------------
# [ìˆ˜ìˆ  ë¶€ìœ„ 2] 400 Error (Limit 100) ë°©ì–´ìš© Callout ìƒì„±ê¸° (ë¦¬ìŠ¤íŠ¸ ë°˜í™˜)
# Insight ë‚´ìš©ì´ ê¸¸ì–´ ìˆ˜ì‹ ì¡°ê°ì´ 100ê°œë¥¼ ë„˜ìœ¼ë©´, ì—¬ëŸ¬ ê°œì˜ Callout ë¸”ë¡ìœ¼ë¡œ ìª¼ê°œì„œ ë°˜í™˜í•©ë‹ˆë‹¤.
# ----------------------------------------------------------------------------------------------------
def make_callout(text, icon="ğŸ’¡"):
    full_rich_text = make_rich_text_list(text)
    if not full_rich_text: return []

    # ì•ˆì „í•˜ê²Œ 90ê°œì”© ëŠì–´ì„œ ë¸”ë¡ ë¶„í•  (Notion ì œí•œ: 100ê°œ)
    chunk_size = 90
    chunks = [full_rich_text[i:i + chunk_size] for i in range(0, len(full_rich_text), chunk_size)]
    
    blocks = []
    for chunk in chunks:
        blocks.append({
            "object": "block", 
            "type": "callout", 
            "callout": {"rich_text": chunk, "icon": {"emoji": icon}}
        })
    return blocks
# ----------------------------------------------------------------------------------------------------
def make_quote_block(text):
    """
    í…ìŠ¤íŠ¸ë¥¼ ì¸ìš©êµ¬(Quote) ë¸”ë¡ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. 
    ë‚´ë¶€ì˜ ëª¨ë“  LaTeX ìˆ˜ì‹ì´ ì™„ë²½í•˜ê²Œ ë…¸ì…˜ ìˆ˜ì‹ ê°ì²´ë¡œ ë Œë”ë§ë˜ë„ë¡ ê°•ì œí•©ë‹ˆë‹¤.
    """
    if not text or text.strip() == "":
        return {
            "object": "block",
            "type": "quote",
            "quote": {"rich_text": [{"type": "text", "text": {"content": " "}}]}
        }
        
    # [í•µì‹¬ ìˆ˜ìˆ  ë¶€ìœ„] í…ìŠ¤íŠ¸ë¥¼ ìƒìœ¼ë¡œ ë„£ì§€ ì•Šê³  ë°˜ë“œì‹œ ìˆ˜ì‹ ë³€í™˜ê¸°ë¥¼ ê±°ì¹˜ê²Œ í•¨
    rendered_rich_text = make_rich_text_list(text)
    
    # ë§Œì•½ ë³€í™˜ê¸°ê°€ ì‹¤íŒ¨í•´ì„œ ë¹ˆ ë°°ì—´ì´ ì˜¤ë©´ ìµœì†Œí•œì˜ ê³µë°±ì´ë¼ë„ ë„£ì–´ ì—ëŸ¬ ë°©ì§€
    if not rendered_rich_text:
        rendered_rich_text = [{"type": "text", "text": {"content": " "}}]
        
    return {
        "object": "block",
        "type": "quote",
        "quote": {"rich_text": rendered_rich_text}
    }
# ==========================================================
# [Core Logic 4] ë³¸ë¬¸ ë‚´ìš© ìƒì„± (The Body Builder) - V30
# ==========================================================
# ==========================================
# 1. ì›ë³¸ ì£¼ì„ì„ 100% ì‚´ë¦° í‘œ ìƒì„±ê¸°
# ==========================================
def make_teacher_decoding_table(decoding_list):
    """
    [ì‹ ê·œ ê¸°ëŠ¥] 'ì„ ìƒë‹˜ì˜ ì‹œì„ ' ë°ì´í„°ë¥¼ Notion í‘œ(Table)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    êµ¬ì¡°: [ê¸°í˜¸ | êµ¬ë¶„ | ë‚´ìš© | AI í•´ì„]
    """
    if not decoding_list: return None
    
    table_block = {
        "object": "block",
        "type": "table",
        "table": {
            "table_width": 4,
            "has_column_header": True,
            "has_row_header": False,
            "children": [
                # í—¤ë” í–‰
                {
                    "type": "table_row",
                    "table_row": {
                        "cells": [
                            [{"text": {"content": "ê¸°í˜¸"}}],
                            [{"text": {"content": "êµ¬ë¶„"}}],
                            [{"text": {"content": "ì„ ìƒë‹˜ì˜ ë©”ëª¨ (OCR)"}}],
                            [{"text": {"content": "ğŸ’¡ AIì˜ í•´ì„ (Interpretation)"}}]
                        ]
                    }
                }
            ]
        }
    }
    
    # ë°ì´í„° í–‰ ì¶”ê°€
    for item in decoding_list:
        symbol = item.get("symbol", "")
        dtype = item.get("type", "")
        content = item.get("content", "")
        comment = item.get("ai_comment", "")
        
        row = {
            "type": "table_row",
            "table_row": {
                "cells": [
                    make_rich_text_list(symbol) or [{"type": "text", "text": {"content": " "}}],
                    make_rich_text_list(dtype) or [{"type": "text", "text": {"content": " "}}],
                    make_rich_text_list(content) or [{"type": "text", "text": {"content": " "}}],
                    make_rich_text_list(comment) or [{"type": "text", "text": {"content": " "}}]
                ]
            }
        }
        table_block["table"]["children"].append(row)
        
    return table_block

def append_children(page_id, body_content):
    """
    [í•µì‹¬] í˜ì´ì§€ ë³¸ë¬¸ì— ë¸”ë¡ë“¤ì„ ìˆœì„œëŒ€ë¡œ ìŒ“ì•„ ì˜¬ë¦½ë‹ˆë‹¤.
    ì„ ìƒë‹˜ ìš”ì²­ ìˆœì„œ:
    1. ğŸ“¸ ì›ë³¸ ë¬¸ì œ (Image)
    2. ğŸ§  ì„ ìƒë‹˜ì˜ ì‹œì„  (Teacher's Decoding) -> Table
    3. ğŸ¤– í–‰ë™ ê°•ë ¹ (Action Protocol) -> Callout
    4. âœï¸ ì†ê¸€ì”¨ í’€ì´ (Verbatim) -> Quote
    5. ğŸ“ AI ì •ì„ í•´ì„¤ (Standard Solution) -> Text
    6. ğŸ“š ì‹¤ì „ ê°œë… (My Dictionary) -> Toggle/Callout
    7. ğŸ† Insight -> Callout
    """
    
    # [Pre-flight Check] ë¸”ë¡ ë°ì´í„° ì†Œë… (ë¹ˆ ìˆ˜ì‹ ì œê±° ë“±)
    def sanitize_blocks_recursive(blocks):
        clean_blocks = []
        for block in blocks:
            # 1. Rich Text ê²€ì‚¬
            for type_key in ["paragraph", "heading_1", "heading_2", "heading_3", "callout", "quote", "bulleted_list_item", "numbered_list_item"]:
                if type_key in block and "rich_text" in block[type_key]:
                    new_rich_text = []
                    for rt in block[type_key]["rich_text"]:
                        if rt.get("type") == "equation":
                            expr = rt.get("equation", {}).get("expression", "")
                            if not expr or not str(expr).strip():
                                new_rich_text.append({"type": "text", "text": {"content": " "}})
                            else: new_rich_text.append(rt)
                        elif rt.get("type") == "text":
                            content = rt.get("text", {}).get("content", "")
                            if content: new_rich_text.append(rt)
                        else: new_rich_text.append(rt)
                    
                    if not new_rich_text:
                        new_rich_text = [{"type": "text", "text": {"content": " "}}]
                    block[type_key]["rich_text"] = new_rich_text

            # 2. Block Equation ê²€ì‚¬
            if block.get("type") == "equation":
                expr = block.get("equation", {}).get("expression", "")
                if not expr or not str(expr).strip(): continue

            clean_blocks.append(block)
        return clean_blocks

    all_blocks = []
    
    # -------------------------------------------------------
    # 1. ğŸ“¸ ì›ë³¸ ì´ë¯¸ì§€ (Image)
    # -------------------------------------------------------
    img_url = body_content.get("image_url")
    if img_url and img_url.startswith("http"):
        all_blocks.append(make_heading_2("ğŸ“¸ ì›ë³¸ ë¬¸ì œ & í•„ê¸°"))
        all_blocks.append({
            "object": "block",
            "type": "image",
            "image": {
                "type": "external",
                "external": {"url": img_url}
            }
        })
    
    # -------------------------------------------------------
    # 2. ğŸ§  ì„ ìƒë‹˜ì˜ ì‹œì„  (Teacher's Decoding) [ì‹ ê·œ/í†µí•©]
    # -------------------------------------------------------
    decoding_list = body_content.get("teacher_decoding", [])
    if decoding_list:
        all_blocks.append(make_heading_2("ğŸ§  ì„ ìƒë‹˜ì˜ ì‹œì„  (Teacher's Decoding)", "blue_background"))
        table = make_teacher_decoding_table(decoding_list)
        if table: all_blocks.append(table)
        all_blocks.append(make_text_block(" ")) # ê³µë°±

    # -------------------------------------------------------
    # 3. ğŸ¤– í–‰ë™ ê°•ë ¹ & ì „ëµ (Action Protocol & Algorithm)
    # -------------------------------------------------------
    strategy = body_content.get("strategy_overview", "")
    protocol = body_content.get("action_protocol", "")
    
    if strategy or protocol:
        all_blocks.append(make_heading_2("ğŸ¤– AIê°€ ì œì•ˆí•˜ëŠ” í•„ì—°ì„± & í–‰ë™ê°•ë ¹"))
        if strategy:
            all_blocks.append(make_text_block(f"ğŸ—ºï¸ ì „ëµ ë¡œë“œë§µ:\n{strategy}"))
        if protocol:
            all_blocks.append(make_text_block(f"âš¡ AIê°€ ì œì•ˆí•˜ëŠ” í•„ì—°ì„± & í–‰ë™ê°•ë ¹:\n{protocol}"))
        all_blocks.append(make_text_block(" "))

    # -------------------------------------------------------
    # 4. âœï¸ ì„ ìƒë‹˜ì˜ ì†í•„ê¸° í’€ì´ (Verbatim)
    # -------------------------------------------------------
    verbatim = body_content.get("verbatim_handwriting", "")
    if verbatim:
        all_blocks.append(make_heading_2("âœï¸ ì„ ìƒë‹˜ì˜ ì†í•„ê¸° í’€ì´ (Verbatim)"))
        all_blocks.append(make_quote_block(verbatim))
        all_blocks.append(make_text_block(" "))

    # -------------------------------------------------------
    # 5. ğŸ“ AI ì •ì„ í•´ì„¤ (Standard Solution)
    # -------------------------------------------------------
    ai_sol = body_content.get("ai_solution", "")
    if ai_sol:
        all_blocks.append(make_heading_2("ğŸ“ AI ì •ì„ í•´ì„¤ (Standard Solution)"))
        chunks = [ai_sol[i:i+2000] for i in range(0, len(ai_sol), 2000)]
        for chunk in chunks:
            all_blocks.append(make_text_block(chunk))
        all_blocks.append(make_text_block(" "))

    # -------------------------------------------------------
    # 6. ğŸ“š ì‹¤ì „ ê°œë… (My Dictionary)
    # -------------------------------------------------------
    concepts = body_content.get("practical_concepts", [])
    if concepts:
        all_blocks.append(make_heading_2("ğŸ“š ì‹¤ì „ ê°œë… (My Dictionary)"))
        for c in concepts:
            title = c.get("title", "ê°œë…")
            content = c.get("content", "")
            all_blocks.append({
                "object": "block",
                "type": "toggle",
                "toggle": {
                    "rich_text": [{"type": "text", "text": {"content": f"ğŸ“Œ {title}"}, "annotations": {"bold": True}}],
                    "children": [
                        {
                            "object": "block",
                            "type": "paragraph",
                            "paragraph": {
                                "rich_text": [{"type": "text", "text": {"content": content[:2000]}}]
                            }
                        }
                    ]
                }
            })
        all_blocks.append(make_text_block(" "))

    # -------------------------------------------------------
    # 7. ğŸ† 1íƒ€ ê°•ì‚¬ì˜ Insight (ë§ˆë¬´ë¦¬)
    # -------------------------------------------------------
    insight = body_content.get("instructor_solution", "")
    if insight:
        all_blocks.append(make_heading_2("ğŸ† 1íƒ€ ê°•ì‚¬ì˜ Insight", "yellow_background"))
        all_blocks.extend(make_callout(insight, "ğŸ”¥"))

    # =======================================================
    # [Final Step] ë¸”ë¡ ì „ì†¡ (Batch Upload)
    # =======================================================
    # ì „ì†¡ ì „ ìµœì¢… ì†Œë…
    final_children = sanitize_blocks_recursive([c for c in all_blocks if c])
    
    batch_size = 90
    url = f"https://api.notion.com/v1/blocks/{page_id}/children"
    
    is_all_success = True
    
    for i in range(0, len(final_children), batch_size):
        batch = final_children[i:i + batch_size]
        payload = {"children": batch}
        
        success_chunk = False
        last_error = ""
        
        # [ë³µêµ¬ ì™„ë£Œ] Retry Logic with Unarchive Handling
        for attempt in range(3):
            try:
                res = requests.patch(url, headers=HEADERS, json=payload)
                
                if res.status_code == 200:
                    success_chunk = True
                    break 
                
                # [Error Handling] Archived Error -> í˜ì´ì§€ ë³µêµ¬ ì‹œë„
                elif res.status_code == 400 and "archived" in res.text.lower():
                    print(f"ğŸ’€ [Notion] í˜ì´ì§€ê°€ ì‚­ì œë¨(Archived) ê°ì§€. ê°•ì œ ë³µêµ¬(Unarchive) ì‹œë„ ì¤‘...")
                    restore_url = f"https://api.notion.com/v1/pages/{page_id}"
                    restore_payload = {"archived": False}
                    restore_res = requests.patch(restore_url, headers=HEADERS, json=restore_payload)
                    
                    if restore_res.status_code == 200:
                        print(f"ğŸ§Ÿ [Notion] í˜ì´ì§€ ë³µêµ¬ ì„±ê³µ! ë¸”ë¡ ì „ì†¡ ì¬ì‹œë„...")
                        time.sleep(1)
                        continue 
                    else:
                        print(f"âš°ï¸ [Notion] í˜ì´ì§€ ë³µêµ¬ ì‹¤íŒ¨: {restore_res.text}")
                
                # ê·¸ ì™¸ ì—ëŸ¬
                else:
                    last_error = res.text
                    print(f"âš ï¸ [Append Fail] {res.status_code}: {res.text[:150]}...")
                    time.sleep(1)
                    
            except Exception as e:
                last_error = str(e)
                print(f"âš ï¸ [Append Error] {e}")
                time.sleep(1)
        
        if not success_chunk:
            print(f"âŒ [Critical] ë¸”ë¡ ì „ì†¡ ì‹¤íŒ¨. Reason: {last_error}")
            is_all_success = False
            break 
            
    if is_all_success:
        return True, "ì„±ê³µ"
    else:
        return False, f"ì‹¤íŒ¨: {last_error}"